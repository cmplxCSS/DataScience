{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import GPy, GPyOpt\n",
    "import numpy as np\n",
    "import pandas as pds\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "#from tensorflow.keras.layers import Lambda\n",
    "from keras.layers import Activation, Dropout, BatchNormalization, Dense, RepeatVector, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "import h5py\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.initializers import RandomUniform\n",
    "from keras import optimizers\n",
    "#random.seed(42)\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file locations, features and target\n",
    "feature_dict = {}\n",
    "target_state_dict={}\n",
    "\n",
    "path_to_file = \"C:\\\\Users\\\\mooc\\\\Documents\\\\Bosch-MT\\\\Daten\\\\UCI_machine_learning_repository\\\\Condition_hydraulic_systems_data_set\"\n",
    "target_filename = \"profile.txt\"\n",
    "\n",
    "feature_list = ['PS1', 'PS2', 'PS3', 'PS4', 'PS5', 'PS6', 'EPS1', 'FS1', 'FS2', 'TS1', 'TS2', 'TS3', 'TS4', 'VS1', 'CE', 'CP', 'SE']\n",
    "target_list = ['cooler cond.', 'valve cond.', 'pump leakage', 'hydraulic accum.', 'stable flag' ]\n",
    "\n",
    "target_state_dict['stable flag']=[0,1 ]\n",
    "target_state_dict['pump leakage']=[0,1,2]\n",
    "target_state_dict['cooler cond.']=[3,20,100]\n",
    "target_state_dict['valve cond.']=[73,80,90,100]\n",
    "target_state_dict['hydraulic accum.']=[90,100,115,130]\n",
    "\n",
    "\n",
    "leakage_key_list=['c100_h130_pl0_v100_s0', 'c100_h130_pl1_v100_s0', 'c100_h130_pl2_v100_s0']\n",
    "valve_key_list=['c100_h130_pl0_v100_s0', 'c100_h130_pl0_v90_s0', 'c100_h130_pl0_v80_s0', 'c100_h130_pl0_v73_s0' ]\n",
    "hydraulic_key_list=['c100_h130_pl0_v100_s0', 'c100_h115_pl0_v100_s0', 'c100_h100_pl0_v100_s0', 'c100_h90_pl0_v100_s0']\n",
    "stable_key_list=['c100_h130_pl0_v100_s0', 'c100_h130_pl0_v100_s1']\n",
    "cooler_key_list=['c100_h130_pl0_v100_s0', 'c20_h130_pl0_v100_s0', 'c3_h130_pl0_v100_s0']\n",
    "\n",
    "all_key_list=['c100_h130_pl0_v100_s0', 'c100_h130_pl1_v100_s0', 'c100_h130_pl2_v100_s0',\n",
    "              'c100_h130_pl0_v100_s0', 'c100_h130_pl0_v90_s0', 'c100_h130_pl0_v80_s0', 'c100_h130_pl0_v73_s0',\n",
    "              'c100_h130_pl0_v100_s0', 'c100_h115_pl0_v100_s0', 'c100_h100_pl0_v100_s0', 'c100_h90_pl0_v100_s0',\n",
    "              'c100_h130_pl0_v100_s0', 'c100_h130_pl0_v100_s1',\n",
    "              'c100_h130_pl0_v100_s0', 'c20_h130_pl0_v100_s0', 'c3_h130_pl0_v100_s0'\n",
    "             ]\n",
    "\n",
    "train_key = ['c100_h130_pl0_v100_s0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_dict(target_list, target_filename, path_to_file):\n",
    "    #read in target values from target file\n",
    "    \n",
    "    target_dict = {}\n",
    "    target_dict = {key: [] for key in target_list}\n",
    "    target_file = os.path.join(path_to_file, target_filename)\n",
    "\n",
    "    with open(target_file) as f:\n",
    "        for line in f:\n",
    "            value = line.split()\n",
    "            for i in range(0, len(value)):\n",
    "                #value_list =[]\n",
    "                #for j in range(0,6000):\n",
    "                #    value_list.append(int(value[i]))\n",
    "            \n",
    "                #print(len(value_list))\n",
    "                target_dict[target_list[i]].append(int(value[i]))\n",
    "                \n",
    "    return target_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_dict(feature_list, path_to_file):\n",
    "    for f in feature_list:\n",
    "        file_name = os.path.join(path_to_file, \"%s.txt\" %(f)) \n",
    "        \n",
    "        with open(file_name) as file:\n",
    "            #print(f)\n",
    "            feature_dict[f]= [[float(digit) for digit in line.split()] for line in file]\n",
    "    \n",
    "    return feature_dict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_tensor(target_dict, target_list, feature_list):\n",
    "    #create target tensor\n",
    "    #print(len(target_dict[target_list[0]]))\n",
    "\n",
    "    n_trows = len(target_dict[target_list[0]])\n",
    "    n_tcolumns = len(feature_dict[feature_list[0]][0])\n",
    "    n_targets = len(target_list)\n",
    "    #print(\"Dimensions: %s, %s, %s\" %(n_trows, n_tcolumns, n_targets))\n",
    "\n",
    "    target_2d = np.zeros((2205,6000))\n",
    "    target_value_list = list(target_dict.values())\n",
    "\n",
    "\n",
    "    for target in range(0, n_targets):\n",
    "        for row in range(0, n_trows):\n",
    "                     for column in range(0,n_tcolumns):\n",
    "                         target_2d[row][column]=target_value_list[target][row]\n",
    "\n",
    "        if target == 0:\n",
    "            target_tensor= np.copy(target_2d)\n",
    "        else:\n",
    "            target_tensor = np.dstack((target_tensor,target_2d))\n",
    "                    \n",
    "\n",
    "    #print(\"target_sensor shape: \" + str(target_tensor.shape))\n",
    "    return target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_tensor(feature_dict, feature_list):\n",
    "    #create feature tensor\n",
    "    \n",
    "    n_frows = len(feature_dict[feature_list[0]])\n",
    "    n_fcolumns = len(feature_dict[feature_list[0]][0])\n",
    "    n_features = len(feature_list)\n",
    "\n",
    "    feature_2d = np.zeros((n_frows,n_fcolumns))\n",
    "    #feature_tensor = np.zeros((n_frows, n_fcolumns, n_features))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for feature in range(0, n_features):\n",
    "        for row in range(0, n_frows):\n",
    "            #for column in range(0,n_tcolumns):\n",
    "            for column in range(0,n_fcolumns):\n",
    "                if len(feature_dict[feature_list[feature]][row]) == n_fcolumns:\n",
    "                    #print(feature_dict[feature_list[feature]][row][column])\n",
    "                    feature_2d[row][column]=feature_dict[feature_list[feature]][row][column]\n",
    "                else:\n",
    "                    padding =  n_fcolumns - len(feature_dict[feature_list[feature]][row])\n",
    "                    feature_dict[feature_list[feature]][row] = np.pad(feature_dict[feature_list[feature]][row], (0, padding), 'constant')\n",
    "                    feature_2d[row][column]=feature_dict[feature_list[feature]][row][column]\n",
    "\n",
    "        if feature == 0:\n",
    "            feature_tensor = np.copy(feature_2d)\n",
    "            #print(feature_tensor)\n",
    "        else:\n",
    "            feature_tensor = np.dstack((feature_tensor,feature_2d))\n",
    "            #print(feature_tensor)\n",
    "      \n",
    "    #print(\"feature tensor shape: \" + str(feature_tensor.shape))\n",
    "    return feature_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_feature_and_target_tensor(feature_tensor, target_tensor):\n",
    "    #Combine (scaled) feature and target Tensor\n",
    "    combined_tensor = np.dstack((feature_tensor, target_tensor))\n",
    "    #scaled_combined_tensor = np.dstack((scaled_feature_tensor, target_tensor))\n",
    "    \n",
    "    #print(combined_tensor.shape)\n",
    "    \n",
    "    return combined_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_of_target_dict(target_dict, target_name):\n",
    "    #get index of targets\n",
    "    \n",
    "    target_index = {k:i for i,k in enumerate(target_dict.keys())}\n",
    "    #print(\"Index values: \" + str(target_name))\n",
    "    \n",
    "    index = len(feature_tensor[0,0,:]) + target_index[target_name]\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_dict(target_dict):\n",
    "\n",
    "    ix_leakage = get_index_of_target_dict(target_dict, 'pump leakage')\n",
    "    ix_stable = get_index_of_target_dict(target_dict, 'stable flag')\n",
    "    ix_hyd = get_index_of_target_dict(target_dict, 'hydraulic accum.')\n",
    "    ix_cooler = get_index_of_target_dict(target_dict, 'cooler cond.')\n",
    "    ix_valve = get_index_of_target_dict(target_dict, 'valve cond.')\n",
    "    #print(ix_leakage, ix_stable, ix_hyd)\n",
    "    #(combined_tensor[:,1,ix_stable]==0)\n",
    "\n",
    "\n",
    "    stable_list = [0,1]\n",
    "    valve_list = [73,80,90,100]\n",
    "    leakage_list = [0,1,2]\n",
    "    hydraulic_list = [90,100,115,130]\n",
    "    cooler_list= [3,20,100]\n",
    "\n",
    "    \n",
    "    tensor_dict = {}\n",
    "    scaled_tensor_dict = {}\n",
    "    \n",
    "    counter = 1\n",
    "    for stable in stable_list:\n",
    "        #print(stable)\n",
    "        for valve in valve_list:\n",
    "            for leakage in leakage_list:\n",
    "                for hyd in hydraulic_list:\n",
    "                    for cooler in cooler_list:\n",
    "                        \n",
    "                        \n",
    "                        tensor = combined_tensor[(combined_tensor[:,1,ix_leakage]==leakage)&\n",
    "                                    (combined_tensor[:,1,ix_stable]==stable)&\n",
    "                                    (combined_tensor[:,1,ix_hyd]==hyd)& \n",
    "                                    (combined_tensor[:,1,ix_cooler]==cooler)& \n",
    "                                    (combined_tensor[:,1,ix_valve]==valve)]\n",
    "\n",
    "    \n",
    "                        if len(tensor) != 0:\n",
    "                            #print(\"groeser null\")\n",
    "                            tensor_dict['c%s_h%s_pl%s_v%s_s%s' %(cooler,hyd,leakage,valve,stable)] = tensor\n",
    "\n",
    "\n",
    "                        \n",
    "                            #scaled_tensor_dict['c%s_h%s_pl%s_v%s_s%s' %(cooler,hyd,leakage,valve,stable)] = scaled_combined_tensor[(scaled_combined_tensor[:,1,ix_leakage]==leakage)&\n",
    "                            #                                                                                            (combined_tensor[:,1,ix_stable]==stable) & \n",
    "                            #                                                                                            (combined_tensor[:,1,ix_hyd]==hyd)&\n",
    "                            #                                                                                            (combined_tensor[:,1,ix_cooler]==cooler)&\n",
    "                            #                                                                                            (combined_tensor[:,1,ix_valve]==valve)]\n",
    "                            #print(\"tensor_%s_%s_%s_%s_%s\" % (cooler, hyd, leakage, valve, stable))\n",
    "                            #print(tensor_dict['c%s_h%s_pl%s_v%s_s%s' %(cooler,hyd,leakage,valve,stable)].shape)\n",
    "                            #print(\" Set: \" + str(counter))\n",
    "                            counter +=1\n",
    "                    \n",
    "\n",
    "    #print(\"Number of keys/combinations: \" + str(len(tensor_dict.keys())))\n",
    "    return tensor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_tensor_columns_to_rows(tensor_dict):\n",
    "#stacked columns t rows\n",
    "    rows, columns, features  = tensor_dict.shape\n",
    "        \n",
    "    #print(rows, columns, features)\n",
    "    for feature in range(0,features):\n",
    "                        \n",
    "            \n",
    "        for row in range(0, rows):\n",
    "        \n",
    "            if row == 0:\n",
    "                stacked_tensor = np.copy(tensor_dict[row,:,feature])\n",
    "                \n",
    "            else:\n",
    "                stacked_tensor = np.hstack((stacked_tensor,tensor_dict[row,:,feature]))\n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "        if feature == 0:\n",
    "            full_stacked_tensor = np.copy(stacked_tensor.reshape(len(stacked_tensor),1))\n",
    "            \n",
    "        else:\n",
    "            full_stacked_tensor = np.hstack((full_stacked_tensor, stacked_tensor.reshape(len(stacked_tensor),1)))\n",
    "    \n",
    "    #print(\"full stacked tensor shape: \" + str(full_stacked_tensor.shape))\n",
    "    return full_stacked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation_multi_step(sequences, look_back, look_ahead):\n",
    "    # split a multivariate sequence one step into samples\n",
    "    \n",
    "    X, y = list(), list()\n",
    "    print(\"look_back: \" + str(look_back))\n",
    "    print(\"look_ahead: \" + str(look_ahead))\n",
    "    print(\"Stacked Sequence Shape: \" + str(sequences.shape)) \n",
    "    \n",
    "    \n",
    "    for i in range(len(sequences)):\n",
    "        #print(i)\n",
    "        #rint(len(sequepnces))\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + look_back\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix+1 > len(sequences) :\n",
    "            break\n",
    "        elif end_ix+look_ahead > len(sequences):\n",
    "            break\n",
    "        #gather input and output parts of the pattern\n",
    "        #print(sequences[i:end_ix])\n",
    "        #print(sequences[end_ix:end_ix+look_ahead])\n",
    "                \n",
    "        seq_x = sequences[i:end_ix]\n",
    "        seq_y = sequences[end_ix:end_ix+look_ahead]\n",
    "        #print(\"x sequence length: \" + str(len(seq_x)))\n",
    "        #print(\"y sequence length: \" + str(len(seq_y)))\n",
    "        \n",
    "   \n",
    "        #seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    print(\"X look_back: \" + str(array(X).shape))\n",
    "    print(\"y look_ahead: \" + str(array(y).shape))\n",
    "    \n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_for_LSTM(train_tensor, look_back, look_ahead):\n",
    "    \n",
    "    full_stacked_train_tensor = stack_tensor_columns_to_rows(train_tensor)\n",
    "    \n",
    "    X, y = data_preparation_multi_step(full_stacked_train_tensor, look_back, look_ahead)\n",
    "    \n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_stacked_tensor_dict(tensor_dict):\n",
    "    \n",
    "    full_stacked_tensor_dict = {}  \n",
    "    i = 1\n",
    "    \n",
    "    for key in tensor_dict.keys():\n",
    "        full_stacked_tensor_dict[key] = stack_tensor_columns_to_rows(tensor_dict[key][:,:,0:17])\n",
    "        #print(i)\n",
    "        i +=1\n",
    "        \n",
    "    return full_stacked_tensor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Xy_key_list_stacked_tensor(tensor_dict, key_list, look_back, look_ahead):\n",
    "\n",
    "    X_key_list_stacked_tensor_dict = {}\n",
    "    y_key_list_stacked_tensor_dict = {}\n",
    "\n",
    "    #for key in tensor_dict.keys():\n",
    "    for key in key_list:\n",
    "        #print(key)\n",
    "        X_key_list_stacked_tensor_dict[key], y_key_list_stacked_tensor_dict[key] = transform_data_for_LSTM(tensor_dict[key][:,:,0:17], \n",
    "                                                                                       look_back, look_ahead)\n",
    "        #X_scaled_full_staked_tensor[key], y_scaled_full_staked_tensor[key] = transform_data_for_LSTM(scaled_tensor_dict[key][:,:,0:17], n_steps=n_steps)\n",
    "\n",
    "    #print(X_full_staked_tensor[key].shape, y_full_staked_tensor[key].shape)\n",
    "\n",
    "    return X_key_list_stacked_tensor_dict, y_key_list_stacked_tensor_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Xy_key_list_scaled_stacked_tensor(X_key_list_stacked_tensor_dict, y_key_list_stacked_tensor_dict):\n",
    "\n",
    "    X_scaled_key_list_stacked_tensor_dict = {}\n",
    "    y_scaled_key_list_stacked_tensor_dict = {}\n",
    "    xscaler = MinMaxScaler()\n",
    "    yscaler = MinMaxScaler()\n",
    "    \n",
    "    for key in X_key_list_stacked_tensor_dict.keys():\n",
    "        \n",
    "        #print(key)\n",
    "        #only for 2d tensoren\n",
    "        #y_scaled_full_staked_tensor[key] = yscaler.fit_transform(y_full_staked_tensor[key])\n",
    "        \n",
    "        for feature in range(0,17):\n",
    "            #print(feature)\n",
    "            X_scaler_tensor = xscaler.fit_transform(X_key_list_stacked_tensor_dict[key][:,:,feature])\n",
    "            y_scaler_tensor = yscaler.fit_transform(y_key_list_stacked_tensor_dict[key][:,:,feature])\n",
    "            #y_scaler_tensor = scaler.fit_transform(y_full_staked_tensor[key][:,:])\n",
    "            if feature == 0:\n",
    "                X_key_list_scaled_feature_tensor_dict = np.copy(X_scaler_tensor)\n",
    "                y_key_list_scaled_feature_tensor_dict = np.copy(y_scaler_tensor)\n",
    "                #print(feature_tensor)\n",
    "            else:\n",
    "                X_key_list_scaled_feature_tensor_dict = np.dstack((X_key_list_scaled_feature_tensor_dict,\n",
    "                                                                   X_scaler_tensor))\n",
    "                y_key_list_scaled_feature_tensor_dict = np.dstack((y_key_list_scaled_feature_tensor_dict, \n",
    "                                                                   y_scaler_tensor))\n",
    "    \n",
    "        X_scaled_key_list_stacked_tensor_dict[key]= X_key_list_scaled_feature_tensor_dict\n",
    "        y_scaled_key_list_stacked_tensor_dict[key]= y_key_list_scaled_feature_tensor_dict\n",
    "    \n",
    "    \n",
    "    return X_scaled_key_list_stacked_tensor_dict, y_scaled_key_list_stacked_tensor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Dict Keys: dict_keys(['pump leakage', 'cooler cond.', 'stable flag', 'valve cond.', 'hydraulic accum.'])\n",
      "Feature Dict Keys: dict_keys(['PS2', 'FS1', 'PS1', 'PS4', 'PS3', 'PS5', 'PS6', 'VS1', 'TS1', 'TS4', 'CE', 'FS2', 'SE', 'CP', 'TS3', 'EPS1', 'TS2'])\n",
      "Target Tensor Shape: (2205, 6000, 5)\n",
      "Feature Tensor Shape: (2205, 6000, 17)\n",
      "Combined Tensor Shape: (2205, 6000, 22)\n",
      "Tensor dict keys: dict_keys(['c100_h115_pl1_v100_s1', 'c3_h100_pl2_v73_s0', 'c100_h100_pl0_v73_s0', 'c20_h115_pl1_v80_s0', 'c20_h115_pl2_v100_s0', 'c20_h130_pl2_v100_s1', 'c100_h90_pl0_v100_s0', 'c20_h130_pl0_v80_s0', 'c100_h100_pl2_v73_s0', 'c20_h130_pl2_v80_s0', 'c20_h90_pl1_v73_s0', 'c100_h100_pl1_v80_s0', 'c3_h90_pl0_v90_s0', 'c3_h115_pl0_v90_s0', 'c3_h90_pl1_v73_s0', 'c100_h90_pl2_v100_s1', 'c20_h90_pl2_v80_s0', 'c100_h115_pl1_v100_s0', 'c20_h130_pl2_v73_s0', 'c3_h115_pl2_v90_s0', 'c3_h115_pl1_v73_s0', 'c100_h90_pl2_v73_s0', 'c100_h100_pl2_v90_s0', 'c20_h90_pl0_v73_s0', 'c20_h130_pl1_v90_s0', 'c3_h100_pl2_v80_s0', 'c100_h100_pl0_v100_s1', 'c3_h130_pl0_v73_s0', 'c20_h90_pl0_v100_s1', 'c20_h100_pl1_v73_s0', 'c20_h115_pl1_v90_s0', 'c20_h130_pl0_v90_s0', 'c3_h130_pl1_v100_s0', 'c3_h100_pl0_v100_s1', 'c100_h90_pl1_v80_s0', 'c3_h115_pl2_v80_s0', 'c3_h130_pl0_v80_s0', 'c3_h130_pl1_v73_s0', 'c100_h90_pl1_v100_s0', 'c20_h100_pl1_v90_s0', 'c100_h115_pl2_v100_s0', 'c20_h100_pl0_v100_s1', 'c20_h100_pl0_v90_s0', 'c3_h130_pl1_v80_s0', 'c3_h100_pl2_v90_s0', 'c100_h100_pl1_v90_s0', 'c100_h100_pl2_v100_s0', 'c100_h115_pl0_v100_s0', 'c20_h115_pl2_v73_s0', 'c3_h100_pl1_v73_s0', 'c3_h90_pl1_v100_s1', 'c3_h90_pl2_v80_s0', 'c20_h100_pl2_v100_s0', 'c20_h100_pl1_v100_s1', 'c100_h115_pl0_v90_s0', 'c100_h100_pl0_v80_s0', 'c3_h90_pl1_v100_s0', 'c3_h115_pl1_v80_s0', 'c20_h115_pl0_v100_s0', 'c20_h90_pl0_v100_s0', 'c100_h115_pl2_v90_s0', 'c20_h100_pl1_v80_s0', 'c20_h130_pl1_v73_s0', 'c100_h130_pl0_v73_s0', 'c3_h100_pl0_v100_s0', 'c20_h130_pl1_v80_s0', 'c20_h90_pl2_v73_s0', 'c3_h130_pl0_v100_s0', 'c3_h115_pl2_v100_s1', 'c20_h115_pl1_v73_s0', 'c100_h115_pl0_v80_s0', 'c20_h100_pl0_v100_s0', 'c100_h100_pl1_v100_s1', 'c3_h130_pl2_v80_s0', 'c3_h90_pl0_v80_s0', 'c3_h130_pl1_v100_s1', 'c3_h100_pl0_v80_s0', 'c100_h130_pl2_v100_s1', 'c100_h90_pl2_v100_s0', 'c3_h115_pl2_v73_s0', 'c3_h100_pl2_v100_s1', 'c3_h130_pl2_v90_s0', 'c3_h90_pl0_v73_s0', 'c20_h90_pl1_v90_s0', 'c100_h100_pl1_v100_s0', 'c100_h130_pl0_v90_s0', 'c3_h100_pl1_v100_s0', 'c20_h130_pl0_v100_s1', 'c3_h130_pl0_v100_s1', 'c3_h130_pl2_v73_s0', 'c100_h90_pl2_v90_s0', 'c100_h130_pl2_v80_s0', 'c20_h130_pl0_v73_s0', 'c20_h115_pl0_v73_s0', 'c3_h115_pl1_v100_s0', 'c20_h90_pl1_v100_s1', 'c3_h130_pl0_v90_s0', 'c100_h90_pl2_v80_s0', 'c3_h115_pl2_v100_s0', 'c100_h100_pl0_v100_s0', 'c3_h90_pl1_v80_s0', 'c20_h90_pl2_v100_s0', 'c3_h115_pl0_v100_s1', 'c100_h130_pl2_v73_s0', 'c3_h100_pl1_v80_s0', 'c100_h115_pl2_v80_s0', 'c3_h100_pl1_v90_s0', 'c3_h100_pl0_v73_s0', 'c20_h100_pl0_v73_s0', 'c20_h130_pl1_v100_s1', 'c3_h115_pl1_v100_s1', 'c20_h130_pl2_v90_s0', 'c3_h115_pl0_v100_s0', 'c3_h130_pl1_v90_s0', 'c100_h115_pl0_v73_s0', 'c20_h100_pl2_v80_s0', 'c100_h90_pl1_v100_s1', 'c3_h130_pl2_v100_s0', 'c20_h100_pl2_v90_s0', 'c100_h130_pl2_v100_s0', 'c20_h90_pl2_v90_s0', 'c100_h90_pl1_v73_s0', 'c3_h90_pl0_v100_s0', 'c3_h90_pl1_v90_s0', 'c3_h90_pl2_v100_s0', 'c100_h100_pl0_v90_s0', 'c3_h115_pl1_v90_s0', 'c20_h90_pl2_v100_s1', 'c20_h115_pl1_v100_s0', 'c100_h130_pl1_v90_s0', 'c100_h115_pl0_v100_s1', 'c3_h90_pl2_v73_s0', 'c100_h90_pl0_v73_s0', 'c20_h115_pl2_v100_s1', 'c20_h115_pl0_v80_s0', 'c20_h90_pl1_v80_s0', 'c100_h100_pl2_v100_s1', 'c100_h130_pl1_v100_s0', 'c20_h115_pl0_v90_s0', 'c100_h115_pl1_v90_s0', 'c100_h90_pl0_v80_s0', 'c100_h90_pl0_v90_s0', 'c20_h115_pl2_v80_s0', 'c20_h130_pl0_v100_s0', 'c3_h115_pl0_v73_s0', 'c3_h100_pl1_v100_s1', 'c100_h115_pl1_v80_s0', 'c3_h100_pl2_v100_s0', 'c3_h90_pl2_v100_s1', 'c100_h90_pl0_v100_s1', 'c3_h130_pl2_v100_s1', 'c20_h115_pl2_v90_s0', 'c100_h115_pl1_v73_s0', 'c3_h90_pl0_v100_s1', 'c100_h130_pl1_v100_s1', 'c3_h90_pl2_v90_s0', 'c100_h115_pl2_v100_s1', 'c20_h100_pl2_v73_s0', 'c100_h130_pl0_v100_s0', 'c20_h130_pl2_v100_s0', 'c20_h100_pl2_v100_s1', 'c20_h115_pl0_v100_s1', 'c100_h130_pl2_v90_s0', 'c20_h100_pl1_v100_s0', 'c100_h90_pl1_v90_s0', 'c3_h100_pl0_v90_s0', 'c20_h90_pl1_v100_s0', 'c3_h115_pl0_v80_s0', 'c100_h130_pl1_v80_s0', 'c100_h130_pl0_v80_s0', 'c20_h130_pl1_v100_s0', 'c100_h130_pl0_v100_s1', 'c20_h90_pl0_v90_s0', 'c20_h115_pl1_v100_s1', 'c100_h115_pl2_v73_s0', 'c100_h100_pl1_v73_s0', 'c100_h130_pl1_v73_s0', 'c20_h100_pl0_v80_s0', 'c20_h90_pl0_v80_s0', 'c100_h100_pl2_v80_s0'])\n",
      "Number of State combinations : 180\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_dict = create_target_dict(target_list, target_filename, path_to_file)\n",
    "print(\"Target Dict Keys: \" + str(target_dict.keys()))\n",
    "feature_dict = create_feature_dict(feature_list, path_to_file)\n",
    "print(\"Feature Dict Keys: \" + str(feature_dict.keys()))\n",
    "\n",
    "target_tensor = create_target_tensor(target_dict, target_list, feature_list)\n",
    "print(\"Target Tensor Shape: \" +  str(target_tensor.shape))\n",
    "feature_tensor = create_feature_tensor(feature_dict, feature_list)\n",
    "print(\"Feature Tensor Shape: \" +  str(feature_tensor.shape))\n",
    "combined_tensor = combine_feature_and_target_tensor(feature_tensor, target_tensor)\n",
    "print(\"Combined Tensor Shape: \" +  str(combined_tensor.shape))\n",
    "\n",
    "\n",
    "tensor_dict = create_tensor_dict(target_dict)\n",
    "print(\"Tensor dict keys: \" + str(tensor_dict.keys()))\n",
    "print(\"Number of State combinations : \" + str(len(tensor_dict.keys())))\n",
    "\n",
    "full_stacked_tensor = stack_tensor_columns_to_rows(tensor_dict[train_key[0]])\n",
    "full_stacked_tensor_dict = get_full_stacked_tensor_dict(tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look_back: 20\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59980, 20, 17)\n",
      "y look_ahead: (59980, 1, 17)\n",
      "(59980, 20, 17)\n",
      "(59980, 1, 17)\n",
      "59980\n"
     ]
    }
   ],
   "source": [
    "X_key_list_stacked_tensor_dict, y_key_list_stacked_tensor_dict = create_Xy_key_list_stacked_tensor(tensor_dict, train_key, look_back=20, look_ahead=1)\n",
    "\n",
    "\n",
    "X_scaled_key_list_stacked_tensor_dict, y_scaled_key_list_stacked_tensor_dict = create_Xy_key_list_scaled_stacked_tensor(X_key_list_stacked_tensor_dict, \n",
    "                                                                                                 y_key_list_stacked_tensor_dict)\n",
    "\n",
    "print(X_scaled_key_list_stacked_tensor_dict[train_key[0]].shape)\n",
    "print(y_scaled_key_list_stacked_tensor_dict[train_key[0]].shape)\n",
    "print(len(X_scaled_key_list_stacked_tensor_dict[train_key[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked LSTM Class and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_callbacks():\n",
    "\n",
    "        stacked_path_checkpoint = 'optimzed_lstm_parameter.keras'\n",
    "        stacked_callback_checkpoint = ModelCheckpoint(filepath=stacked_path_checkpoint,\n",
    "                                                      monitor='val_loss',\n",
    "                                                      verbose=1,\n",
    "                                                      save_weights_only=True,\n",
    "                                                      save_best_only=True)\n",
    "\n",
    "        stacked_callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                                        #patience=5,\n",
    "                                                        patience = 10,\n",
    "                                                        verbose=1)\n",
    "\n",
    "        stacked_callback_tensorboard = TensorBoard(log_dir='C:\\\\Users\\\\mooc\\\\Documents\\\\Bosch-MT\\\\Daten\\\\log',\n",
    "                                                   histogram_freq=0,\n",
    "                                                   write_graph=True)\n",
    "\n",
    "        stacked_callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                       #factor=0.5,\n",
    "                                                       #min_lr=1e-5,\n",
    "                                                       factor=0.1,\n",
    "                                                       min_lr=1e-7,\n",
    "                                                       #patience=3,\n",
    "                                                       patience =0,\n",
    "                                                       verbose=1)\n",
    "\n",
    "        stacked_callbacks = [stacked_callback_early_stopping,\n",
    "                             stacked_callback_checkpoint,\n",
    "                             stacked_callback_tensorboard,\n",
    "                             stacked_callback_reduce_lr]\n",
    "        \n",
    "        return stacked_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stackedLSTM(object):\n",
    "    \n",
    "    def __init__(self, var_units, var_epochs, var_batch_size, var_look_back, \n",
    "                 var_dropout_list, fix_features=1, var_layers=3, var_look_ahead=1, var_learning_rate=0.1,\n",
    "                 fix_loss='mse', fix_optimizer='adam', fix_hidden_activation='relu'):\n",
    "        \n",
    "        self.var_dropout_list = var_dropout_list\n",
    "        print(self.var_dropout_list)\n",
    "        \n",
    "        self.var_learning_rate = var_learning_rate\n",
    "        #self.var_dropout = var_dropout\n",
    "        self.var_batch_size = var_batch_size\n",
    "        self.var_epochs = var_epochs\n",
    "        self.var_look_back = var_look_back\n",
    "        self.var_units= var_units\n",
    "        self.var_look_ahead = var_look_ahead\n",
    "        self.var_n_hidden = var_layers - 2\n",
    "        self.var_layers = var_layers\n",
    "        \n",
    "        self.fix_model = Sequential()\n",
    "        self.fix_features = fix_features\n",
    "        self.fix_loss = fix_loss\n",
    "        self.fix_optimizer = fix_optimizer\n",
    "        self.fix_hidden_activation = fix_hidden_activation\n",
    "        self.fix_init = RandomUniform(minval=-0.05, maxval=0.05)\n",
    "    \n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        \n",
    "       \n",
    "        print(\"dropout l0: \" + str(self.var_dropout_list[0]))\n",
    "        self.fix_model.add(LSTM(units=self.var_units,\n",
    "                                input_shape=(self.var_look_back, self.fix_features),\n",
    "                                activation=self.fix_hidden_activation,\n",
    "                                return_sequences= True if self.var_n_hidden>1 else False))\n",
    "        self.fix_model.add(Dropout(self.var_dropout_list[0]))\n",
    "        \n",
    "        \n",
    "            \n",
    "        #add hidden layers\n",
    "        for i in range(2,self.var_n_hidden+1):\n",
    "            return_sequences = True\n",
    "            if i == self.var_n_hidden:\n",
    "                return_sequences = False\n",
    "            print(\"dropout:\" + str(self.var_dropout_list[i-1]))\n",
    "            self.fix_model.add(LSTM(self.var_layers, return_sequences=return_sequences))\n",
    "            self.fix_model.add(Dropout(self.var_dropout_list[i-1]))\n",
    "        \n",
    "        \n",
    "            #add dense layer with output dimension to get output for one time_step\n",
    "        self.fix_model.add(Dense(output_dim=self.fix_features,\n",
    "                          activation='linear',\n",
    "                          kernel_initializer=self.fix_init))\n",
    "\n",
    "            #Repeat for look_ahead steps to get outputs for look_ahead timesteps.\n",
    "        self.fix_model.add(RepeatVector(self.var_look_ahead))\n",
    "\n",
    "            #add activation\n",
    "        #self.fix_model.add(Activation(\"linear\"))\n",
    "\n",
    "            #compile model and print summary\n",
    "        #self.fix_model.compile(loss=self.fix_loss, optimizer= Adam(lr=self.var_learning_rate,decay=.99))\n",
    "        self.fix_model.compile(loss=self.fix_loss, optimizer= Adam(lr=self.var_learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False))\n",
    "            \n",
    "        self.fix_model.summary()\n",
    "        \n",
    "        return self.fix_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(fix_model, X_train, y_train, X_val, y_val, var_batch_size, var_epochs):\n",
    "\n",
    "    history = fix_model.fit(X_train, y_train,\n",
    "                  epochs=var_epochs, verbose=1, \n",
    "                  validation_data=(X_val, y_val),\n",
    "                  shuffle = False,\n",
    "                  callbacks = set_callbacks(),\n",
    "                  batch_size=var_batch_size\n",
    "                  )\n",
    "        \n",
    "    #plot loss and val_loss    \n",
    "    plt.plot(histor.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(\"train and validatio loss\")\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()\n",
    "        \n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(fix_model, X_val, y_val, var_batch_size):\n",
    "    \n",
    "    validation_loss = fix_model.evaluate(X_val, y_val, var_batch_size, verbose=1)\n",
    "    \n",
    "    return validation_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization objective function\n",
    "def lstm_ojective_function(param):\n",
    "    \n",
    "    \n",
    "    param = param.flatten()\n",
    "    train_key=['c100_h130_pl0_v100_s0']\n",
    "    val_key=['c100_h130_pl1_v100_s0']\n",
    "    #test_key\n",
    "    \n",
    "    number_param = len(param)\n",
    "    print(\"number params: \" + str(param))\n",
    "    \n",
    "        \n",
    "    fix_optimizers = 'adam'\n",
    "    fix_loss = 'mse'\n",
    "    fix_shuffle = False\n",
    "    fix_patience = 3\n",
    "    fix_features = 1\n",
    "    \n",
    "    var_dropout_list = []\n",
    "        \n",
    "    var_learning_rate = float(param[-6])\n",
    "    var_units= int(param[-5])\n",
    "    var_batch_size = int(param[-4])\n",
    "    var_look_back = int(param[-3])\n",
    "    #var_look_ahead = int(param[5])\n",
    "    #var_look_ahead = int(param[-3])\n",
    "    var_look_ahead = 1\n",
    "    var_epochs = int(param[-2])\n",
    "    var_layers = int(param[-1])\n",
    "    print(\"number_layers: \" + str(var_layers)) \n",
    "    \n",
    "    #for layer in var_layers - 1:\n",
    "    for layer in range(0,var_layers-1):\n",
    "        print(param[layer])\n",
    "        var_dropout_list.append(param[layer])\n",
    "        \n",
    "    \n",
    "        \n",
    "    print(\"Using HyperParams. units:%s, batch_size:%s, look_back:%s, look_ahead:%s, layers:%s, epochs:%s, learning_rate:%s\" % \n",
    "                 (var_units, var_batch_size, var_look_back, var_look_ahead, var_layers, var_epochs, var_learning_rate))\n",
    "    \n",
    "    print(\"dropout_list:\" + str(var_dropout_list))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"create data Sequence with look_ahead and look_back...\")\n",
    "    #train data\n",
    "    X_key_list_stacked_tensor_dict_train, y_key_list_stacked_tensor_dict_train = create_Xy_key_list_stacked_tensor(tensor_dict, train_key,\n",
    "                                                                                                       var_look_back, \n",
    "                                                                                                       var_look_ahead)\n",
    "\n",
    "    #val data\n",
    "    X_key_list_stacked_tensor_dict_val, y_key_list_stacked_tensor_dict_val = create_Xy_key_list_stacked_tensor(tensor_dict, val_key,\n",
    "                                                                                                       var_look_back, \n",
    "                                                                                                       var_look_ahead)\n",
    "\n",
    "    \n",
    "    \n",
    "    #train dta\n",
    "    X_scaled_key_list_stacked_tensor_dict_train, y_scaled_key_list_stacked_tensor_dict_train = create_Xy_key_list_scaled_stacked_tensor(X_key_list_stacked_tensor_dict_train, \n",
    "                                                                                                                            y_key_list_stacked_tensor_dict_train)\n",
    "\n",
    "    \n",
    "    X_scaled_key_list_stacked_tensor_dict_val, y_scaled_key_list_stacked_tensor_dict_val = create_Xy_key_list_scaled_stacked_tensor(X_key_list_stacked_tensor_dict_val, \n",
    "                                                                                                                            y_key_list_stacked_tensor_dict_val)\n",
    "    \n",
    "    #if fixed == False:\n",
    "    print(\"initilize Layer variable length LSTM Model...\")\n",
    "    stacked_lstm_model = stackedLSTM(var_units=var_units, var_epochs=var_epochs, \n",
    "                                         var_batch_size=var_batch_size, \n",
    "                                         var_look_back=var_look_back, \n",
    "                                         var_look_ahead=var_look_ahead, \n",
    "                                         var_layers=var_layers, \n",
    "                                         var_dropout_list=var_dropout_list,\n",
    "                                         var_learning_rate = var_learning_rate \n",
    "                                            )\n",
    "\n",
    "    print(\"build model...\")\n",
    "    lstm_model = stacked_lstm_model.build_model()\n",
    "    \n",
    "    \n",
    "    print(\"train model...\")\n",
    "    train_model(lstm_model, X_train=X_scaled_key_list_stacked_tensor_dict_train[train_key[0]][:,:,0:fix_features], \n",
    "                                 y_train=y_scaled_key_list_stacked_tensor_dict_train[train_key[0]][:,:,0:fix_features], \n",
    "                                 X_val=X_scaled_key_list_stacked_tensor_dict_val[val_key[0]][:,:,0:fix_features], \n",
    "                                 y_val=y_scaled_key_list_stacked_tensor_dict_val[val_key[0]][:,:,0:fix_features], \n",
    "                                 var_batch_size=var_batch_size,\n",
    "                                 var_epochs=var_epochs)\n",
    "    \n",
    "    print(\"evaluate model...\")\n",
    "    validation_loss = evaluate_model(lstm_model, \n",
    "                                     X_scaled_key_list_stacked_tensor_dict_val[val_key[0]][:,:,0:fix_features], \n",
    "                                     y_scaled_key_list_stacked_tensor_dict_val[val_key[0]][:,:,0:fix_features], \n",
    "                                     var_batch_size=var_batch_size)\n",
    "    \n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_bounds =[{'name': 'l1_dropout', 'type': 'continuous', 'domain': (0.2, 0.8)},\n",
    "                   {'name': 'l2_dropout', 'type': 'continuous', 'domain': (0.2, 0.8)},\n",
    "                   {'name': 'l3_dropout', 'type': 'continuous', 'domain': (0.2, 0.8)},\n",
    "                   {'name': 'l4_dropout', 'type': 'continuous', 'domain': (0.2, 0.8)},\n",
    "                   {'name': 'l5_dropout', 'type': 'continuous', 'domain': (0.2, 0.8)},\n",
    "                   {'name': 'learning_rate', 'type': 'continuous', 'domain': (0.00000001, 0.1)},\n",
    "                   {'name': 'units', 'type': 'discrete', 'domain': (32,64,128,256,512)},\n",
    "                   {'name': 'batch_size', 'type': 'discrete', 'domain': (1,8,16,32,64)},\n",
    "                   {'name': 'look_back', 'type': 'discrete', 'domain': (5,10,15,20,30,40,50)},\n",
    "                   #{'name': 'look_ahead', 'type': 'discrete', 'domain': (1,2,3,4,5)},\n",
    "                   {'name': 'epochs', 'type':'discrete', 'domain':(25,50,100,150,200)},\n",
    "                   {'name': 'layers', 'type':'discrete', 'domain':(3,4,5,6)}\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number params: [3.31771453e-01 7.75311515e-01 2.37466567e-01 4.70862638e-01\n",
      " 5.86012857e-01 8.36865419e-02 6.40000000e+01 8.00000000e+00\n",
      " 3.00000000e+01 3.00000000e+02 6.00000000e+00]\n",
      "number_layers: 6\n",
      "0.33177145274232045\n",
      "0.7753115146868892\n",
      "0.23746656688249462\n",
      "0.4708626384840908\n",
      "0.5860128566955825\n",
      "Using HyperParams. units:64, batch_size:8, look_back:30, look_ahead:1, layers:6, epochs:300, learning_rate:0.08368654186254541\n",
      "dropout_list:[0.33177145274232045, 0.7753115146868892, 0.23746656688249462, 0.4708626384840908, 0.5860128566955825]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 30\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59970, 30, 17)\n",
      "y look_ahead: (59970, 1, 17)\n",
      "look_back: 30\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59970, 30, 17)\n",
      "y look_ahead: (59970, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.33177145274232045, 0.7753115146868892, 0.23746656688249462, 0.4708626384840908, 0.5860128566955825]\n",
      "build model...\n",
      "dropout l0: 0.33177145274232045\n",
      "dropout:0.7753115146868892\n",
      "dropout:0.23746656688249462\n",
      "dropout:0.4708626384840908\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_71 (LSTM)               (None, 30, 64)            16896     \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_72 (LSTM)               (None, 30, 6)             1704      \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 30, 6)             0         \n",
      "_________________________________________________________________\n",
      "lstm_73 (LSTM)               (None, 30, 6)             312       \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 30, 6)             0         \n",
      "_________________________________________________________________\n",
      "lstm_74 (LSTM)               (None, 6)                 312       \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 7         \n",
      "_________________________________________________________________\n",
      "repeat_vector_27 (RepeatVect (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 19,231\n",
      "Trainable params: 19,231\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59970 samples, validate on 59970 samples\n",
      "Epoch 1/300\n",
      "59970/59970 [==============================] - 415s 7ms/step - loss: 0.0036 - val_loss: 0.1382\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13817, saving model to optimzed_lstm_parameter.keras\n",
      "Epoch 2/300\n",
      "59970/59970 [==============================] - 425s 7ms/step - loss: 0.0039 - val_loss: 0.1382\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.13817\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.008368654549121857.\n",
      "Epoch 3/300\n",
      "59970/59970 [==============================] - 421s 7ms/step - loss: 0.0331 - val_loss: 0.1381\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13817 to 0.13811, saving model to optimzed_lstm_parameter.keras\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0008368654176592827.\n",
      "Epoch 4/300\n",
      "59970/59970 [==============================] - 425s 7ms/step - loss: 0.0988 - val_loss: 0.1007\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13811 to 0.10075, saving model to optimzed_lstm_parameter.keras\n",
      "Epoch 5/300\n",
      "59970/59970 [==============================] - 430s 7ms/step - loss: 0.0976 - val_loss: 0.1007\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.10075 to 0.10075, saving model to optimzed_lstm_parameter.keras\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 8.368653943762183e-05.\n",
      "Epoch 6/300\n",
      "59970/59970 [==============================] - 433s 7ms/step - loss: 0.0971 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.10075 to 0.09503, saving model to optimzed_lstm_parameter.keras\n",
      "Epoch 7/300\n",
      "59970/59970 [==============================] - 427s 7ms/step - loss: 0.0954 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09503 to 0.09481, saving model to optimzed_lstm_parameter.keras\n",
      "Epoch 8/300\n",
      "59970/59970 [==============================] - 438s 7ms/step - loss: 0.0953 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09481 to 0.09479, saving model to optimzed_lstm_parameter.keras\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 8.368653652723879e-06.\n",
      "Epoch 9/300\n",
      "59970/59970 [==============================] - 426s 7ms/step - loss: 0.0950 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09479 to 0.09476, saving model to optimzed_lstm_parameter.keras\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 8.368653652723879e-07.\n",
      "Epoch 10/300\n",
      "59970/59970 [==============================] - 421s 7ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09476 to 0.09476, saving model to optimzed_lstm_parameter.keras\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 11/300\n",
      "59970/59970 [==============================] - 419s 7ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09476 to 0.09476, saving model to optimzed_lstm_parameter.keras\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 12/300\n",
      "59970/59970 [==============================] - 425s 7ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09476 to 0.09476, saving model to optimzed_lstm_parameter.keras\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 13/300\n",
      "59970/59970 [==============================] - 427s 7ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.09476 to 0.09476, saving model to optimzed_lstm_parameter.keras\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 14/300\n",
      "59970/59970 [==============================] - 427s 7ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.09476 to 0.09476, saving model to optimzed_lstm_parameter.keras\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 15/300\n",
      "59970/59970 [==============================] - 405s 7ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09476 to 0.09476, saving model to optimzed_lstm_parameter.keras\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 16/300\n",
      "59970/59970 [==============================] - 414s 7ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09476 to 0.09476, saving model to optimzed_lstm_parameter.keras\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 17/300\n",
      "59970/59970 [==============================] - 432s 7ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.09476 to 0.09476, saving model to optimzed_lstm_parameter.keras\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 18/300\n",
      "59970/59970 [==============================] - 433s 7ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.09476 to 0.09476, saving model to optimzed_lstm_parameter.keras\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 19/300\n",
      "59970/59970 [==============================] - 431s 7ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.09476 to 0.09476, saving model to optimzed_lstm_parameter.keras\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 20/300\n",
      "39752/59970 [==================>...........] - ETA: 2:04 - loss: 0.0989"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-42863bfe0864>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                                       \u001b[0minitial_design_numdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                                       \u001b[0macquisition_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'EI'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                                                       exact_feval = True)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\GPyOpt\\methods\\bayesian_optimization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, domain, constraints, cost_withGradients, model_type, X, Y, initial_design_numdata, initial_design_type, acquisition_type, normalize_Y, exact_feval, acquisition_optimizer_type, model_update_interval, evaluator_type, batch_size, num_cores, verbosity, verbosity_model, maximize, de_duplication, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitial_design_type\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0minitial_design_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitial_design_numdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_design_numdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_design_chooser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;31m# --- CHOOSE the model type. If an instance of a GPyOpt model is passed (possibly user defined), it is used.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\GPyOpt\\methods\\bayesian_optimization.py\u001b[0m in \u001b[0;36m_init_design_chooser\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_design\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitial_design_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitial_design_numdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m         \u001b[1;31m# Case 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\GPyOpt\\core\\task\\objective.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_procs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mf_evals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_evals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eval_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\GPyOpt\\core\\task\\objective.py\u001b[0m in \u001b[0;36m_eval_func\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mst_time\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mrlt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mf_evals\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf_evals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrlt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mcost_evals\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mst_time\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-bb67b465e957>\u001b[0m in \u001b[0;36mlstm_ojective_function\u001b[1;34m(param)\u001b[0m\n\u001b[0;32m     87\u001b[0m                                  \u001b[0my_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_scaled_key_list_stacked_tensor_dict_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_key\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfix_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                                  \u001b[0mvar_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                                  var_epochs=var_epochs)\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"evaluate model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-7565883ed23e>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(fix_model, X_train, y_train, X_val, y_val, var_batch_size, var_epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m                   \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                   \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                   \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                   )\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimized_lstm_model = GPyOpt.methods.BayesianOptimization(f=lstm_ojective_function, \n",
    "                                                      domain=parameter_bounds,\n",
    "                                                      initial_design_numdata = 1,\n",
    "                                                      acquisition_type='EI',\n",
    "                                                      exact_feval = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number params: [4.65189522e-01 4.50482260e-01 3.97387455e-01 5.05529877e-01\n",
      " 4.26971267e-01 4.21218352e-02 1.28000000e+02 6.40000000e+01\n",
      " 1.50000000e+01 2.00000000e+02 4.00000000e+00]\n",
      "number_layers: 4\n",
      "0.46518952183644996\n",
      "0.4504822601581006\n",
      "0.397387455377258\n",
      "Using HyperParams. units:128, batch_size:64, look_back:15, look_ahead:1, layers:4, epochs:200, learning_rate:0.04212183517076364\n",
      "dropout_list:[0.46518952183644996, 0.4504822601581006, 0.397387455377258]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 15\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59985, 15, 17)\n",
      "y look_ahead: (59985, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.46518952183644996, 0.4504822601581006, 0.397387455377258]\n",
      "build model...\n",
      "dropout l0: 0.46518952183644996\n",
      "dropout:0.4504822601581006\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 15, 128)           66560     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 4)                 2128      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 5         \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 68,693\n",
      "Trainable params: 68,693\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59985 samples, validate on 59985 samples\n",
      "Epoch 1/200\n",
      "59985/59985 [==============================] - 40s 660us/step - loss: 0.1651 - val_loss: 0.1302\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13020, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/200\n",
      "59985/59985 [==============================] - 40s 664us/step - loss: 0.1668 - val_loss: 0.1322\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.13020\n",
      "Epoch 3/200\n",
      "59985/59985 [==============================] - 45s 755us/step - loss: 0.1655 - val_loss: 0.1301\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13020 to 0.13006, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 4/200\n",
      "59985/59985 [==============================] - 42s 707us/step - loss: 0.1680 - val_loss: 0.1301\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13006\n",
      "Epoch 5/200\n",
      "59985/59985 [==============================] - 44s 731us/step - loss: 0.1641 - val_loss: 0.1306\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.13006\n",
      "Epoch 6/200\n",
      "59985/59985 [==============================] - 43s 724us/step - loss: 0.1658 - val_loss: 0.1327\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13006\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.004212183505296708.\n",
      "Epoch 7/200\n",
      "59985/59985 [==============================] - 42s 693us/step - loss: 0.1149 - val_loss: 0.1021\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.13006 to 0.10212, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 8/200\n",
      "59985/59985 [==============================] - 43s 725us/step - loss: 0.1150 - val_loss: 0.1028\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.10212\n",
      "Epoch 9/200\n",
      "59985/59985 [==============================] - 44s 732us/step - loss: 0.1155 - val_loss: 0.1029\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.10212\n",
      "Epoch 10/200\n",
      "59985/59985 [==============================] - 41s 679us/step - loss: 0.1161 - val_loss: 0.1032\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.10212\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00042121834121644497.\n",
      "Epoch 11/200\n",
      "59985/59985 [==============================] - 43s 717us/step - loss: 0.0986 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.10212 to 0.09501, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 12/200\n",
      "59985/59985 [==============================] - 43s 718us/step - loss: 0.0969 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09501 to 0.09501, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 13/200\n",
      "59985/59985 [==============================] - 41s 678us/step - loss: 0.0970 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.09501 to 0.09500, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 14/200\n",
      "59985/59985 [==============================] - 44s 734us/step - loss: 0.0970 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.09500 to 0.09499, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 4.21218341216445e-05.\n",
      "Epoch 15/200\n",
      "59985/59985 [==============================] - 44s 734us/step - loss: 0.0952 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09499 to 0.09491, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 16/200\n",
      "59985/59985 [==============================] - 42s 699us/step - loss: 0.0951 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09491 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 17/200\n",
      "59985/59985 [==============================] - 42s 707us/step - loss: 0.0951 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.09488 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 18/200\n",
      "59985/59985 [==============================] - 43s 718us/step - loss: 0.0951 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.212183557683602e-06.\n",
      "Epoch 19/200\n",
      "59985/59985 [==============================] - 43s 713us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.09486 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 20/200\n",
      "59985/59985 [==============================] - 44s 731us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 21/200\n",
      "59985/59985 [==============================] - 41s 684us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 4.2121837395825423e-07.\n",
      "Epoch 22/200\n",
      "59985/59985 [==============================] - 46s 765us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 23/200\n",
      "59985/59985 [==============================] - 43s 710us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 24/200\n",
      "59985/59985 [==============================] - 42s 696us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 25/200\n",
      "59985/59985 [==============================] - 42s 707us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 26/200\n",
      "59985/59985 [==============================] - 43s 711us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 27/200\n",
      "59985/59985 [==============================] - 42s 699us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 28/200\n",
      "59985/59985 [==============================] - 42s 695us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 29/200\n",
      "59985/59985 [==============================] - 43s 712us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 30/200\n",
      "59985/59985 [==============================] - 42s 695us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 31/200\n",
      "59985/59985 [==============================] - 46s 769us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 32/200\n",
      "59985/59985 [==============================] - 45s 753us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 33/200\n",
      "59985/59985 [==============================] - 41s 678us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 34/200\n",
      "59985/59985 [==============================] - 44s 741us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 35/200\n",
      "59985/59985 [==============================] - 41s 682us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 36/200\n",
      "59985/59985 [==============================] - 43s 718us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 37/200\n",
      "59985/59985 [==============================] - 43s 718us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 38/200\n",
      "59985/59985 [==============================] - 45s 752us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 39/200\n",
      "59985/59985 [==============================] - 40s 659us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 40/200\n",
      "59985/59985 [==============================] - 43s 717us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 41/200\n",
      "59985/59985 [==============================] - 44s 738us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 42/200\n",
      "59985/59985 [==============================] - 41s 676us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 43/200\n",
      "59985/59985 [==============================] - 42s 698us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 44/200\n",
      "59985/59985 [==============================] - 45s 743us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 45/200\n",
      "59985/59985 [==============================] - 40s 671us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 46/200\n",
      "59985/59985 [==============================] - 43s 722us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 47/200\n",
      "59985/59985 [==============================] - 42s 705us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 48/200\n",
      "59985/59985 [==============================] - 42s 697us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 49/200\n",
      "59985/59985 [==============================] - 41s 686us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 50/200\n",
      "59985/59985 [==============================] - 39s 651us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 51/200\n",
      "59985/59985 [==============================] - 43s 714us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 52/200\n",
      "59985/59985 [==============================] - 42s 693us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 53/200\n",
      "59985/59985 [==============================] - 42s 708us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 54/200\n",
      "59985/59985 [==============================] - 42s 707us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 55/200\n",
      "59985/59985 [==============================] - 42s 693us/step - loss: 0.0949 - val_loss: 0.0949 - loss: 0.0\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 56/200\n",
      "59985/59985 [==============================] - 43s 723us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 57/200\n",
      "59985/59985 [==============================] - 45s 753us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 58/200\n",
      "59985/59985 [==============================] - 41s 675us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 59/200\n",
      "59985/59985 [==============================] - 41s 686us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 60/200\n",
      "59985/59985 [==============================] - 44s 740us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 61/200\n",
      "59985/59985 [==============================] - 42s 704us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 62/200\n",
      "59985/59985 [==============================] - 43s 714us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 63/200\n",
      "59985/59985 [==============================] - 41s 691us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59985/59985 [==============================] - 41s 677us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 65/200\n",
      "59985/59985 [==============================] - 42s 706us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 66/200\n",
      "59985/59985 [==============================] - 41s 690us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 67/200\n",
      "59985/59985 [==============================] - 40s 673us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 68/200\n",
      "59985/59985 [==============================] - 42s 697us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 69/200\n",
      "59985/59985 [==============================] - 41s 687us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 70/200\n",
      "59985/59985 [==============================] - 40s 670us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 71/200\n",
      "59985/59985 [==============================] - 42s 698us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 72/200\n",
      "59985/59985 [==============================] - 41s 691us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 73/200\n",
      "59985/59985 [==============================] - 40s 666us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 74/200\n",
      "59985/59985 [==============================] - 43s 725us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 75/200\n",
      "59985/59985 [==============================] - 41s 683us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 76/200\n",
      "59985/59985 [==============================] - 40s 668us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 77/200\n",
      "59985/59985 [==============================] - 44s 728us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 78/200\n",
      "59985/59985 [==============================] - 42s 707us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 79/200\n",
      "59985/59985 [==============================] - 41s 680us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 80/200\n",
      "59985/59985 [==============================] - 39s 650us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 81/200\n",
      "59985/59985 [==============================] - 44s 729us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 82/200\n",
      "59985/59985 [==============================] - 44s 731us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 83/200\n",
      "59985/59985 [==============================] - 44s 727us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 84/200\n",
      "59985/59985 [==============================] - 39s 652us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 85/200\n",
      "59985/59985 [==============================] - 42s 705us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 86/200\n",
      "59985/59985 [==============================] - 42s 702us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 87/200\n",
      "59985/59985 [==============================] - 42s 701us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 88/200\n",
      "59985/59985 [==============================] - 43s 712us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 89/200\n",
      "59985/59985 [==============================] - 43s 715us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 90/200\n",
      "59985/59985 [==============================] - 42s 698us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 91/200\n",
      "59985/59985 [==============================] - 43s 711us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 92/200\n",
      "59985/59985 [==============================] - 44s 738us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 93/200\n",
      "59985/59985 [==============================] - 40s 666us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 94/200\n",
      "59985/59985 [==============================] - 41s 688us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 95/200\n",
      "59985/59985 [==============================] - 42s 699us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 96/200\n",
      "59985/59985 [==============================] - 43s 712us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 97/200\n",
      "59985/59985 [==============================] - 42s 707us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 98/200\n",
      "59985/59985 [==============================] - 43s 711us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 99/200\n",
      "59985/59985 [==============================] - 42s 704us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 100/200\n",
      "59985/59985 [==============================] - 41s 690us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 101/200\n",
      "59985/59985 [==============================] - 39s 656us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 102/200\n",
      "59985/59985 [==============================] - 44s 735us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00102: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 103/200\n",
      "59985/59985 [==============================] - 42s 697us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 104/200\n",
      "59985/59985 [==============================] - 42s 700us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 105/200\n",
      "59985/59985 [==============================] - 42s 696us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 106/200\n",
      "59985/59985 [==============================] - 42s 698us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 107/200\n",
      "59985/59985 [==============================] - 41s 687us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 108/200\n",
      "59985/59985 [==============================] - 41s 690us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 109/200\n",
      "59985/59985 [==============================] - 41s 683us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 110/200\n",
      "59985/59985 [==============================] - 42s 699us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 111/200\n",
      "59985/59985 [==============================] - 43s 715us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 112/200\n",
      "59985/59985 [==============================] - 40s 672us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 113/200\n",
      "59985/59985 [==============================] - 43s 722us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 114/200\n",
      "59985/59985 [==============================] - 42s 708us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 115/200\n",
      "59985/59985 [==============================] - 42s 693us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 116/200\n",
      "59985/59985 [==============================] - 42s 695us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 117/200\n",
      "59985/59985 [==============================] - 40s 666us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 118/200\n",
      "59985/59985 [==============================] - 44s 730us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 119/200\n",
      "59985/59985 [==============================] - 42s 701us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 120/200\n",
      "59985/59985 [==============================] - 39s 652us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 121/200\n",
      "59985/59985 [==============================] - 43s 718us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 122/200\n",
      "59985/59985 [==============================] - 43s 721us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 123/200\n",
      "59985/59985 [==============================] - 42s 707us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00123: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 124/200\n",
      "59985/59985 [==============================] - 43s 712us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 125/200\n",
      "59985/59985 [==============================] - 42s 699us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 126/200\n",
      "59985/59985 [==============================] - 42s 698us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 127/200\n",
      "59985/59985 [==============================] - 44s 732us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 128/200\n",
      "59985/59985 [==============================] - 42s 697us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 129/200\n",
      "59985/59985 [==============================] - 41s 686us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00129: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 130/200\n",
      "59985/59985 [==============================] - 43s 714us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 131/200\n",
      "59985/59985 [==============================] - 44s 727us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 132/200\n",
      "59985/59985 [==============================] - 41s 687us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00132: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 133/200\n",
      "59985/59985 [==============================] - 45s 744us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 134/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59985/59985 [==============================] - 43s 715us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 135/200\n",
      "59985/59985 [==============================] - 42s 706us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00135: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 136/200\n",
      "59985/59985 [==============================] - 40s 669us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 137/200\n",
      "59985/59985 [==============================] - 42s 703us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 138/200\n",
      "59985/59985 [==============================] - 44s 731us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 139/200\n",
      "59985/59985 [==============================] - 42s 699us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 140/200\n",
      "59985/59985 [==============================] - 41s 691us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 141/200\n",
      "59985/59985 [==============================] - 41s 691us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00141: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 142/200\n",
      "59985/59985 [==============================] - 42s 703us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 143/200\n",
      "59985/59985 [==============================] - 44s 729us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 144/200\n",
      "59985/59985 [==============================] - 43s 723us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00144: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 145/200\n",
      "59985/59985 [==============================] - 40s 672us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 146/200\n",
      "59985/59985 [==============================] - 42s 699us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 147/200\n",
      "59985/59985 [==============================] - 43s 722us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00147: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 148/200\n",
      "59985/59985 [==============================] - 41s 689us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 149/200\n",
      "59985/59985 [==============================] - 42s 699us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 150/200\n",
      "59985/59985 [==============================] - 42s 701us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00150: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 151/200\n",
      "59985/59985 [==============================] - 42s 708us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 152/200\n",
      "59985/59985 [==============================] - 45s 753us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 153/200\n",
      "59985/59985 [==============================] - 40s 673us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00153: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 154/200\n",
      "59985/59985 [==============================] - 44s 740us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 155/200\n",
      "59985/59985 [==============================] - 42s 699us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 156/200\n",
      "59985/59985 [==============================] - 43s 717us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00156: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 157/200\n",
      "59985/59985 [==============================] - 39s 650us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 158/200\n",
      "59985/59985 [==============================] - 43s 725us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 159/200\n",
      "59985/59985 [==============================] - 43s 713us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 160/200\n",
      "59985/59985 [==============================] - 41s 691us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 161/200\n",
      "59985/59985 [==============================] - 41s 678us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 162/200\n",
      "59985/59985 [==============================] - 47s 779us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00162: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 163/200\n",
      "59985/59985 [==============================] - 41s 679us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 164/200\n",
      "59985/59985 [==============================] - 43s 711us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 165/200\n",
      "59985/59985 [==============================] - 43s 717us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00165: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 166/200\n",
      "59985/59985 [==============================] - 42s 699us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 167/200\n",
      "59985/59985 [==============================] - 39s 647us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 168/200\n",
      "59985/59985 [==============================] - 42s 702us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00168: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59985/59985 [==============================] - 43s 722us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 170/200\n",
      "59985/59985 [==============================] - 42s 707us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 171/200\n",
      "59985/59985 [==============================] - 41s 689us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00171: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 172/200\n",
      "59985/59985 [==============================] - 41s 684us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 173/200\n",
      "59985/59985 [==============================] - 44s 729us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 174/200\n",
      "59985/59985 [==============================] - 40s 671us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00174: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 175/200\n",
      "59985/59985 [==============================] - 41s 681us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 176/200\n",
      "59985/59985 [==============================] - 44s 735us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 177/200\n",
      "59985/59985 [==============================] - 40s 668us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00177: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 178/200\n",
      "59985/59985 [==============================] - 41s 686us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 179/200\n",
      "59985/59985 [==============================] - 44s 726us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 180/200\n",
      "59985/59985 [==============================] - 40s 671us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 181/200\n",
      "59985/59985 [==============================] - 44s 731us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 182/200\n",
      "59985/59985 [==============================] - 43s 710us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 183/200\n",
      "59985/59985 [==============================] - 39s 654us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00183: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 184/200\n",
      "59985/59985 [==============================] - 42s 707us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 185/200\n",
      "59985/59985 [==============================] - 45s 742us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 186/200\n",
      "59985/59985 [==============================] - 43s 715us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00186: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 187/200\n",
      "59985/59985 [==============================] - 43s 716us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 188/200\n",
      "59985/59985 [==============================] - 43s 720us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 189/200\n",
      "59985/59985 [==============================] - 43s 711us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00189: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 190/200\n",
      "59985/59985 [==============================] - 41s 679us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 191/200\n",
      "59985/59985 [==============================] - 45s 749us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 192/200\n",
      "59985/59985 [==============================] - 43s 709us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 193/200\n",
      "59985/59985 [==============================] - 41s 692us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 194/200\n",
      "59985/59985 [==============================] - 40s 672us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 195/200\n",
      "59985/59985 [==============================] - 44s 739us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00195: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 196/200\n",
      "59985/59985 [==============================] - 44s 728us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 197/200\n",
      "59985/59985 [==============================] - 41s 689us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 198/200\n",
      "59985/59985 [==============================] - 42s 707us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00198: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 199/200\n",
      "59985/59985 [==============================] - 41s 686us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 200/200\n",
      "59985/59985 [==============================] - 45s 743us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "evaluate model...\n",
      "59985/59985 [==============================] - 9s 158us/step\n",
      "number params: [6.55865351e-01 2.85109304e-01 3.71694541e-01 3.39400200e-01\n",
      " 2.55743785e-01 8.80182551e-02 1.28000000e+02 1.60000000e+01\n",
      " 4.00000000e+01 2.50000000e+01 6.00000000e+00]\n",
      "number_layers: 6\n",
      "0.6558653509060439\n",
      "0.28510930386942723\n",
      "0.3716945405140515\n",
      "0.3394002003010893\n",
      "0.25574378535875364\n",
      "Using HyperParams. units:128, batch_size:16, look_back:40, look_ahead:1, layers:6, epochs:25, learning_rate:0.08801825511778026\n",
      "dropout_list:[0.6558653509060439, 0.28510930386942723, 0.3716945405140515, 0.3394002003010893, 0.25574378535875364]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 40\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59960, 40, 17)\n",
      "y look_ahead: (59960, 1, 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initilize Layer variable length LSTM Model...\n",
      "[0.6558653509060439, 0.28510930386942723, 0.3716945405140515, 0.3394002003010893, 0.25574378535875364]\n",
      "build model...\n",
      "dropout l0: 0.6558653509060439\n",
      "dropout:0.28510930386942723\n",
      "dropout:0.3716945405140515\n",
      "dropout:0.3394002003010893\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 40, 128)           66560     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 40, 6)             3240      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 40, 6)             0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 40, 6)             312       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 40, 6)             0         \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 6)                 312       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 7         \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 70,431\n",
      "Trainable params: 70,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59960 samples, validate on 59960 samples\n",
      "Epoch 1/25\n",
      "59960/59960 [==============================] - 429s 7ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00001: val_loss did not improve from inf\n",
      "Epoch 2/25\n",
      "59960/59960 [==============================] - 427s 7ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00002: val_loss did not improve from inf\n",
      "Epoch 3/25\n",
      "59960/59960 [==============================] - 425s 7ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00003: val_loss did not improve from inf\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00880182534456253.\n",
      "Epoch 4/25\n",
      "59960/59960 [==============================] - 422s 7ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00004: val_loss did not improve from inf\n",
      "Epoch 5/25\n",
      "59960/59960 [==============================] - 425s 7ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00005: val_loss did not improve from inf\n",
      "Epoch 00005: early stopping\n",
      "evaluate model...\n",
      "59960/59960 [==============================] - 89s 1ms/step\n",
      "number params: [3.33592932e-01 6.40532730e-01 3.84195157e-01 2.81158165e-01\n",
      " 7.24922893e-01 5.73609002e-02 3.20000000e+01 1.00000000e+00\n",
      " 1.00000000e+01 2.00000000e+02 5.00000000e+00]\n",
      "number_layers: 5\n",
      "0.3335929322163589\n",
      "0.6405327299664529\n",
      "0.38419515745271315\n",
      "0.2811581650984012\n",
      "Using HyperParams. units:32, batch_size:1, look_back:10, look_ahead:1, layers:5, epochs:200, learning_rate:0.05736090018864687\n",
      "dropout_list:[0.3335929322163589, 0.6405327299664529, 0.38419515745271315, 0.2811581650984012]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 10\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59990, 10, 17)\n",
      "y look_ahead: (59990, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.3335929322163589, 0.6405327299664529, 0.38419515745271315, 0.2811581650984012]\n",
      "build model...\n",
      "dropout l0: 0.3335929322163589\n",
      "dropout:0.6405327299664529\n",
      "dropout:0.38419515745271315\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 10, 32)            4352      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 10, 5)             760       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 10, 5)             0         \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 6         \n",
      "_________________________________________________________________\n",
      "repeat_vector_5 (RepeatVecto (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 5,338\n",
      "Trainable params: 5,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59990 samples, validate on 59990 samples\n",
      "Epoch 1/200\n",
      "59990/59990 [==============================] - 798s 13ms/step - loss: 2.2011e-05 - val_loss: 0.1425\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14254, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/200\n",
      "59990/59990 [==============================] - 801s 13ms/step - loss: 1.8932e-05 - val_loss: 0.1425\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.14254\n",
      "Epoch 3/200\n",
      "59990/59990 [==============================] - 797s 13ms/step - loss: 1.8932e-05 - val_loss: 0.1425\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.14254\n",
      "Epoch 4/200\n",
      "59990/59990 [==============================] - 790s 13ms/step - loss: 1.8932e-05 - val_loss: 0.1425\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.14254\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.005736089870333672.\n",
      "Epoch 5/200\n",
      "59990/59990 [==============================] - 791s 13ms/step - loss: 2.0582e-05 - val_loss: 0.1388\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.14254 to 0.13877, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 6/200\n",
      "59990/59990 [==============================] - 790s 13ms/step - loss: 2.0084e-05 - val_loss: 0.1388\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13877\n",
      "Epoch 7/200\n",
      "59990/59990 [==============================] - 777s 13ms/step - loss: 2.0084e-05 - val_loss: 0.1388\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13877\n",
      "Epoch 8/200\n",
      "59990/59990 [==============================] - 777s 13ms/step - loss: 2.0084e-05 - val_loss: 0.1388\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.13877\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005736089777201415.\n",
      "Epoch 9/200\n",
      "59990/59990 [==============================] - 782s 13ms/step - loss: 0.0582 - val_loss: 0.1391\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.13877\n",
      "Epoch 10/200\n",
      "59990/59990 [==============================] - 787s 13ms/step - loss: 0.0591 - val_loss: 0.1391\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.13877\n",
      "Epoch 00010: early stopping\n",
      "evaluate model...\n",
      "59990/59990 [==============================] - 130s 2ms/step\n",
      "number params: [7.33046715e-01 3.53708428e-01 3.11615636e-01 3.71737801e-01\n",
      " 3.62615250e-01 1.36239814e-02 6.40000000e+01 1.00000000e+00\n",
      " 1.00000000e+01 2.50000000e+01 6.00000000e+00]\n",
      "number_layers: 6\n",
      "0.7330467147499617\n",
      "0.35370842775722156\n",
      "0.31161563626398947\n",
      "0.37173780093057834\n",
      "0.36261524982962173\n",
      "Using HyperParams. units:64, batch_size:1, look_back:10, look_ahead:1, layers:6, epochs:25, learning_rate:0.013623981444842502\n",
      "dropout_list:[0.7330467147499617, 0.35370842775722156, 0.31161563626398947, 0.37173780093057834, 0.36261524982962173]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 10\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59990, 10, 17)\n",
      "y look_ahead: (59990, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.7330467147499617, 0.35370842775722156, 0.31161563626398947, 0.37173780093057834, 0.36261524982962173]\n",
      "build model...\n",
      "dropout l0: 0.7330467147499617\n",
      "dropout:0.35370842775722156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout:0.31161563626398947\n",
      "dropout:0.37173780093057834\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_19 (LSTM)               (None, 10, 64)            16896     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 10, 6)             1704      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 10, 6)             0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 10, 6)             312       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 10, 6)             0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 6)                 312       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 7         \n",
      "_________________________________________________________________\n",
      "repeat_vector_6 (RepeatVecto (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 19,231\n",
      "Trainable params: 19,231\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59990 samples, validate on 59990 samples\n",
      "Epoch 1/25\n",
      "59990/59990 [==============================] - 1029s 17ms/step - loss: 9.7147e-05 - val_loss: 0.1385\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13853, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/25\n",
      "59990/59990 [==============================] - 1021s 17ms/step - loss: 1.0639e-05 - val_loss: 0.1385\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.13853\n",
      "Epoch 3/25\n",
      "59990/59990 [==============================] - 1792s 30ms/step - loss: 1.0639e-05 - val_loss: 0.1385\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.13853\n",
      "Epoch 4/25\n",
      "59990/59990 [==============================] - 1393s 23ms/step - loss: 1.0639e-05 - val_loss: 0.1385\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13853\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00136239817366004.\n",
      "Epoch 5/25\n",
      "59990/59990 [==============================] - 1012s 17ms/step - loss: 0.0095 - val_loss: 0.1392\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.13853\n",
      "Epoch 6/25\n",
      "59990/59990 [==============================] - 1007s 17ms/step - loss: 0.0099 - val_loss: 0.1392\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13853\n",
      "Epoch 00006: early stopping\n",
      "evaluate model...\n",
      "59990/59990 [==============================] - 171s 3ms/step\n",
      "number params: [3.57897143e-01 7.04883110e-01 5.33673516e-01 7.90023568e-01\n",
      " 3.27398061e-01 6.98725774e-02 6.40000000e+01 1.00000000e+00\n",
      " 3.00000000e+01 1.00000000e+02 4.00000000e+00]\n",
      "number_layers: 4\n",
      "0.35789714271092177\n",
      "0.704883109723667\n",
      "0.5336735159565302\n",
      "Using HyperParams. units:64, batch_size:1, look_back:30, look_ahead:1, layers:4, epochs:100, learning_rate:0.06987257737764543\n",
      "dropout_list:[0.35789714271092177, 0.704883109723667, 0.5336735159565302]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 30\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59970, 30, 17)\n",
      "y look_ahead: (59970, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.35789714271092177, 0.704883109723667, 0.5336735159565302]\n",
      "build model...\n",
      "dropout l0: 0.35789714271092177\n",
      "dropout:0.704883109723667\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, 30, 64)            16896     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 4)                 1104      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 5         \n",
      "_________________________________________________________________\n",
      "repeat_vector_7 (RepeatVecto (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 18,005\n",
      "Trainable params: 18,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59970 samples, validate on 59970 samples\n",
      "Epoch 1/100\n",
      "59970/59970 [==============================] - 1487s 25ms/step - loss: 0.0247 - val_loss: 0.1445\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14447, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/100\n",
      "59970/59970 [==============================] - 1492s 25ms/step - loss: 0.0261 - val_loss: 0.1383\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14447 to 0.13825, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 3/100\n",
      "59970/59970 [==============================] - 1500s 25ms/step - loss: 0.0249 - val_loss: 0.1388\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.13825\n",
      "Epoch 4/100\n",
      "59970/59970 [==============================] - 1501s 25ms/step - loss: 0.0260 - val_loss: 0.1384\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13825\n",
      "Epoch 5/100\n",
      "59970/59970 [==============================] - 1505s 25ms/step - loss: 0.0226 - val_loss: 0.1455\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.13825\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.006987258046865463.\n",
      "Epoch 6/100\n",
      "59970/59970 [==============================] - 1503s 25ms/step - loss: 0.0014 - val_loss: 0.1389\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13825\n",
      "Epoch 7/100\n",
      "59970/59970 [==============================] - 1500s 25ms/step - loss: 3.8703e-04 - val_loss: 0.1387\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13825\n",
      "Epoch 00007: early stopping\n",
      "evaluate model...\n",
      "59970/59970 [==============================] - 213s 4ms/step\n",
      "number params: [2.93880829e-01 4.50930436e-01 7.50818841e-01 5.89381202e-01\n",
      " 6.20790600e-01 2.40543753e-02 1.28000000e+02 6.40000000e+01\n",
      " 1.50000000e+01 2.00000000e+02 4.00000000e+00]\n",
      "number_layers: 4\n",
      "0.29388082945633764\n",
      "0.4509304356769909\n",
      "0.7508188409001748\n",
      "Using HyperParams. units:128, batch_size:64, look_back:15, look_ahead:1, layers:4, epochs:200, learning_rate:0.024054375331353215\n",
      "dropout_list:[0.29388082945633764, 0.4509304356769909, 0.7508188409001748]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 15\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59985, 15, 17)\n",
      "y look_ahead: (59985, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.29388082945633764, 0.4509304356769909, 0.7508188409001748]\n",
      "build model...\n",
      "dropout l0: 0.29388082945633764\n",
      "dropout:0.4509304356769909\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_25 (LSTM)               (None, 15, 128)           66560     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 4)                 2128      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 5         \n",
      "_________________________________________________________________\n",
      "repeat_vector_8 (RepeatVecto (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 68,693\n",
      "Trainable params: 68,693\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59985 samples, validate on 59985 samples\n",
      "Epoch 1/200\n",
      "59985/59985 [==============================] - 43s 711us/step - loss: 0.1675 - val_loss: 0.1284\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12840, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/200\n",
      "59985/59985 [==============================] - 41s 681us/step - loss: 0.1713 - val_loss: 0.1287\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.12840\n",
      "Epoch 3/200\n",
      "59985/59985 [==============================] - 42s 705us/step - loss: 0.1699 - val_loss: 0.1281\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12840 to 0.12807, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 4/200\n",
      "59985/59985 [==============================] - 44s 732us/step - loss: 0.1704 - val_loss: 0.1259\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12807 to 0.12588, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 5/200\n",
      "59985/59985 [==============================] - 42s 704us/step - loss: 0.1680 - val_loss: 0.1281\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12588\n",
      "Epoch 6/200\n",
      "59985/59985 [==============================] - 44s 726us/step - loss: 0.1690 - val_loss: 0.1278\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12588\n",
      "Epoch 7/200\n",
      "59985/59985 [==============================] - 41s 689us/step - loss: 0.1704 - val_loss: 0.1252\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12588 to 0.12517, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 8/200\n",
      "59985/59985 [==============================] - 42s 708us/step - loss: 0.1683 - val_loss: 0.1285\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12517\n",
      "Epoch 9/200\n",
      "59985/59985 [==============================] - 42s 702us/step - loss: 0.1704 - val_loss: 0.1275\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12517\n",
      "Epoch 10/200\n",
      "59985/59985 [==============================] - 43s 722us/step - loss: 0.1708 - val_loss: 0.1263\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.12517\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.002405437454581261.\n",
      "Epoch 11/200\n",
      "59985/59985 [==============================] - 42s 708us/step - loss: 0.1135 - val_loss: 0.1021\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12517 to 0.10208, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 12/200\n",
      "59985/59985 [==============================] - 42s 698us/step - loss: 0.1141 - val_loss: 0.1027\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.10208\n",
      "Epoch 13/200\n",
      "59985/59985 [==============================] - 42s 698us/step - loss: 0.1145 - val_loss: 0.1028\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.10208\n",
      "Epoch 14/200\n",
      "59985/59985 [==============================] - 43s 710us/step - loss: 0.1148 - val_loss: 0.1027\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.10208\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002405437408015132.\n",
      "Epoch 15/200\n",
      "59985/59985 [==============================] - 43s 711us/step - loss: 0.0984 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.10208 to 0.09500, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 16/200\n",
      "59985/59985 [==============================] - 43s 714us/step - loss: 0.0968 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09500 to 0.09500, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 17/200\n",
      "59985/59985 [==============================] - 42s 704us/step - loss: 0.0969 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.09500 to 0.09496, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 18/200\n",
      "59985/59985 [==============================] - 43s 713us/step - loss: 0.0969 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.09496 to 0.09496, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 2.405437408015132e-05.\n",
      "Epoch 19/200\n",
      "59985/59985 [==============================] - 42s 698us/step - loss: 0.0951 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.09496 to 0.09490, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 20/200\n",
      "59985/59985 [==============================] - 42s 705us/step - loss: 0.0951 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.09490 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 21/200\n",
      "59985/59985 [==============================] - 42s 698us/step - loss: 0.0951 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.09488 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 22/200\n",
      "59985/59985 [==============================] - 44s 726us/step - loss: 0.0951 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.09487 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 2.4054374080151323e-06.\n",
      "Epoch 23/200\n",
      "59985/59985 [==============================] - 42s 702us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 24/200\n",
      "59985/59985 [==============================] - 43s 715us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 25/200\n",
      "59985/59985 [==============================] - 42s 703us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.4054374989646023e-07.\n",
      "Epoch 26/200\n",
      "59985/59985 [==============================] - 42s 703us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.09486 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 27/200\n",
      "59985/59985 [==============================] - 41s 688us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 28/200\n",
      "59985/59985 [==============================] - 43s 718us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 29/200\n",
      "59985/59985 [==============================] - 44s 729us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 30/200\n",
      "59985/59985 [==============================] - 42s 692us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 31/200\n",
      "59985/59985 [==============================] - 42s 697us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 32/200\n",
      "59985/59985 [==============================] - 42s 703us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 33/200\n",
      "59985/59985 [==============================] - 43s 721us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 34/200\n",
      "59985/59985 [==============================] - 42s 703us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 35/200\n",
      "59985/59985 [==============================] - 43s 710us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 36/200\n",
      "59985/59985 [==============================] - 41s 686us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 37/200\n",
      "59985/59985 [==============================] - 43s 724us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 38/200\n",
      "59985/59985 [==============================] - 42s 694us/step - loss: 0.0949 - val_loss: 0.0949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00038: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 39/200\n",
      "59985/59985 [==============================] - 43s 715us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 40/200\n",
      "59985/59985 [==============================] - 42s 702us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 41/200\n",
      "59985/59985 [==============================] - 41s 692us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 42/200\n",
      "59985/59985 [==============================] - 43s 714us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 43/200\n",
      "59985/59985 [==============================] - 44s 731us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 44/200\n",
      "59985/59985 [==============================] - 41s 685us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 45/200\n",
      "59985/59985 [==============================] - 43s 714us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 46/200\n",
      "59985/59985 [==============================] - 42s 705us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 47/200\n",
      "59985/59985 [==============================] - 42s 696us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 48/200\n",
      "59985/59985 [==============================] - 43s 710us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 49/200\n",
      "59985/59985 [==============================] - 43s 713us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 50/200\n",
      "59985/59985 [==============================] - 42s 698us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 51/200\n",
      "59985/59985 [==============================] - 42s 705us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 52/200\n",
      "59985/59985 [==============================] - 42s 698us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 53/200\n",
      "59985/59985 [==============================] - 44s 728us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 54/200\n",
      "59985/59985 [==============================] - 42s 692us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 55/200\n",
      "59985/59985 [==============================] - 44s 728us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 56/200\n",
      "59985/59985 [==============================] - 42s 708us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 57/200\n",
      "59985/59985 [==============================] - 43s 723us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 58/200\n",
      "59985/59985 [==============================] - 40s 663us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 59/200\n",
      "59985/59985 [==============================] - 43s 711us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 60/200\n",
      "59985/59985 [==============================] - 43s 713us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 61/200\n",
      "59985/59985 [==============================] - 44s 728us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 62/200\n",
      "59985/59985 [==============================] - 41s 682us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 63/200\n",
      "59985/59985 [==============================] - 42s 697us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 64/200\n",
      "59985/59985 [==============================] - 42s 699us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 65/200\n",
      "59985/59985 [==============================] - 43s 720us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 66/200\n",
      "59985/59985 [==============================] - 42s 705us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 67/200\n",
      "59985/59985 [==============================] - 42s 702us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 68/200\n",
      "59985/59985 [==============================] - 43s 711us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 69/200\n",
      "59985/59985 [==============================] - 43s 715us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 70/200\n",
      "59985/59985 [==============================] - 42s 693us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 71/200\n",
      "59985/59985 [==============================] - 43s 724us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 72/200\n",
      "59985/59985 [==============================] - 43s 722us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 73/200\n",
      "59985/59985 [==============================] - 42s 705us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 74/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59985/59985 [==============================] - 41s 687us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 75/200\n",
      "59985/59985 [==============================] - 42s 693us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 76/200\n",
      "59985/59985 [==============================] - 43s 713us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 77/200\n",
      "59985/59985 [==============================] - 43s 712us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 78/200\n",
      "59985/59985 [==============================] - 44s 726us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 79/200\n",
      "59985/59985 [==============================] - 42s 697us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 80/200\n",
      "59985/59985 [==============================] - 42s 705us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 81/200\n",
      "59985/59985 [==============================] - 42s 701us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 82/200\n",
      "59985/59985 [==============================] - 42s 708us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 83/200\n",
      "59985/59985 [==============================] - 42s 701us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 84/200\n",
      "59985/59985 [==============================] - 43s 721us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 85/200\n",
      "59985/59985 [==============================] - 42s 696us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 86/200\n",
      "59985/59985 [==============================] - 42s 703us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 87/200\n",
      "59985/59985 [==============================] - 43s 718us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 88/200\n",
      "59985/59985 [==============================] - 41s 690us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 89/200\n",
      "59985/59985 [==============================] - 43s 723us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 90/200\n",
      "59985/59985 [==============================] - 42s 697us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 91/200\n",
      "59985/59985 [==============================] - 43s 722us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 92/200\n",
      "59985/59985 [==============================] - 42s 702us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 93/200\n",
      "59985/59985 [==============================] - 42s 697us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 94/200\n",
      "59985/59985 [==============================] - 43s 714us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 95/200\n",
      "59985/59985 [==============================] - 43s 709us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 96/200\n",
      "59985/59985 [==============================] - 42s 702us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 97/200\n",
      "59985/59985 [==============================] - 43s 710us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 98/200\n",
      "59985/59985 [==============================] - 42s 697us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 99/200\n",
      "59985/59985 [==============================] - 43s 720us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 100/200\n",
      "59985/59985 [==============================] - 42s 704us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 101/200\n",
      "59985/59985 [==============================] - 42s 707us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 102/200\n",
      "59985/59985 [==============================] - 43s 713us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 103/200\n",
      "59985/59985 [==============================] - 42s 697us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 104/200\n",
      "59985/59985 [==============================] - 44s 730us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 105/200\n",
      "59985/59985 [==============================] - 42s 693us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 106/200\n",
      "59985/59985 [==============================] - 41s 690us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 107/200\n",
      "59985/59985 [==============================] - 42s 707us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 108/200\n",
      "59985/59985 [==============================] - 41s 688us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 109/200\n",
      "59985/59985 [==============================] - 44s 726us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 110/200\n",
      "59985/59985 [==============================] - 42s 708us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 111/200\n",
      "59985/59985 [==============================] - 41s 686us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 112/200\n",
      "59985/59985 [==============================] - 44s 730us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00112: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 113/200\n",
      "59985/59985 [==============================] - 41s 682us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 114/200\n",
      "59985/59985 [==============================] - 43s 725us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 115/200\n",
      "59985/59985 [==============================] - 42s 697us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 116/200\n",
      "59985/59985 [==============================] - 42s 700us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 117/200\n",
      "59985/59985 [==============================] - 42s 704us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 118/200\n",
      "59985/59985 [==============================] - 42s 701us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 119/200\n",
      "59985/59985 [==============================] - 43s 710us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 120/200\n",
      "59985/59985 [==============================] - 43s 710us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 121/200\n",
      "59985/59985 [==============================] - 42s 699us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00121: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 122/200\n",
      "59985/59985 [==============================] - 42s 697us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 123/200\n",
      "59985/59985 [==============================] - 42s 702us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 124/200\n",
      "59985/59985 [==============================] - 44s 728us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00124: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 125/200\n",
      "59985/59985 [==============================] - 42s 695us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 126/200\n",
      "59985/59985 [==============================] - 42s 699us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 127/200\n",
      "59985/59985 [==============================] - 43s 723us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00127: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 128/200\n",
      "59985/59985 [==============================] - 43s 716us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 129/200\n",
      "59985/59985 [==============================] - 40s 673us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 130/200\n",
      "59985/59985 [==============================] - 43s 721us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00130: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 131/200\n",
      "59985/59985 [==============================] - 43s 710us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 132/200\n",
      "59985/59985 [==============================] - 43s 716us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 133/200\n",
      "59985/59985 [==============================] - 43s 714us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00133: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 134/200\n",
      "59985/59985 [==============================] - 41s 692us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 135/200\n",
      "59985/59985 [==============================] - 43s 717us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 136/200\n",
      "59985/59985 [==============================] - 40s 660us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00136: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 137/200\n",
      "59985/59985 [==============================] - 42s 696us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 138/200\n",
      "59985/59985 [==============================] - 44s 726us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 139/200\n",
      "59985/59985 [==============================] - 41s 684us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 140/200\n",
      "59985/59985 [==============================] - 42s 697us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 141/200\n",
      "59985/59985 [==============================] - 43s 720us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 142/200\n",
      "59985/59985 [==============================] - 42s 708us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00142: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 143/200\n",
      "59985/59985 [==============================] - 43s 717us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 144/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59985/59985 [==============================] - 43s 725us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 145/200\n",
      "59985/59985 [==============================] - 41s 688us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00145: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 146/200\n",
      "59985/59985 [==============================] - 41s 676us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 147/200\n",
      "59985/59985 [==============================] - 43s 718us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 148/200\n",
      "59985/59985 [==============================] - 43s 724us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00148: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 149/200\n",
      "59985/59985 [==============================] - 42s 702us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 150/200\n",
      "59985/59985 [==============================] - 42s 698us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 151/200\n",
      "59985/59985 [==============================] - 42s 704us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00151: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 152/200\n",
      "59985/59985 [==============================] - 40s 671us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 153/200\n",
      "59985/59985 [==============================] - 43s 720us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 154/200\n",
      "59985/59985 [==============================] - 44s 729us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00154: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 155/200\n",
      "59985/59985 [==============================] - 41s 691us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 156/200\n",
      "59985/59985 [==============================] - 42s 700us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 157/200\n",
      "59985/59985 [==============================] - 42s 704us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00157: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 158/200\n",
      "59985/59985 [==============================] - 42s 704us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 159/200\n",
      "59985/59985 [==============================] - 42s 706us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 160/200\n",
      "59985/59985 [==============================] - 43s 723us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00160: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 161/200\n",
      "59985/59985 [==============================] - 42s 699us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 162/200\n",
      "59985/59985 [==============================] - 41s 686us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 163/200\n",
      "59985/59985 [==============================] - 42s 700us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 164/200\n",
      "59985/59985 [==============================] - 43s 714us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 165/200\n",
      "59985/59985 [==============================] - 43s 716us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 166/200\n",
      "59985/59985 [==============================] - 42s 701us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00166: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 167/200\n",
      "59985/59985 [==============================] - 43s 710us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 168/200\n",
      "59985/59985 [==============================] - 43s 716us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 169/200\n",
      "59985/59985 [==============================] - 42s 697us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 170/200\n",
      "59985/59985 [==============================] - 42s 707us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 171/200\n",
      "59985/59985 [==============================] - 42s 704us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 172/200\n",
      "59985/59985 [==============================] - 42s 701us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 173/200\n",
      "59985/59985 [==============================] - 44s 732us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 174/200\n",
      "59985/59985 [==============================] - 41s 689us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 175/200\n",
      "59985/59985 [==============================] - 44s 726us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00175: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 176/200\n",
      "59985/59985 [==============================] - 40s 660us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 177/200\n",
      "59985/59985 [==============================] - 44s 739us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 178/200\n",
      "59985/59985 [==============================] - 40s 667us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00178: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59985/59985 [==============================] - 42s 703us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 180/200\n",
      "59985/59985 [==============================] - 42s 706us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 181/200\n",
      "59985/59985 [==============================] - 42s 703us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00181: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 182/200\n",
      "59985/59985 [==============================] - 42s 702us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 183/200\n",
      "59985/59985 [==============================] - 42s 704us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 184/200\n",
      "59985/59985 [==============================] - 43s 709us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 185/200\n",
      "59985/59985 [==============================] - 42s 705us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 186/200\n",
      "59985/59985 [==============================] - 42s 695us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 187/200\n",
      "59985/59985 [==============================] - 43s 716us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00187: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 188/200\n",
      "59985/59985 [==============================] - 41s 682us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 189/200\n",
      "59985/59985 [==============================] - 43s 712us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 190/200\n",
      "59985/59985 [==============================] - 41s 683us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00190: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 191/200\n",
      "59985/59985 [==============================] - 43s 714us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 192/200\n",
      "59985/59985 [==============================] - 44s 730us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 193/200\n",
      "59985/59985 [==============================] - 39s 658us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00193: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 194/200\n",
      "59985/59985 [==============================] - 43s 724us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 195/200\n",
      "59985/59985 [==============================] - 42s 702us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 196/200\n",
      "59985/59985 [==============================] - 44s 735us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00196: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 197/200\n",
      "59985/59985 [==============================] - 40s 669us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 198/200\n",
      "59985/59985 [==============================] - 43s 709us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 199/200\n",
      "59985/59985 [==============================] - 42s 699us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00199: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 200/200\n",
      "59985/59985 [==============================] - 43s 711us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "evaluate model...\n",
      "59985/59985 [==============================] - 10s 166us/step\n",
      "number params: [4.76289946e-01 5.90757384e-01 6.49039796e-01 4.90969678e-01\n",
      " 4.69727927e-01 6.27768427e-02 2.56000000e+02 3.20000000e+01\n",
      " 5.00000000e+01 1.00000000e+02 3.00000000e+00]\n",
      "number_layers: 3\n",
      "0.47628994616438336\n",
      "0.5907573843609754\n",
      "Using HyperParams. units:256, batch_size:32, look_back:50, look_ahead:1, layers:3, epochs:100, learning_rate:0.06277684269627061\n",
      "dropout_list:[0.47628994616438336, 0.5907573843609754]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 50\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59950, 50, 17)\n",
      "y look_ahead: (59950, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.47628994616438336, 0.5907573843609754]\n",
      "build model...\n",
      "dropout l0: 0.47628994616438336\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_27 (LSTM)               (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "repeat_vector_9 (RepeatVecto (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 264,449\n",
      "Trainable params: 264,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59950 samples, validate on 59950 samples\n",
      "Epoch 1/100\n",
      "59950/59950 [==============================] - 302s 5ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00001: val_loss did not improve from inf\n",
      "Epoch 2/100\n",
      "59950/59950 [==============================] - 300s 5ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00002: val_loss did not improve from inf\n",
      "Epoch 3/100\n",
      "59950/59950 [==============================] - 301s 5ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00003: val_loss did not improve from inf\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00627768412232399.\n",
      "Epoch 4/100\n",
      "59950/59950 [==============================] - 302s 5ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00004: val_loss did not improve from inf\n",
      "Epoch 5/100\n",
      "59950/59950 [==============================] - 298s 5ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00005: val_loss did not improve from inf\n",
      "Epoch 00005: early stopping\n",
      "evaluate model...\n",
      "59950/59950 [==============================] - 72s 1ms/step\n",
      "number params: [ 0.36163587  0.23885092  0.41927235  0.37318185  0.70045153  0.09825554\n",
      " 32.          8.          5.         50.          6.        ]\n",
      "number_layers: 6\n",
      "0.36163586743792187\n",
      "0.23885091576430784\n",
      "0.4192723512156224\n",
      "0.37318185287615985\n",
      "0.700451534973701\n",
      "Using HyperParams. units:32, batch_size:8, look_back:5, look_ahead:1, layers:6, epochs:50, learning_rate:0.098255535739922\n",
      "dropout_list:[0.36163586743792187, 0.23885091576430784, 0.4192723512156224, 0.37318185287615985, 0.700451534973701]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 5\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59995, 5, 17)\n",
      "y look_ahead: (59995, 1, 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initilize Layer variable length LSTM Model...\n",
      "[0.36163586743792187, 0.23885091576430784, 0.4192723512156224, 0.37318185287615985, 0.700451534973701]\n",
      "build model...\n",
      "dropout l0: 0.36163586743792187\n",
      "dropout:0.23885091576430784\n",
      "dropout:0.4192723512156224\n",
      "dropout:0.37318185287615985\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, 5, 32)             4352      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 5, 6)              936       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 5, 6)              0         \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 5, 6)              312       \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 5, 6)              0         \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 6)                 312       \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 7         \n",
      "_________________________________________________________________\n",
      "repeat_vector_10 (RepeatVect (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 5,919\n",
      "Trainable params: 5,919\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59995 samples, validate on 59995 samples\n",
      "Epoch 1/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0025 - val_loss: 0.1384\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13844, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0028 - val_loss: 0.1384\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13844 to 0.13843, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 3/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0028 - val_loss: 0.1384\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13843 to 0.13843, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 4/50\n",
      "59995/59995 [==============================] - 80s 1ms/step - loss: 0.0028 - val_loss: 0.1384\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.13843 to 0.13843, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.009825553745031357.\n",
      "Epoch 5/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0268 - val_loss: 0.1391\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.13843\n",
      "Epoch 6/50\n",
      "59995/59995 [==============================] - 77s 1ms/step - loss: 0.0298 - val_loss: 0.1390\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13843\n",
      "Epoch 7/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0298 - val_loss: 0.1390\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13843\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0009825553745031357.\n",
      "Epoch 8/50\n",
      "59995/59995 [==============================] - 81s 1ms/step - loss: 0.0986 - val_loss: 0.1030\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.13843 to 0.10299, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 9/50\n",
      "59995/59995 [==============================] - 80s 1ms/step - loss: 0.0976 - val_loss: 0.1030\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.10299 to 0.10299, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 10/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0976 - val_loss: 0.1030\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.10299\n",
      "Epoch 11/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0976 - val_loss: 0.1030\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.10299\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.82555327937007e-05.\n",
      "Epoch 12/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0974 - val_loss: 0.0952\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.10299 to 0.09520, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 13/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0954 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.09520 to 0.09499, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 14/50\n",
      "59995/59995 [==============================] - 80s 1ms/step - loss: 0.0954 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.09499 to 0.09497, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 15/50\n",
      "59995/59995 [==============================] - 78s 1ms/step - loss: 0.0954 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09497 to 0.09497, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 16/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0953 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09497 to 0.09497, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 9.825553570408374e-06.\n",
      "Epoch 17/50\n",
      "59995/59995 [==============================] - 82s 1ms/step - loss: 0.0950 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.09497 to 0.09493, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 18/50\n",
      "59995/59995 [==============================] - 77s 1ms/step - loss: 0.0950 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.09493 to 0.09490, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 19/50\n",
      "59995/59995 [==============================] - 80s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.09490 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 20/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.09488 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 21/50\n",
      "59995/59995 [==============================] - 78s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.09487 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 22/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.09486 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.825553206610493e-07.\n",
      "Epoch 23/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 24/50\n",
      "59995/59995 [==============================] - 81s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 25/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 26/50\n",
      "59995/59995 [==============================] - 80s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 27/50\n",
      "59995/59995 [==============================] - 78s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 28/50\n",
      "59995/59995 [==============================] - 80s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 29/50\n",
      "59995/59995 [==============================] - 81s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 30/50\n",
      "59995/59995 [==============================] - 78s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 31/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 32/50\n",
      "59995/59995 [==============================] - 80s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 33/50\n",
      "59995/59995 [==============================] - 78s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 34/50\n",
      "59995/59995 [==============================] - 80s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 35/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 36/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 37/50\n",
      "59995/59995 [==============================] - 81s 1ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 38/50\n",
      "59995/59995 [==============================] - 77s 1ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 39/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 40/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 41/50\n",
      "59995/59995 [==============================] - 80s 1ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 42/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 43/50\n",
      "59995/59995 [==============================] - 80s 1ms/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 44/50\n",
      "59995/59995 [==============================] - 77s 1ms/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 45/50\n",
      "59995/59995 [==============================] - 78s 1ms/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 46/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 47/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 48/50\n",
      "59995/59995 [==============================] - 80s 1ms/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 49/50\n",
      "59995/59995 [==============================] - 79s 1ms/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 50/50\n",
      "59995/59995 [==============================] - 80s 1ms/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "evaluate model...\n",
      "59995/59995 [==============================] - 14s 232us/step\n",
      "number params: [4.05871874e-01 6.63760575e-01 6.45161098e-01 3.44307034e-01\n",
      " 2.05216649e-01 6.63944412e-02 1.28000000e+02 3.20000000e+01\n",
      " 1.00000000e+01 2.50000000e+02 3.00000000e+00]\n",
      "number_layers: 3\n",
      "0.4058718740255315\n",
      "0.6637605748322937\n",
      "Using HyperParams. units:128, batch_size:32, look_back:10, look_ahead:1, layers:3, epochs:250, learning_rate:0.06639444116937268\n",
      "dropout_list:[0.4058718740255315, 0.6637605748322937]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 10\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59990, 10, 17)\n",
      "y look_ahead: (59990, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.4058718740255315, 0.6637605748322937]\n",
      "build model...\n",
      "dropout l0: 0.4058718740255315\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, 128)               66560     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "repeat_vector_11 (RepeatVect (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 66,689\n",
      "Trainable params: 66,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59990 samples, validate on 59990 samples\n",
      "Epoch 1/250\n",
      "59990/59990 [==============================] - 41s 680us/step - loss: 533.7993 - val_loss: 0.0244\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.02440, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/250\n",
      "59990/59990 [==============================] - 40s 662us/step - loss: 0.0308 - val_loss: 0.0128\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.02440 to 0.01277, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 3/250\n",
      "59990/59990 [==============================] - 42s 699us/step - loss: 0.0278 - val_loss: 0.0218\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.01277\n",
      "Epoch 4/250\n",
      "59990/59990 [==============================] - 42s 707us/step - loss: 0.0322 - val_loss: 0.0458\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01277\n",
      "Epoch 5/250\n",
      "59990/59990 [==============================] - 40s 660us/step - loss: 0.0562 - val_loss: 0.0983\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01277\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.006639444082975388.\n",
      "Epoch 6/250\n",
      "59990/59990 [==============================] - 40s 666us/step - loss: 0.0602 - val_loss: 0.0294\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01277\n",
      "Epoch 7/250\n",
      "59990/59990 [==============================] - 41s 680us/step - loss: 0.0428 - val_loss: 0.0253\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01277\n",
      "Epoch 00007: early stopping\n",
      "evaluate model...\n",
      "59990/59990 [==============================] - 9s 151us/step\n",
      "number params: [7.07370249e-01 4.07841260e-01 6.54688841e-01 6.26572930e-01\n",
      " 4.60393189e-01 5.45875233e-02 3.20000000e+01 1.60000000e+01\n",
      " 5.00000000e+00 2.00000000e+02 4.00000000e+00]\n",
      "number_layers: 4\n",
      "0.7073702487621827\n",
      "0.4078412598890271\n",
      "0.6546888411340483\n",
      "Using HyperParams. units:32, batch_size:16, look_back:5, look_ahead:1, layers:4, epochs:200, learning_rate:0.05458752327931712\n",
      "dropout_list:[0.7073702487621827, 0.4078412598890271, 0.6546888411340483]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 5\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59995, 5, 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y look_ahead: (59995, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.7073702487621827, 0.4078412598890271, 0.6546888411340483]\n",
      "build model...\n",
      "dropout l0: 0.7073702487621827\n",
      "dropout:0.4078412598890271\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_33 (LSTM)               (None, 5, 32)             4352      \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "lstm_34 (LSTM)               (None, 4)                 592       \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 5         \n",
      "_________________________________________________________________\n",
      "repeat_vector_12 (RepeatVect (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 4,949\n",
      "Trainable params: 4,949\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59995 samples, validate on 59995 samples\n",
      "Epoch 1/200\n",
      "59995/59995 [==============================] - 25s 422us/step - loss: 0.0234 - val_loss: 0.1379\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13790, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/200\n",
      "59995/59995 [==============================] - 23s 387us/step - loss: 0.0258 - val_loss: 0.1379\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.13790\n",
      "Epoch 3/200\n",
      "59995/59995 [==============================] - 25s 423us/step - loss: 0.0259 - val_loss: 0.1380\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.13790\n",
      "Epoch 4/200\n",
      "59995/59995 [==============================] - 25s 423us/step - loss: 0.0259 - val_loss: 0.1380\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13790\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0054587524384260185.\n",
      "Epoch 5/200\n",
      "59995/59995 [==============================] - 25s 423us/step - loss: 0.1025 - val_loss: 0.1225\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13790 to 0.12248, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 6/200\n",
      "59995/59995 [==============================] - 25s 418us/step - loss: 0.1036 - val_loss: 0.1222\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12248 to 0.12222, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 7/200\n",
      "59995/59995 [==============================] - 23s 384us/step - loss: 0.1037 - val_loss: 0.1222\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12222 to 0.12221, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 8/200\n",
      "59995/59995 [==============================] - 23s 390us/step - loss: 0.1037 - val_loss: 0.1222\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.12221 to 0.12221, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 9/200\n",
      "59995/59995 [==============================] - 26s 429us/step - loss: 0.1037 - val_loss: 0.1222\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.12221 to 0.12221, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005458752624690532.\n",
      "Epoch 10/200\n",
      "59995/59995 [==============================] - 26s 428us/step - loss: 0.1000 - val_loss: 0.0956\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12221 to 0.09558, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 11/200\n",
      "59995/59995 [==============================] - 25s 423us/step - loss: 0.0972 - val_loss: 0.0956\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09558 to 0.09557, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 12/200\n",
      "59995/59995 [==============================] - 24s 399us/step - loss: 0.0972 - val_loss: 0.0956\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.09557\n",
      "Epoch 13/200\n",
      "59995/59995 [==============================] - 25s 418us/step - loss: 0.0972 - val_loss: 0.0956\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.09557\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 5.458752857521177e-05.\n",
      "Epoch 14/200\n",
      "59995/59995 [==============================] - 24s 400us/step - loss: 0.0955 - val_loss: 0.0951\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.09557 to 0.09508, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 15/200\n",
      "59995/59995 [==============================] - 25s 418us/step - loss: 0.0952 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09508 to 0.09493, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 16/200\n",
      "59995/59995 [==============================] - 24s 404us/step - loss: 0.0951 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09493 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 17/200\n",
      "59995/59995 [==============================] - 25s 416us/step - loss: 0.0951 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.09488 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 18/200\n",
      "59995/59995 [==============================] - 25s 412us/step - loss: 0.0951 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.09486 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5.458753003040329e-06.\n",
      "Epoch 19/200\n",
      "59995/59995 [==============================] - 25s 414us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 20/200\n",
      "59995/59995 [==============================] - 23s 387us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 21/200\n",
      "59995/59995 [==============================] - 26s 435us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 5.458753093989799e-07.\n",
      "Epoch 22/200\n",
      "59995/59995 [==============================] - 26s 432us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 23/200\n",
      "59995/59995 [==============================] - 24s 396us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 24/200\n",
      "59995/59995 [==============================] - 25s 421us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 25/200\n",
      "59995/59995 [==============================] - 25s 421us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 26/200\n",
      "59995/59995 [==============================] - 25s 417us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 27/200\n",
      "59995/59995 [==============================] - 23s 387us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 28/200\n",
      "59995/59995 [==============================] - 26s 436us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 29/200\n",
      "59995/59995 [==============================] - 23s 388us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 30/200\n",
      "59995/59995 [==============================] - 26s 433us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 31/200\n",
      "59995/59995 [==============================] - 23s 391us/step - loss: 0.0948 - val_loss: 0.0948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 32/200\n",
      "59995/59995 [==============================] - 26s 425us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 33/200\n",
      "59995/59995 [==============================] - 26s 425us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 34/200\n",
      "59995/59995 [==============================] - 24s 401us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 35/200\n",
      "59995/59995 [==============================] - 25s 413us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 36/200\n",
      "59995/59995 [==============================] - 25s 416us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 37/200\n",
      "59995/59995 [==============================] - 25s 421us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 38/200\n",
      "59995/59995 [==============================] - 23s 386us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 39/200\n",
      "59995/59995 [==============================] - 25s 417us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 40/200\n",
      "59995/59995 [==============================] - 24s 407us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 41/200\n",
      "59995/59995 [==============================] - 25s 424us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 42/200\n",
      "59995/59995 [==============================] - 25s 417us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 43/200\n",
      "59995/59995 [==============================] - 24s 397us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 44/200\n",
      "59995/59995 [==============================] - 26s 428us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 45/200\n",
      "59995/59995 [==============================] - 26s 432us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 46/200\n",
      "59995/59995 [==============================] - 23s 389us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 47/200\n",
      "59995/59995 [==============================] - 26s 430us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 48/200\n",
      "59995/59995 [==============================] - 24s 394us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 49/200\n",
      "59995/59995 [==============================] - 25s 423us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 50/200\n",
      "59995/59995 [==============================] - 24s 393us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 51/200\n",
      "59995/59995 [==============================] - 26s 426us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 52/200\n",
      "59995/59995 [==============================] - 26s 429us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 53/200\n",
      "59995/59995 [==============================] - 24s 401us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.09485 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 54/200\n",
      "59995/59995 [==============================] - 25s 422us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 55/200\n",
      "59995/59995 [==============================] - 26s 434us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 56/200\n",
      "59995/59995 [==============================] - 23s 388us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 57/200\n",
      "59995/59995 [==============================] - 24s 392us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 58/200\n",
      "59995/59995 [==============================] - 26s 428us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 59/200\n",
      "59995/59995 [==============================] - 26s 428us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 60/200\n",
      "59995/59995 [==============================] - 25s 424us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 61/200\n",
      "59995/59995 [==============================] - 24s 398us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 62/200\n",
      "59995/59995 [==============================] - 25s 422us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 63/200\n",
      "59995/59995 [==============================] - 26s 430us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 64/200\n",
      "59995/59995 [==============================] - 25s 424us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 65/200\n",
      "59995/59995 [==============================] - 24s 393us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 66/200\n",
      "59995/59995 [==============================] - 23s 391us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 67/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59995/59995 [==============================] - 26s 426us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 68/200\n",
      "59995/59995 [==============================] - 26s 438us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 69/200\n",
      "59995/59995 [==============================] - 24s 392us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 70/200\n",
      "59995/59995 [==============================] - 23s 386us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 71/200\n",
      "59995/59995 [==============================] - 24s 408us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 72/200\n",
      "59995/59995 [==============================] - 26s 425us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 73/200\n",
      "59995/59995 [==============================] - 25s 421us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 74/200\n",
      "59995/59995 [==============================] - 25s 409us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 75/200\n",
      "59995/59995 [==============================] - 25s 415us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 76/200\n",
      "59995/59995 [==============================] - 24s 393us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 77/200\n",
      "59995/59995 [==============================] - 23s 388us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 78/200\n",
      "59995/59995 [==============================] - 26s 430us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 79/200\n",
      "59995/59995 [==============================] - 26s 428us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 80/200\n",
      "59995/59995 [==============================] - 25s 420us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 81/200\n",
      "59995/59995 [==============================] - 25s 424us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 82/200\n",
      "59995/59995 [==============================] - 26s 441us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 83/200\n",
      "59995/59995 [==============================] - 24s 399us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 84/200\n",
      "59995/59995 [==============================] - 23s 386us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 85/200\n",
      "59995/59995 [==============================] - 24s 402us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 86/200\n",
      "59995/59995 [==============================] - 25s 421us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 87/200\n",
      "59995/59995 [==============================] - 24s 406us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 88/200\n",
      "59995/59995 [==============================] - 26s 426us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 89/200\n",
      "59995/59995 [==============================] - 27s 444us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 90/200\n",
      "59995/59995 [==============================] - 23s 388us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 91/200\n",
      "59995/59995 [==============================] - 24s 408us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 92/200\n",
      "59995/59995 [==============================] - 24s 392us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 93/200\n",
      "59995/59995 [==============================] - 25s 412us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 94/200\n",
      "59995/59995 [==============================] - 25s 410us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 95/200\n",
      "59995/59995 [==============================] - 25s 423us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 96/200\n",
      "59995/59995 [==============================] - 24s 403us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 97/200\n",
      "59995/59995 [==============================] - 26s 425us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 98/200\n",
      "59995/59995 [==============================] - 25s 423us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 99/200\n",
      "59995/59995 [==============================] - 24s 400us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 100/200\n",
      "59995/59995 [==============================] - 26s 427us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 101/200\n",
      "59995/59995 [==============================] - 26s 437us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 102/200\n",
      "59995/59995 [==============================] - 23s 387us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00102: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 103/200\n",
      "59995/59995 [==============================] - 23s 385us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 104/200\n",
      "59995/59995 [==============================] - 25s 413us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 105/200\n",
      "59995/59995 [==============================] - 25s 423us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 106/200\n",
      "59995/59995 [==============================] - 25s 425us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 107/200\n",
      "59995/59995 [==============================] - 25s 421us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 108/200\n",
      "59995/59995 [==============================] - 25s 412us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 109/200\n",
      "59995/59995 [==============================] - 23s 387us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 110/200\n",
      "59995/59995 [==============================] - 26s 430us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 111/200\n",
      "59995/59995 [==============================] - 26s 430us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 112/200\n",
      "59995/59995 [==============================] - 24s 398us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 113/200\n",
      "59995/59995 [==============================] - 25s 411us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 114/200\n",
      "59995/59995 [==============================] - 24s 395us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 115/200\n",
      "59995/59995 [==============================] - 25s 419us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 116/200\n",
      "59995/59995 [==============================] - 25s 423us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 117/200\n",
      "59995/59995 [==============================] - 25s 421us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 118/200\n",
      "59995/59995 [==============================] - 24s 395us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 119/200\n",
      "59995/59995 [==============================] - 23s 390us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 120/200\n",
      "59995/59995 [==============================] - 26s 426us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 121/200\n",
      "59995/59995 [==============================] - 26s 427us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 122/200\n",
      "59995/59995 [==============================] - 24s 400us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 123/200\n",
      "59995/59995 [==============================] - 26s 429us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00123: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 124/200\n",
      "59995/59995 [==============================] - 25s 424us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 125/200\n",
      "59995/59995 [==============================] - 24s 406us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 126/200\n",
      "59995/59995 [==============================] - 23s 386us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 127/200\n",
      "59995/59995 [==============================] - 24s 401us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 128/200\n",
      "59995/59995 [==============================] - 26s 428us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 129/200\n",
      "59995/59995 [==============================] - 25s 423us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00129: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 130/200\n",
      "59995/59995 [==============================] - 24s 396us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 131/200\n",
      "59995/59995 [==============================] - 25s 425us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 132/200\n",
      "59995/59995 [==============================] - 26s 426us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00132: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 133/200\n",
      "59995/59995 [==============================] - 24s 403us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 134/200\n",
      "59995/59995 [==============================] - 26s 430us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 135/200\n",
      "59995/59995 [==============================] - 26s 437us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00135: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 136/200\n",
      "59995/59995 [==============================] - 23s 385us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 137/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59995/59995 [==============================] - 26s 432us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 138/200\n",
      "59995/59995 [==============================] - 25s 417us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 139/200\n",
      "59995/59995 [==============================] - 24s 406us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 140/200\n",
      "59995/59995 [==============================] - 23s 386us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 141/200\n",
      "59995/59995 [==============================] - 25s 418us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00141: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 142/200\n",
      "59995/59995 [==============================] - 23s 387us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 143/200\n",
      "59995/59995 [==============================] - 26s 432us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 144/200\n",
      "59995/59995 [==============================] - 25s 424us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00144: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 145/200\n",
      "59995/59995 [==============================] - 24s 403us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 146/200\n",
      "59995/59995 [==============================] - 26s 427us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 147/200\n",
      "59995/59995 [==============================] - 25s 419us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00147: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 148/200\n",
      "59995/59995 [==============================] - 24s 405us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 149/200\n",
      "59995/59995 [==============================] - 24s 408us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 150/200\n",
      "59995/59995 [==============================] - 25s 424us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00150: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 151/200\n",
      "59995/59995 [==============================] - 25s 423us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 152/200\n",
      "59995/59995 [==============================] - 24s 407us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 153/200\n",
      "59995/59995 [==============================] - 25s 417us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00153: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 154/200\n",
      "59995/59995 [==============================] - 24s 396us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 155/200\n",
      "59995/59995 [==============================] - 25s 423us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 156/200\n",
      "59995/59995 [==============================] - 25s 420us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00156: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 157/200\n",
      "59995/59995 [==============================] - 25s 411us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 158/200\n",
      "59995/59995 [==============================] - 23s 387us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 159/200\n",
      "59995/59995 [==============================] - 26s 434us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 160/200\n",
      "59995/59995 [==============================] - 26s 431us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 161/200\n",
      "59995/59995 [==============================] - 24s 395us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 162/200\n",
      "59995/59995 [==============================] - 25s 415us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00162: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 163/200\n",
      "59995/59995 [==============================] - 24s 400us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 164/200\n",
      "59995/59995 [==============================] - 25s 421us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 165/200\n",
      "59995/59995 [==============================] - 24s 395us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00165: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 166/200\n",
      "59995/59995 [==============================] - 26s 425us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 167/200\n",
      "59995/59995 [==============================] - 23s 391us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 168/200\n",
      "59995/59995 [==============================] - 26s 435us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00168: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 169/200\n",
      "59995/59995 [==============================] - 23s 387us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 170/200\n",
      "59995/59995 [==============================] - 26s 436us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 171/200\n",
      "59995/59995 [==============================] - 25s 422us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00171: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 172/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59995/59995 [==============================] - 24s 398us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 173/200\n",
      "59995/59995 [==============================] - 25s 411us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 174/200\n",
      "59995/59995 [==============================] - 24s 396us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00174: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 175/200\n",
      "59995/59995 [==============================] - 25s 415us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 176/200\n",
      "59995/59995 [==============================] - 25s 420us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 177/200\n",
      "59995/59995 [==============================] - 25s 421us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00177: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 178/200\n",
      "59995/59995 [==============================] - 24s 394us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 179/200\n",
      "59995/59995 [==============================] - 26s 434us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 180/200\n",
      "59995/59995 [==============================] - 25s 419us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 181/200\n",
      "59995/59995 [==============================] - 23s 384us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 182/200\n",
      "59995/59995 [==============================] - 24s 407us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 183/200\n",
      "59995/59995 [==============================] - 26s 426us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00183: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 184/200\n",
      "59995/59995 [==============================] - 26s 428us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 185/200\n",
      "59995/59995 [==============================] - 24s 398us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 186/200\n",
      "59995/59995 [==============================] - 24s 408us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00186: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 187/200\n",
      "59995/59995 [==============================] - 23s 386us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 188/200\n",
      "59995/59995 [==============================] - 25s 423us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 189/200\n",
      "59995/59995 [==============================] - 25s 421us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00189: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 190/200\n",
      "59995/59995 [==============================] - 25s 411us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 191/200\n",
      "59995/59995 [==============================] - 23s 387us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 192/200\n",
      "59995/59995 [==============================] - 26s 434us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 193/200\n",
      "59995/59995 [==============================] - 26s 429us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 194/200\n",
      "59995/59995 [==============================] - 25s 415us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 195/200\n",
      "59995/59995 [==============================] - 23s 389us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00195: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 196/200\n",
      "59995/59995 [==============================] - 26s 429us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 197/200\n",
      "59995/59995 [==============================] - 26s 433us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 198/200\n",
      "59995/59995 [==============================] - 24s 396us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00198: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 199/200\n",
      "59995/59995 [==============================] - 23s 390us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 200/200\n",
      "59995/59995 [==============================] - 26s 426us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.09484 to 0.09484, saving model to save_stacked_checkpoint.keras\n",
      "evaluate model...\n",
      "59995/59995 [==============================] - 4s 73us/step\n",
      "number params: [6.60919799e-01 7.72374597e-01 5.59042926e-01 3.84267707e-01\n",
      " 4.29826259e-01 6.91075788e-02 3.20000000e+01 1.00000000e+00\n",
      " 5.00000000e+00 1.00000000e+02 5.00000000e+00]\n",
      "number_layers: 5\n",
      "0.6609197988515296\n",
      "0.7723745968673796\n",
      "0.5590429260244085\n",
      "0.3842677071559065\n",
      "Using HyperParams. units:32, batch_size:1, look_back:5, look_ahead:1, layers:5, epochs:100, learning_rate:0.06910757882973778\n",
      "dropout_list:[0.6609197988515296, 0.7723745968673796, 0.5590429260244085, 0.3842677071559065]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 5\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59995, 5, 17)\n",
      "y look_ahead: (59995, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.6609197988515296, 0.7723745968673796, 0.5590429260244085, 0.3842677071559065]\n",
      "build model...\n",
      "dropout l0: 0.6609197988515296\n",
      "dropout:0.7723745968673796\n",
      "dropout:0.5590429260244085\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_35 (LSTM)               (None, 5, 32)             4352      \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 5, 32)             0         \n",
      "_________________________________________________________________\n",
      "lstm_36 (LSTM)               (None, 5, 5)              760       \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 5, 5)              0         \n",
      "_________________________________________________________________\n",
      "lstm_37 (LSTM)               (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 6         \n",
      "_________________________________________________________________\n",
      "repeat_vector_13 (RepeatVect (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 5,338\n",
      "Trainable params: 5,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59995 samples, validate on 59995 samples\n",
      "Epoch 1/100\n",
      "59995/59995 [==============================] - 485s 8ms/step - loss: 5.6689e-05 - val_loss: 0.1430\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14301, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/100\n",
      "59995/59995 [==============================] - 485s 8ms/step - loss: 1.9756e-05 - val_loss: 0.1430\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.14301\n",
      "Epoch 3/100\n",
      "59995/59995 [==============================] - 484s 8ms/step - loss: 1.9756e-05 - val_loss: 0.1430\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.14301\n",
      "Epoch 4/100\n",
      "59995/59995 [==============================] - 481s 8ms/step - loss: 1.9756e-05 - val_loss: 0.1430\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.14301\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.006910757720470429.\n",
      "Epoch 5/100\n",
      "59995/59995 [==============================] - 483s 8ms/step - loss: 1.4457e-05 - val_loss: 0.1385\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.14301 to 0.13847, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 6/100\n",
      "59995/59995 [==============================] - 483s 8ms/step - loss: 1.4215e-05 - val_loss: 0.1385\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13847\n",
      "Epoch 7/100\n",
      "59995/59995 [==============================] - 482s 8ms/step - loss: 1.4215e-05 - val_loss: 0.1385\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13847\n",
      "Epoch 8/100\n",
      "59995/59995 [==============================] - 482s 8ms/step - loss: 1.4215e-05 - val_loss: 0.1385\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.13847\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0006910757627338172.\n",
      "Epoch 9/100\n",
      "59995/59995 [==============================] - 482s 8ms/step - loss: 0.0463 - val_loss: 0.1392\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.13847\n",
      "Epoch 10/100\n",
      "59995/59995 [==============================] - 483s 8ms/step - loss: 0.0474 - val_loss: 0.1392\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.13847\n",
      "Epoch 00010: early stopping\n",
      "evaluate model...\n",
      "59995/59995 [==============================] - 84s 1ms/step\n",
      "number params: [7.59900740e-01 3.34114481e-01 5.68033450e-01 5.37070677e-01\n",
      " 7.66120280e-01 1.07297862e-02 2.56000000e+02 8.00000000e+00\n",
      " 2.00000000e+01 1.50000000e+02 6.00000000e+00]\n",
      "number_layers: 6\n",
      "0.7599007400264535\n",
      "0.33411448120689624\n",
      "0.5680334500174901\n",
      "0.5370706772818568\n",
      "0.7661202799508764\n",
      "Using HyperParams. units:256, batch_size:8, look_back:20, look_ahead:1, layers:6, epochs:150, learning_rate:0.010729786235621668\n",
      "dropout_list:[0.7599007400264535, 0.33411448120689624, 0.5680334500174901, 0.5370706772818568, 0.7661202799508764]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 20\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59980, 20, 17)\n",
      "y look_ahead: (59980, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.7599007400264535, 0.33411448120689624, 0.5680334500174901, 0.5370706772818568, 0.7661202799508764]\n",
      "build model...\n",
      "dropout l0: 0.7599007400264535\n",
      "dropout:0.33411448120689624\n",
      "dropout:0.5680334500174901\n",
      "dropout:0.5370706772818568\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_38 (LSTM)               (None, 20, 256)           264192    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, 20, 6)             6312      \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 20, 6)             0         \n",
      "_________________________________________________________________\n",
      "lstm_40 (LSTM)               (None, 20, 6)             312       \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 20, 6)             0         \n",
      "_________________________________________________________________\n",
      "lstm_41 (LSTM)               (None, 6)                 312       \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 7         \n",
      "_________________________________________________________________\n",
      "repeat_vector_14 (RepeatVect (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 271,135\n",
      "Trainable params: 271,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59980 samples, validate on 59980 samples\n",
      "Epoch 1/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0247 - val_loss: 0.1391\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13914, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/150\n",
      "59980/59980 [==============================] - 585s 10ms/step - loss: 0.0268 - val_loss: 0.1391\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13914 to 0.13913, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 3/150\n",
      "59980/59980 [==============================] - 585s 10ms/step - loss: 0.0269 - val_loss: 0.1391\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13913 to 0.13913, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 4/150\n",
      "59980/59980 [==============================] - 584s 10ms/step - loss: 0.0269 - val_loss: 0.1391\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13913\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.001072978600859642.\n",
      "Epoch 5/150\n",
      "59980/59980 [==============================] - 584s 10ms/step - loss: 0.0985 - val_loss: 0.1042\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.13913 to 0.10419, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 6/150\n",
      "59980/59980 [==============================] - 587s 10ms/step - loss: 0.0977 - val_loss: 0.1042\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.10419 to 0.10419, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 7/150\n",
      "59980/59980 [==============================] - 585s 10ms/step - loss: 0.0977 - val_loss: 0.1042\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.10419\n",
      "Epoch 8/150\n",
      "59980/59980 [==============================] - 604s 10ms/step - loss: 0.0977 - val_loss: 0.1042\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.10419\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010729786008596421.\n",
      "Epoch 9/150\n",
      "59980/59980 [==============================] - 591s 10ms/step - loss: 0.0976 - val_loss: 0.0952\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.10419 to 0.09522, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 10/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0955 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09522 to 0.09503, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 11/150\n",
      "59980/59980 [==============================] - 589s 10ms/step - loss: 0.0954 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09503 to 0.09502, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 12/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0954 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09502 to 0.09501, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 13/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0954 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.09501 to 0.09501, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0729786299634726e-05.\n",
      "Epoch 14/150\n",
      "59980/59980 [==============================] - 591s 10ms/step - loss: 0.0950 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.09501 to 0.09496, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 15/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0950 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09496 to 0.09492, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 16/150\n",
      "59980/59980 [==============================] - 591s 10ms/step - loss: 0.0950 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09492 to 0.09490, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 17/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.09490 to 0.09489, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 18/150\n",
      "59980/59980 [==============================] - 586s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.09489 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0729786481533665e-06.\n",
      "Epoch 19/150\n",
      "59980/59980 [==============================] - 589s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 20/150\n",
      "59980/59980 [==============================] - 589s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 21/150\n",
      "59980/59980 [==============================] - 589s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0729786481533665e-07.\n",
      "Epoch 22/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 23/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 24/150\n",
      "59980/59980 [==============================] - 591s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 25/150\n",
      "59980/59980 [==============================] - 588s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 26/150\n",
      "59980/59980 [==============================] - 589s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 27/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 28/150\n",
      "59980/59980 [==============================] - 588s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 29/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 30/150\n",
      "59980/59980 [==============================] - 589s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.09488 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 31/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 32/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 33/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 34/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 35/150\n",
      "59980/59980 [==============================] - 587s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 36/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 37/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 38/150\n",
      "59980/59980 [==============================] - 591s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 39/150\n",
      "59980/59980 [==============================] - 588s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 40/150\n",
      "59980/59980 [==============================] - 588s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 41/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 42/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 43/150\n",
      "59980/59980 [==============================] - 591s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 44/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 45/150\n",
      "59980/59980 [==============================] - 597s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 46/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 47/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 48/150\n",
      "59980/59980 [==============================] - 595s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 49/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 50/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 51/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 52/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 53/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 54/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 55/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 56/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 57/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 58/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 59/150\n",
      "59980/59980 [==============================] - 595s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 60/150\n",
      "59980/59980 [==============================] - 596s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 61/150\n",
      "59980/59980 [==============================] - 595s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 62/150\n",
      "59980/59980 [==============================] - 596s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 63/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 64/150\n",
      "59980/59980 [==============================] - 599s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 65/150\n",
      "59980/59980 [==============================] - 596s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 66/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 67/150\n",
      "59980/59980 [==============================] - 597s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 68/150\n",
      "59980/59980 [==============================] - 598s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 69/150\n",
      "59980/59980 [==============================] - 597s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 70/150\n",
      "59980/59980 [==============================] - 596s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 71/150\n",
      "59980/59980 [==============================] - 600s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 72/150\n",
      "59980/59980 [==============================] - 597s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 73/150\n",
      "59980/59980 [==============================] - 598s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 74/150\n",
      "59980/59980 [==============================] - 598s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 75/150\n",
      "59980/59980 [==============================] - 598s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 76/150\n",
      "59980/59980 [==============================] - 600s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 77/150\n",
      "59980/59980 [==============================] - 600s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 78/150\n",
      "59980/59980 [==============================] - 597s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 79/150\n",
      "59980/59980 [==============================] - 601s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 80/150\n",
      "59980/59980 [==============================] - 599s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 81/150\n",
      "59980/59980 [==============================] - 598s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 82/150\n",
      "59980/59980 [==============================] - 599s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 83/150\n",
      "59980/59980 [==============================] - 601s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 84/150\n",
      "59980/59980 [==============================] - 601s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 85/150\n",
      "59980/59980 [==============================] - 605s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 86/150\n",
      "59980/59980 [==============================] - 603s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 87/150\n",
      "59980/59980 [==============================] - 599s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 88/150\n",
      "59980/59980 [==============================] - 599s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 89/150\n",
      "59980/59980 [==============================] - 600s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 90/150\n",
      "59980/59980 [==============================] - 598s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 91/150\n",
      "59980/59980 [==============================] - 600s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 92/150\n",
      "59980/59980 [==============================] - 584s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 93/150\n",
      "59980/59980 [==============================] - 585s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 94/150\n",
      "59980/59980 [==============================] - 588s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 95/150\n",
      "59980/59980 [==============================] - 589s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 96/150\n",
      "59980/59980 [==============================] - 589s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 97/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 98/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 99/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 100/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 101/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 102/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00102: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 103/150\n",
      "59980/59980 [==============================] - 591s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 104/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 105/150\n",
      "59980/59980 [==============================] - 590s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 106/150\n",
      "59980/59980 [==============================] - 591s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 107/150\n",
      "59980/59980 [==============================] - 591s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 108/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 109/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 110/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 111/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 112/150\n",
      "59980/59980 [==============================] - 596s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 113/150\n",
      "59980/59980 [==============================] - 595s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 114/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 115/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 116/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 117/150\n",
      "59980/59980 [==============================] - 596s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 118/150\n",
      "59980/59980 [==============================] - 595s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 119/150\n",
      "59980/59980 [==============================] - 595s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 120/150\n",
      "59980/59980 [==============================] - 596s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 121/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 122/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 123/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00123: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 124/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 125/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 126/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.09487 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 127/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.09487 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 128/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 129/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00129: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 130/150\n",
      "59980/59980 [==============================] - 595s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 131/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 132/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00132: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 133/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 134/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 135/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00135: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 136/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 137/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 138/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 139/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 140/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 141/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00141: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 142/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 143/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 144/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00144: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 145/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 146/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 147/150\n",
      "59980/59980 [==============================] - 594s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00147: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 148/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 149/150\n",
      "59980/59980 [==============================] - 592s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 150/150\n",
      "59980/59980 [==============================] - 593s 10ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00150: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "evaluate model...\n",
      "59980/59980 [==============================] - 107s 2ms/step\n",
      "number params: [7.50980800e-01 2.67359042e-01 5.32081966e-01 7.36281687e-01\n",
      " 4.61091844e-01 7.61393261e-02 5.12000000e+02 1.00000000e+00\n",
      " 1.00000000e+01 1.50000000e+02 6.00000000e+00]\n",
      "number_layers: 6\n",
      "0.7509808004935385\n",
      "0.2673590418695857\n",
      "0.5320819663888114\n",
      "0.7362816873674496\n",
      "0.461091843861177\n",
      "Using HyperParams. units:512, batch_size:1, look_back:10, look_ahead:1, layers:6, epochs:150, learning_rate:0.07613932608119997\n",
      "dropout_list:[0.7509808004935385, 0.2673590418695857, 0.5320819663888114, 0.7362816873674496, 0.461091843861177]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 10\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59990, 10, 17)\n",
      "y look_ahead: (59990, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.7509808004935385, 0.2673590418695857, 0.5320819663888114, 0.7362816873674496, 0.461091843861177]\n",
      "build model...\n",
      "dropout l0: 0.7509808004935385\n",
      "dropout:0.2673590418695857\n",
      "dropout:0.5320819663888114\n",
      "dropout:0.7362816873674496\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_42 (LSTM)               (None, 10, 512)           1052672   \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 10, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_43 (LSTM)               (None, 10, 6)             12456     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 10, 6)             0         \n",
      "_________________________________________________________________\n",
      "lstm_44 (LSTM)               (None, 10, 6)             312       \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 10, 6)             0         \n",
      "_________________________________________________________________\n",
      "lstm_45 (LSTM)               (None, 6)                 312       \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 7         \n",
      "_________________________________________________________________\n",
      "repeat_vector_15 (RepeatVect (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 1,065,759\n",
      "Trainable params: 1,065,759\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59990 samples, validate on 59990 samples\n",
      "Epoch 1/150\n",
      "59990/59990 [==============================] - 3354s 56ms/step - loss: 2.8197e-05 - val_loss: 0.1429\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14288, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/150\n",
      "59990/59990 [==============================] - 3357s 56ms/step - loss: 2.1120e-05 - val_loss: 0.1429\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.14288\n",
      "Epoch 3/150\n",
      "59990/59990 [==============================] - 3364s 56ms/step - loss: 2.1120e-05 - val_loss: 0.1429\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.14288\n",
      "Epoch 4/150\n",
      "59990/59990 [==============================] - 3370s 56ms/step - loss: 2.1120e-05 - val_loss: 0.1429\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.14288\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0076139323413372045.\n",
      "Epoch 5/150\n",
      "59990/59990 [==============================] - 3379s 56ms/step - loss: 1.3435e-05 - val_loss: 0.1384\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.14288 to 0.13839, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 6/150\n",
      "59990/59990 [==============================] - 3378s 56ms/step - loss: 1.3025e-05 - val_loss: 0.1384\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13839\n",
      "Epoch 7/150\n",
      "59990/59990 [==============================] - 3372s 56ms/step - loss: 1.3025e-05 - val_loss: 0.1384\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13839\n",
      "Epoch 8/150\n",
      "59990/59990 [==============================] - 3375s 56ms/step - loss: 1.3025e-05 - val_loss: 0.1384\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.13839\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0007613932248204947.\n",
      "Epoch 9/150\n",
      "59990/59990 [==============================] - 3383s 56ms/step - loss: 0.0397 - val_loss: 0.1392\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.13839\n",
      "Epoch 10/150\n",
      "59990/59990 [==============================] - 3381s 56ms/step - loss: 0.0408 - val_loss: 0.1392\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.13839\n",
      "Epoch 00010: early stopping\n",
      "evaluate model...\n",
      "59990/59990 [==============================] - 294s 5ms/step\n",
      "number params: [2.46856354e-01 6.02221111e-01 5.29390059e-01 5.71747160e-01\n",
      " 6.31180199e-01 5.51029001e-02 1.60000000e+01 1.00000000e+00\n",
      " 2.00000000e+01 3.00000000e+02 3.00000000e+00]\n",
      "number_layers: 3\n",
      "0.24685635420294713\n",
      "0.6022211113701436\n",
      "Using HyperParams. units:16, batch_size:1, look_back:20, look_ahead:1, layers:3, epochs:300, learning_rate:0.055102900140652174\n",
      "dropout_list:[0.24685635420294713, 0.6022211113701436]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 20\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59980, 20, 17)\n",
      "y look_ahead: (59980, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.24685635420294713, 0.6022211113701436]\n",
      "build model...\n",
      "dropout l0: 0.24685635420294713\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_46 (LSTM)               (None, 16)                1152      \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "repeat_vector_16 (RepeatVect (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 1,169\n",
      "Trainable params: 1,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59980 samples, validate on 59980 samples\n",
      "Epoch 1/300\n",
      "59980/59980 [==============================] - 623s 10ms/step - loss: 2.3865e-05 - val_loss: 0.1422\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14217, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/300\n",
      "59980/59980 [==============================] - 623s 10ms/step - loss: 2.1128e-05 - val_loss: 0.1422\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.14217\n",
      "Epoch 3/300\n",
      "59980/59980 [==============================] - 622s 10ms/step - loss: 2.1128e-05 - val_loss: 0.1422\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.14217\n",
      "Epoch 4/300\n",
      "59980/59980 [==============================] - 622s 10ms/step - loss: 2.1128e-05 - val_loss: 0.1422\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.14217\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00551028996706009.\n",
      "Epoch 5/300\n",
      "59980/59980 [==============================] - 620s 10ms/step - loss: 2.8275e-05 - val_loss: 0.1388\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.14217 to 0.13881, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 6/300\n",
      "59980/59980 [==============================] - 622s 10ms/step - loss: 2.7304e-05 - val_loss: 0.1388\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.13881\n",
      "Epoch 7/300\n",
      "59980/59980 [==============================] - 620s 10ms/step - loss: 2.7304e-05 - val_loss: 0.1388\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13881\n",
      "Epoch 8/300\n",
      "59980/59980 [==============================] - 621s 10ms/step - loss: 2.7304e-05 - val_loss: 0.1388\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.13881\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005510290153324604.\n",
      "Epoch 9/300\n",
      "59980/59980 [==============================] - 620s 10ms/step - loss: 0.0608 - val_loss: 0.1391\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.13881\n",
      "Epoch 10/300\n",
      "59980/59980 [==============================] - 622s 10ms/step - loss: 0.0614 - val_loss: 0.1391\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.13881\n",
      "Epoch 00010: early stopping\n",
      "evaluate model...\n",
      "59980/59980 [==============================] - 88s 1ms/step\n",
      "number params: [4.33561935e-01 3.80436422e-01 6.85195145e-01 7.01525860e-01\n",
      " 4.31910251e-01 2.93688088e-02 3.20000000e+01 6.40000000e+01\n",
      " 1.50000000e+01 2.50000000e+02 5.00000000e+00]\n",
      "number_layers: 5\n",
      "0.43356193464830517\n",
      "0.3804364220946941\n",
      "0.6851951445204014\n",
      "0.7015258601451377\n",
      "Using HyperParams. units:32, batch_size:64, look_back:15, look_ahead:1, layers:5, epochs:250, learning_rate:0.0293688088293865\n",
      "dropout_list:[0.43356193464830517, 0.3804364220946941, 0.6851951445204014, 0.7015258601451377]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 15\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59985, 15, 17)\n",
      "y look_ahead: (59985, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.43356193464830517, 0.3804364220946941, 0.6851951445204014, 0.7015258601451377]\n",
      "build model...\n",
      "dropout l0: 0.43356193464830517\n",
      "dropout:0.3804364220946941\n",
      "dropout:0.6851951445204014\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_47 (LSTM)               (None, 15, 32)            4352      \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_48 (LSTM)               (None, 15, 5)             760       \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 15, 5)             0         \n",
      "_________________________________________________________________\n",
      "lstm_49 (LSTM)               (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 6         \n",
      "_________________________________________________________________\n",
      "repeat_vector_17 (RepeatVect (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 5,338\n",
      "Trainable params: 5,338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59985 samples, validate on 59985 samples\n",
      "Epoch 1/250\n",
      "59985/59985 [==============================] - 26s 429us/step - loss: 0.1413 - val_loss: 0.1202\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12019, saving model to save_stacked_checkpoint.keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/250\n",
      "59985/59985 [==============================] - 24s 398us/step - loss: 0.1421 - val_loss: 0.1200\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12019 to 0.12001, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 3/250\n",
      "59985/59985 [==============================] - 24s 405us/step - loss: 0.1419 - val_loss: 0.1200\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12001 to 0.11997, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 4/250\n",
      "59985/59985 [==============================] - 25s 413us/step - loss: 0.1419 - val_loss: 0.1200\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.11997\n",
      "Epoch 5/250\n",
      "59985/59985 [==============================] - 24s 408us/step - loss: 0.1419 - val_loss: 0.1200\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.11997\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0029368808493018153.\n",
      "Epoch 6/250\n",
      "59985/59985 [==============================] - 23s 385us/step - loss: 0.1019 - val_loss: 0.0954\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.11997 to 0.09544, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 7/250\n",
      "59985/59985 [==============================] - 23s 386us/step - loss: 0.0996 - val_loss: 0.0955\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.09544\n",
      "Epoch 8/250\n",
      "59985/59985 [==============================] - 24s 402us/step - loss: 0.0998 - val_loss: 0.0955\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09544\n",
      "Epoch 9/250\n",
      "59985/59985 [==============================] - 24s 402us/step - loss: 0.0998 - val_loss: 0.0955\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09544\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002936880802735687.\n",
      "Epoch 10/250\n",
      "59985/59985 [==============================] - 25s 411us/step - loss: 0.0957 - val_loss: 0.0950\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09544 to 0.09501, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 11/250\n",
      "59985/59985 [==============================] - 24s 398us/step - loss: 0.0954 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09501 to 0.09489, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 12/250\n",
      "59985/59985 [==============================] - 23s 390us/step - loss: 0.0954 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09489 to 0.09487, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 13/250\n",
      "59985/59985 [==============================] - 24s 392us/step - loss: 0.0954 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.09487 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 14/250\n",
      "59985/59985 [==============================] - 24s 404us/step - loss: 0.0954 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.09486 to 0.09486, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.9368809191510084e-05.\n",
      "Epoch 15/250\n",
      "59985/59985 [==============================] - 24s 408us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09486 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 16/250\n",
      "59985/59985 [==============================] - 24s 408us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 17/250\n",
      "59985/59985 [==============================] - 23s 382us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 2.9368809919105845e-06.\n",
      "Epoch 18/250\n",
      "59985/59985 [==============================] - 24s 394us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 19/250\n",
      "59985/59985 [==============================] - 24s 401us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 20/250\n",
      "59985/59985 [==============================] - 24s 406us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 2.9368809464358495e-07.\n",
      "Epoch 21/250\n",
      "59985/59985 [==============================] - 25s 415us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 22/250\n",
      "59985/59985 [==============================] - 23s 389us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 23/250\n",
      "59985/59985 [==============================] - 23s 388us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 24/250\n",
      "59985/59985 [==============================] - 24s 401us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 25/250\n",
      "59985/59985 [==============================] - 25s 409us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 26/250\n",
      "59985/59985 [==============================] - 25s 414us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 27/250\n",
      "59985/59985 [==============================] - 23s 392us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 28/250\n",
      "59985/59985 [==============================] - 23s 388us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 29/250\n",
      "59985/59985 [==============================] - 24s 403us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 30/250\n",
      "59985/59985 [==============================] - 24s 403us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 31/250\n",
      "59985/59985 [==============================] - 25s 414us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 32/250\n",
      "59985/59985 [==============================] - 24s 396us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 33/250\n",
      "59985/59985 [==============================] - 23s 385us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 34/250\n",
      "59985/59985 [==============================] - 24s 403us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 35/250\n",
      "59985/59985 [==============================] - 24s 404us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 36/250\n",
      "59985/59985 [==============================] - 24s 408us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 37/250\n",
      "59985/59985 [==============================] - 24s 398us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 38/250\n",
      "59985/59985 [==============================] - 23s 387us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 39/250\n",
      "59985/59985 [==============================] - 24s 396us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 40/250\n",
      "59985/59985 [==============================] - 24s 404us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 41/250\n",
      "59985/59985 [==============================] - 25s 413us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 42/250\n",
      "59985/59985 [==============================] - 24s 396us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 43/250\n",
      "59985/59985 [==============================] - 23s 388us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 44/250\n",
      "59985/59985 [==============================] - 24s 401us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 45/250\n",
      "59985/59985 [==============================] - 24s 406us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 46/250\n",
      "59985/59985 [==============================] - 25s 410us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 47/250\n",
      "59985/59985 [==============================] - 24s 399us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 48/250\n",
      "59985/59985 [==============================] - 23s 391us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 49/250\n",
      "59985/59985 [==============================] - 24s 401us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 50/250\n",
      "59985/59985 [==============================] - 24s 405us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 51/250\n",
      "59985/59985 [==============================] - 25s 409us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 52/250\n",
      "59985/59985 [==============================] - 24s 408us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 53/250\n",
      "59985/59985 [==============================] - 23s 390us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 54/250\n",
      "59985/59985 [==============================] - 23s 392us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 55/250\n",
      "59985/59985 [==============================] - 24s 403us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 56/250\n",
      "59985/59985 [==============================] - 24s 408us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 57/250\n",
      "59985/59985 [==============================] - 25s 412us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 58/250\n",
      "59985/59985 [==============================] - 23s 388us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 59/250\n",
      "59985/59985 [==============================] - 24s 393us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 60/250\n",
      "59985/59985 [==============================] - 24s 403us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 61/250\n",
      "59985/59985 [==============================] - 24s 405us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 62/250\n",
      "59985/59985 [==============================] - 25s 413us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 63/250\n",
      "59985/59985 [==============================] - 24s 401us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 64/250\n",
      "59985/59985 [==============================] - 23s 380us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 65/250\n",
      "59985/59985 [==============================] - 24s 394us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 66/250\n",
      "59985/59985 [==============================] - 24s 403us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 67/250\n",
      "59985/59985 [==============================] - 25s 409us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 68/250\n",
      "59985/59985 [==============================] - 25s 416us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 69/250\n",
      "59985/59985 [==============================] - 23s 379us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 70/250\n",
      "59985/59985 [==============================] - 24s 392us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 71/250\n",
      "59985/59985 [==============================] - 24s 405us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 72/250\n",
      "59985/59985 [==============================] - 25s 409us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 73/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59985/59985 [==============================] - 25s 413us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 74/250\n",
      "59985/59985 [==============================] - 23s 377us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 75/250\n",
      "59985/59985 [==============================] - 24s 396us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 76/250\n",
      "59985/59985 [==============================] - 24s 403us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 77/250\n",
      "59985/59985 [==============================] - 25s 412us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 78/250\n",
      "59985/59985 [==============================] - 25s 413us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 79/250\n",
      "59985/59985 [==============================] - 23s 390us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 80/250\n",
      "59985/59985 [==============================] - 23s 391us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 81/250\n",
      "59985/59985 [==============================] - 24s 401us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 82/250\n",
      "59985/59985 [==============================] - 24s 405us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 83/250\n",
      "59985/59985 [==============================] - 25s 417us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 84/250\n",
      "59985/59985 [==============================] - 23s 387us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 85/250\n",
      "59985/59985 [==============================] - 23s 390us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 86/250\n",
      "59985/59985 [==============================] - 24s 402us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 87/250\n",
      "59985/59985 [==============================] - 24s 408us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 88/250\n",
      "59985/59985 [==============================] - 25s 411us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 89/250\n",
      "59985/59985 [==============================] - 24s 395us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 90/250\n",
      "59985/59985 [==============================] - 23s 391us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 91/250\n",
      "59985/59985 [==============================] - 24s 395us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 92/250\n",
      "59985/59985 [==============================] - 24s 407us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 93/250\n",
      "59985/59985 [==============================] - 25s 416us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 94/250\n",
      "59985/59985 [==============================] - 24s 394us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 95/250\n",
      "59985/59985 [==============================] - 23s 389us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 96/250\n",
      "59985/59985 [==============================] - 24s 394us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 97/250\n",
      "59985/59985 [==============================] - 24s 405us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 98/250\n",
      "59985/59985 [==============================] - 25s 413us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 99/250\n",
      "59985/59985 [==============================] - 24s 401us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 100/250\n",
      "59985/59985 [==============================] - 24s 392us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 101/250\n",
      "59985/59985 [==============================] - 24s 400us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 102/250\n",
      "59985/59985 [==============================] - 24s 407us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 103/250\n",
      "59985/59985 [==============================] - 25s 415us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 104/250\n",
      "59985/59985 [==============================] - 24s 407us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 105/250\n",
      "59985/59985 [==============================] - 23s 380us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 106/250\n",
      "59985/59985 [==============================] - 24s 393us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 107/250\n",
      "59985/59985 [==============================] - 24s 401us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 108/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59985/59985 [==============================] - 25s 409us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 109/250\n",
      "59985/59985 [==============================] - 25s 413us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 110/250\n",
      "59985/59985 [==============================] - 23s 390us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00110: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 111/250\n",
      "59985/59985 [==============================] - 23s 390us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 112/250\n",
      "59985/59985 [==============================] - 24s 402us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 113/250\n",
      "59985/59985 [==============================] - 24s 403us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00113: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 114/250\n",
      "59985/59985 [==============================] - 25s 413us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 115/250\n",
      "59985/59985 [==============================] - 24s 395us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 116/250\n",
      "59985/59985 [==============================] - 23s 389us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 117/250\n",
      "59985/59985 [==============================] - 24s 397us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 118/250\n",
      "59985/59985 [==============================] - 25s 409us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 119/250\n",
      "59985/59985 [==============================] - 25s 412us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 120/250\n",
      "59985/59985 [==============================] - 24s 400us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 121/250\n",
      "59985/59985 [==============================] - 23s 384us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 122/250\n",
      "59985/59985 [==============================] - 24s 396us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00122: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 123/250\n",
      "59985/59985 [==============================] - 24s 405us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 124/250\n",
      "59985/59985 [==============================] - 25s 416us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 125/250\n",
      "59985/59985 [==============================] - 24s 402us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00125: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 126/250\n",
      "59985/59985 [==============================] - 23s 385us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 127/250\n",
      "59985/59985 [==============================] - 24s 397us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 128/250\n",
      "59985/59985 [==============================] - 24s 405us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00128: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 129/250\n",
      "59985/59985 [==============================] - 25s 412us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 130/250\n",
      "59985/59985 [==============================] - 24s 407us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 131/250\n",
      "59985/59985 [==============================] - 23s 380us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 132/250\n",
      "59985/59985 [==============================] - 24s 396us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 133/250\n",
      "59985/59985 [==============================] - 24s 404us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 134/250\n",
      "59985/59985 [==============================] - 24s 407us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00134: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 135/250\n",
      "59985/59985 [==============================] - 25s 412us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 136/250\n",
      "59985/59985 [==============================] - 23s 387us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 137/250\n",
      "59985/59985 [==============================] - 24s 396us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00137: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 138/250\n",
      "59985/59985 [==============================] - 24s 401us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 139/250\n",
      "59985/59985 [==============================] - 24s 403us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 140/250\n",
      "59985/59985 [==============================] - 25s 414us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00140: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 141/250\n",
      "59985/59985 [==============================] - 24s 397us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 142/250\n",
      "59985/59985 [==============================] - 23s 388us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 143/250\n",
      "59985/59985 [==============================] - 24s 397us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00143: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 144/250\n",
      "59985/59985 [==============================] - 24s 405us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 145/250\n",
      "59985/59985 [==============================] - 25s 411us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 146/250\n",
      "59985/59985 [==============================] - 24s 399us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00146: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 147/250\n",
      "59985/59985 [==============================] - 23s 384us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 148/250\n",
      "59985/59985 [==============================] - 24s 395us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 149/250\n",
      "59985/59985 [==============================] - 24s 399us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00149: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 150/250\n",
      "59985/59985 [==============================] - 25s 411us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 151/250\n",
      "59985/59985 [==============================] - 24s 399us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 152/250\n",
      "59985/59985 [==============================] - 23s 379us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00152: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 153/250\n",
      "59985/59985 [==============================] - 24s 395us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 154/250\n",
      "59985/59985 [==============================] - 24s 403us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 155/250\n",
      "59985/59985 [==============================] - 24s 407us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00155: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 156/250\n",
      "59985/59985 [==============================] - 25s 413us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 157/250\n",
      "59985/59985 [==============================] - 23s 387us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 158/250\n",
      "59985/59985 [==============================] - 23s 391us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00158: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 159/250\n",
      "59985/59985 [==============================] - 24s 402us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 160/250\n",
      "59985/59985 [==============================] - 24s 405us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 161/250\n",
      "59985/59985 [==============================] - 25s 409us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00161: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 162/250\n",
      "59985/59985 [==============================] - 24s 393us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 163/250\n",
      "59985/59985 [==============================] - 24s 395us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 164/250\n",
      "59985/59985 [==============================] - 24s 397us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 165/250\n",
      "59985/59985 [==============================] - 24s 406us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 166/250\n",
      "59985/59985 [==============================] - 25s 412us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 167/250\n",
      "59985/59985 [==============================] - 24s 401us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00167: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 168/250\n",
      "59985/59985 [==============================] - 23s 389us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 169/250\n",
      "59985/59985 [==============================] - 24s 392us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 170/250\n",
      "59985/59985 [==============================] - 24s 402us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00170: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 171/250\n",
      "59985/59985 [==============================] - 24s 404us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 172/250\n",
      "59985/59985 [==============================] - 25s 413us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 173/250\n",
      "59985/59985 [==============================] - 24s 397us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00173: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 174/250\n",
      "59985/59985 [==============================] - 23s 391us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 175/250\n",
      "59985/59985 [==============================] - 24s 397us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 176/250\n",
      "59985/59985 [==============================] - 24s 406us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00176: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 177/250\n",
      "59985/59985 [==============================] - 25s 413us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 178/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59985/59985 [==============================] - 24s 401us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 179/250\n",
      "59985/59985 [==============================] - 23s 386us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00179: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 180/250\n",
      "59985/59985 [==============================] - 24s 397us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 181/250\n",
      "59985/59985 [==============================] - 24s 406us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 182/250\n",
      "59985/59985 [==============================] - 25s 410us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00182: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 183/250\n",
      "59985/59985 [==============================] - 24s 405us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 184/250\n",
      "59985/59985 [==============================] - 23s 383us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 185/250\n",
      "59985/59985 [==============================] - 24s 397us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00185: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 186/250\n",
      "59985/59985 [==============================] - 24s 403us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 187/250\n",
      "59985/59985 [==============================] - 24s 408us/step - loss: 0.0948 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 188/250\n",
      "59985/59985 [==============================] - 25s 412us/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00188: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 189/250\n",
      "59985/59985 [==============================] - 23s 386us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 190/250\n",
      "59985/59985 [==============================] - 24s 397us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 191/250\n",
      "59985/59985 [==============================] - 24s 401us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00191: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 192/250\n",
      "59985/59985 [==============================] - 24s 404us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 193/250\n",
      "59985/59985 [==============================] - 25s 416us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 194/250\n",
      "59985/59985 [==============================] - 23s 389us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00194: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 195/250\n",
      "59985/59985 [==============================] - 24s 393us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 196/250\n",
      "59985/59985 [==============================] - 24s 400us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 197/250\n",
      "59985/59985 [==============================] - 24s 408us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00197: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 198/250\n",
      "59985/59985 [==============================] - 25s 415us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 199/250\n",
      "59985/59985 [==============================] - 23s 390us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 200/250\n",
      "59985/59985 [==============================] - 24s 395us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00200: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 201/250\n",
      "59985/59985 [==============================] - 24s 399us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 202/250\n",
      "59985/59985 [==============================] - 24s 406us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 203/250\n",
      "59985/59985 [==============================] - 25s 412us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00203: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 204/250\n",
      "59985/59985 [==============================] - 24s 400us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 205/250\n",
      "59985/59985 [==============================] - 24s 393us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 206/250\n",
      "59985/59985 [==============================] - 24s 399us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00206: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 207/250\n",
      "59985/59985 [==============================] - 24s 407us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 208/250\n",
      "59985/59985 [==============================] - 25s 411us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 209/250\n",
      "59985/59985 [==============================] - 24s 406us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00209: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 210/250\n",
      "59985/59985 [==============================] - 23s 384us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 211/250\n",
      "59985/59985 [==============================] - 24s 401us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 212/250\n",
      "59985/59985 [==============================] - 24s 405us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00212: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 213/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59985/59985 [==============================] - 25s 412us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 214/250\n",
      "59985/59985 [==============================] - 24s 399us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 215/250\n",
      "59985/59985 [==============================] - 23s 386us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00215: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 216/250\n",
      "59985/59985 [==============================] - 23s 391us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 217/250\n",
      "59985/59985 [==============================] - 24s 402us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 218/250\n",
      "59985/59985 [==============================] - 24s 405us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00218: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 219/250\n",
      "59985/59985 [==============================] - 25s 414us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 220/250\n",
      "59985/59985 [==============================] - 24s 394us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 221/250\n",
      "59985/59985 [==============================] - 23s 388us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00221: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 222/250\n",
      "59985/59985 [==============================] - 24s 398us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 223/250\n",
      "59985/59985 [==============================] - 24s 405us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 224/250\n",
      "59985/59985 [==============================] - 25s 413us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00224: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 225/250\n",
      "59985/59985 [==============================] - 24s 394us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 226/250\n",
      "59985/59985 [==============================] - 23s 389us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 227/250\n",
      "59985/59985 [==============================] - 24s 404us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00227: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 228/250\n",
      "59985/59985 [==============================] - 24s 406us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 229/250\n",
      "59985/59985 [==============================] - 25s 416us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 230/250\n",
      "59985/59985 [==============================] - 24s 397us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00230: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 231/250\n",
      "59985/59985 [==============================] - 23s 387us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 232/250\n",
      "59985/59985 [==============================] - 24s 402us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 233/250\n",
      "59985/59985 [==============================] - 24s 403us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00233: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 234/250\n",
      "59985/59985 [==============================] - 25s 413us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 235/250\n",
      "59985/59985 [==============================] - 24s 401us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 236/250\n",
      "59985/59985 [==============================] - 23s 389us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00236: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 237/250\n",
      "59985/59985 [==============================] - 24s 400us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 238/250\n",
      "59985/59985 [==============================] - 24s 400us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 239/250\n",
      "59985/59985 [==============================] - 24s 408us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00239: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 240/250\n",
      "59985/59985 [==============================] - 24s 400us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 241/250\n",
      "59985/59985 [==============================] - 23s 387us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 242/250\n",
      "59985/59985 [==============================] - 24s 395us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00242: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 243/250\n",
      "59985/59985 [==============================] - 24s 407us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 244/250\n",
      "59985/59985 [==============================] - 25s 413us/step - loss: 0.0949 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 245/250\n",
      "59985/59985 [==============================] - 24s 402us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00245: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 246/250\n",
      "59985/59985 [==============================] - 23s 388us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 247/250\n",
      "59985/59985 [==============================] - 24s 396us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 248/250\n",
      "59985/59985 [==============================] - 24s 404us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00248: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 249/250\n",
      "59985/59985 [==============================] - 25s 412us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 250/250\n",
      "59985/59985 [==============================] - 24s 400us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.09485 to 0.09485, saving model to save_stacked_checkpoint.keras\n",
      "evaluate model...\n",
      "59985/59985 [==============================] - 5s 77us/step\n",
      "number params: [7.42142979e-01 6.50102298e-01 2.88251694e-01 6.10567557e-01\n",
      " 2.19344967e-01 7.21796164e-02 1.28000000e+02 1.60000000e+01\n",
      " 4.00000000e+01 3.00000000e+02 3.00000000e+00]\n",
      "number_layers: 3\n",
      "0.742142978994895\n",
      "0.6501022984121871\n",
      "Using HyperParams. units:128, batch_size:16, look_back:40, look_ahead:1, layers:3, epochs:300, learning_rate:0.07217961638463444\n",
      "dropout_list:[0.742142978994895, 0.6501022984121871]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 40\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59960, 40, 17)\n",
      "y look_ahead: (59960, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.742142978994895, 0.6501022984121871]\n",
      "build model...\n",
      "dropout l0: 0.742142978994895\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_50 (LSTM)               (None, 128)               66560     \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "repeat_vector_18 (RepeatVect (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 66,689\n",
      "Trainable params: 66,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59960 samples, validate on 59960 samples\n",
      "Epoch 1/300\n",
      "59960/59960 [==============================] - 345s 6ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00001: val_loss did not improve from inf\n",
      "Epoch 2/300\n",
      "59960/59960 [==============================] - 341s 6ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00002: val_loss did not improve from inf\n",
      "Epoch 3/300\n",
      "59960/59960 [==============================] - 341s 6ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00003: val_loss did not improve from inf\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.007217961549758911.\n",
      "Epoch 4/300\n",
      "59960/59960 [==============================] - 316s 5ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00004: val_loss did not improve from inf\n",
      "Epoch 5/300\n",
      "59960/59960 [==============================] - 317s 5ms/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00005: val_loss did not improve from inf\n",
      "Epoch 00005: early stopping\n",
      "evaluate model...\n",
      "59960/59960 [==============================] - 73s 1ms/step\n",
      "number params: [2.89205545e-01 5.20694775e-01 6.39689583e-01 2.84958734e-01\n",
      " 7.77963293e-01 3.58469640e-02 2.56000000e+02 6.40000000e+01\n",
      " 3.00000000e+01 1.00000000e+02 6.00000000e+00]\n",
      "number_layers: 6\n",
      "0.2892055445138313\n",
      "0.5206947752526307\n",
      "0.639689583370217\n",
      "0.2849587340574895\n",
      "0.777963293346781\n",
      "Using HyperParams. units:256, batch_size:64, look_back:30, look_ahead:1, layers:6, epochs:100, learning_rate:0.03584696397654057\n",
      "dropout_list:[0.2892055445138313, 0.5206947752526307, 0.639689583370217, 0.2849587340574895, 0.777963293346781]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 30\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59970, 30, 17)\n",
      "y look_ahead: (59970, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.2892055445138313, 0.5206947752526307, 0.639689583370217, 0.2849587340574895, 0.777963293346781]\n",
      "build model...\n",
      "dropout l0: 0.2892055445138313\n",
      "dropout:0.5206947752526307\n",
      "dropout:0.639689583370217\n",
      "dropout:0.2849587340574895\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_51 (LSTM)               (None, 30, 256)           264192    \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_52 (LSTM)               (None, 30, 6)             6312      \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 30, 6)             0         \n",
      "_________________________________________________________________\n",
      "lstm_53 (LSTM)               (None, 30, 6)             312       \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 30, 6)             0         \n",
      "_________________________________________________________________\n",
      "lstm_54 (LSTM)               (None, 6)                 312       \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 7         \n",
      "_________________________________________________________________\n",
      "repeat_vector_19 (RepeatVect (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 271,135\n",
      "Trainable params: 271,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59970 samples, validate on 59970 samples\n",
      "Epoch 1/100\n",
      "59970/59970 [==============================] - 417s 7ms/step - loss: 0.1489 - val_loss: 0.1280\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12799, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/100\n",
      "59970/59970 [==============================] - 472s 8ms/step - loss: 0.1516 - val_loss: 0.1280\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12799 to 0.12797, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 3/100\n",
      "59970/59970 [==============================] - 468s 8ms/step - loss: 0.1516 - val_loss: 0.1277\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12797 to 0.12773, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 4/100\n",
      "59970/59970 [==============================] - 468s 8ms/step - loss: 0.1516 - val_loss: 0.1277\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12773 to 0.12772, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 5/100\n",
      "59970/59970 [==============================] - 473s 8ms/step - loss: 0.1516 - val_loss: 0.1277\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.12772 to 0.12770, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 6/100\n",
      "59970/59970 [==============================] - 473s 8ms/step - loss: 0.1516 - val_loss: 0.1277\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.12770\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0035846963524818424.\n",
      "Epoch 7/100\n",
      "59970/59970 [==============================] - 440s 7ms/step - loss: 0.1031 - val_loss: 0.0957\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12770 to 0.09574, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 8/100\n",
      "59970/59970 [==============================] - 442s 7ms/step - loss: 0.1006 - val_loss: 0.0958\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09574\n",
      "Epoch 9/100\n",
      "59970/59970 [==============================] - 433s 7ms/step - loss: 0.1009 - val_loss: 0.0959\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09574\n",
      "Epoch 10/100\n",
      "59970/59970 [==============================] - 436s 7ms/step - loss: 0.1010 - val_loss: 0.0959\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.09574\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00035846964456141.\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59970/59970 [==============================] - 442s 7ms/step - loss: 0.0960 - val_loss: 0.0951\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09574 to 0.09505, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 12/100\n",
      "59970/59970 [==============================] - 441s 7ms/step - loss: 0.0956 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09505 to 0.09492, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 13/100\n",
      "59970/59970 [==============================] - 436s 7ms/step - loss: 0.0955 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.09492 to 0.09489, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 14/100\n",
      "59970/59970 [==============================] - 443s 7ms/step - loss: 0.0955 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.09489 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 15/100\n",
      "59970/59970 [==============================] - 439s 7ms/step - loss: 0.0955 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.584696387406439e-05.\n",
      "Epoch 16/100\n",
      "59970/59970 [==============================] - 437s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 17/100\n",
      "59970/59970 [==============================] - 439s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 18/100\n",
      "59970/59970 [==============================] - 438s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.5846962418872865e-06.\n",
      "Epoch 19/100\n",
      "59970/59970 [==============================] - 443s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 20/100\n",
      "59970/59970 [==============================] - 438s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 21/100\n",
      "59970/59970 [==============================] - 441s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.5846962873620216e-07.\n",
      "Epoch 22/100\n",
      "59970/59970 [==============================] - 444s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 23/100\n",
      "59970/59970 [==============================] - 436s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 24/100\n",
      "59970/59970 [==============================] - 440s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 25/100\n",
      "59970/59970 [==============================] - 440s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 26/100\n",
      "59970/59970 [==============================] - 438s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 27/100\n",
      "59970/59970 [==============================] - 446s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 28/100\n",
      "59970/59970 [==============================] - 440s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 29/100\n",
      "59970/59970 [==============================] - 437s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 30/100\n",
      "59970/59970 [==============================] - 439s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 31/100\n",
      "59970/59970 [==============================] - 440s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 32/100\n",
      "59970/59970 [==============================] - 437s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 33/100\n",
      "59970/59970 [==============================] - 447s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 34/100\n",
      "59970/59970 [==============================] - 441s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 35/100\n",
      "59970/59970 [==============================] - 437s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 36/100\n",
      "59970/59970 [==============================] - 437s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 37/100\n",
      "59970/59970 [==============================] - 444s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 38/100\n",
      "59970/59970 [==============================] - 437s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 39/100\n",
      "59970/59970 [==============================] - 442s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 40/100\n",
      "59970/59970 [==============================] - 444s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 41/100\n",
      "59970/59970 [==============================] - 443s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 42/100\n",
      "59970/59970 [==============================] - 441s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 43/100\n",
      "59970/59970 [==============================] - 436s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 44/100\n",
      "59970/59970 [==============================] - 437s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 45/100\n",
      "59970/59970 [==============================] - 437s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 46/100\n",
      "59970/59970 [==============================] - 437s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 47/100\n",
      "59970/59970 [==============================] - 435s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 48/100\n",
      "59970/59970 [==============================] - 436s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 49/100\n",
      "59970/59970 [==============================] - 440s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 50/100\n",
      "59970/59970 [==============================] - 440s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 51/100\n",
      "59970/59970 [==============================] - 440s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 52/100\n",
      "59970/59970 [==============================] - 439s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 53/100\n",
      "59970/59970 [==============================] - 440s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 54/100\n",
      "59970/59970 [==============================] - 449s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 55/100\n",
      "59970/59970 [==============================] - 443s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 56/100\n",
      "59970/59970 [==============================] - 433s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 57/100\n",
      "59970/59970 [==============================] - 441s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 58/100\n",
      "59970/59970 [==============================] - 440s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 59/100\n",
      "59970/59970 [==============================] - 434s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 60/100\n",
      "59970/59970 [==============================] - 434s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 61/100\n",
      "59970/59970 [==============================] - 444s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 62/100\n",
      "59970/59970 [==============================] - 438s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 63/100\n",
      "59970/59970 [==============================] - 437s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 64/100\n",
      "59970/59970 [==============================] - 441s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 65/100\n",
      "59970/59970 [==============================] - 443s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 66/100\n",
      "59970/59970 [==============================] - 439s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 67/100\n",
      "59970/59970 [==============================] - 441s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 68/100\n",
      "59970/59970 [==============================] - 437s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 69/100\n",
      "59970/59970 [==============================] - 439s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 70/100\n",
      "59970/59970 [==============================] - 439s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 71/100\n",
      "59970/59970 [==============================] - 439s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 72/100\n",
      "59970/59970 [==============================] - 439s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 73/100\n",
      "59970/59970 [==============================] - 441s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 74/100\n",
      "59970/59970 [==============================] - 443s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 75/100\n",
      "59970/59970 [==============================] - 440s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 76/100\n",
      "59970/59970 [==============================] - 440s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 77/100\n",
      "59970/59970 [==============================] - 443s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 78/100\n",
      "59970/59970 [==============================] - 445s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 79/100\n",
      "59970/59970 [==============================] - 442s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 80/100\n",
      "59970/59970 [==============================] - 440s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 81/100\n",
      "59970/59970 [==============================] - 444s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00081: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 82/100\n",
      "59970/59970 [==============================] - 444s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 83/100\n",
      "59970/59970 [==============================] - 433s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 84/100\n",
      "59970/59970 [==============================] - 439s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 85/100\n",
      "59970/59970 [==============================] - 443s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 86/100\n",
      "59970/59970 [==============================] - 436s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 87/100\n",
      "59970/59970 [==============================] - 442s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 88/100\n",
      "59970/59970 [==============================] - 441s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 89/100\n",
      "59970/59970 [==============================] - 442s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 90/100\n",
      "59970/59970 [==============================] - 438s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 91/100\n",
      "59970/59970 [==============================] - 446s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 92/100\n",
      "59970/59970 [==============================] - 442s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 93/100\n",
      "59970/59970 [==============================] - 440s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 94/100\n",
      "59970/59970 [==============================] - 440s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 95/100\n",
      "59970/59970 [==============================] - 437s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 96/100\n",
      "59970/59970 [==============================] - 436s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 97/100\n",
      "59970/59970 [==============================] - 436s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 98/100\n",
      "59970/59970 [==============================] - 448s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 99/100\n",
      "59970/59970 [==============================] - 441s 7ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 100/100\n",
      "59970/59970 [==============================] - 455s 8ms/step - loss: 0.0949 - val_loss: 0.0949\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.09488 to 0.09488, saving model to save_stacked_checkpoint.keras\n",
      "evaluate model...\n",
      "59970/59970 [==============================] - 114s 2ms/step\n",
      "number params: [4.42687823e-01 2.98183372e-01 4.67132630e-01 5.04046051e-01\n",
      " 6.92218594e-01 8.09001653e-02 6.40000000e+01 3.20000000e+01\n",
      " 4.00000000e+01 1.00000000e+02 3.00000000e+00]\n",
      "number_layers: 3\n",
      "0.4426878231320425\n",
      "0.2981833721270958\n",
      "Using HyperParams. units:64, batch_size:32, look_back:40, look_ahead:1, layers:3, epochs:100, learning_rate:0.0809001652629801\n",
      "dropout_list:[0.4426878231320425, 0.2981833721270958]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 40\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59960, 40, 17)\n",
      "y look_ahead: (59960, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.4426878231320425, 0.2981833721270958]\n",
      "build model...\n",
      "dropout l0: 0.4426878231320425\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_55 (LSTM)               (None, 64)                16896     \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "repeat_vector_20 (RepeatVect (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 16,961\n",
      "Trainable params: 16,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59960 samples, validate on 59960 samples\n",
      "Epoch 1/100\n",
      "59960/59960 [==============================] - 46s 764us/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00001: val_loss did not improve from inf\n",
      "Epoch 2/100\n",
      "59960/59960 [==============================] - 45s 755us/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00002: val_loss did not improve from inf\n",
      "Epoch 3/100\n",
      "59960/59960 [==============================] - 45s 757us/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00003: val_loss did not improve from inf\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00809001624584198.\n",
      "Epoch 4/100\n",
      "59960/59960 [==============================] - 45s 747us/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00004: val_loss did not improve from inf\n",
      "Epoch 5/100\n",
      "59960/59960 [==============================] - 41s 683us/step - loss: nan - val_loss: nan\n",
      "\n",
      "Epoch 00005: val_loss did not improve from inf\n",
      "Epoch 00005: early stopping\n",
      "evaluate model...\n",
      "59960/59960 [==============================] - 7s 117us/step\n",
      "number params: [ 0.39452514  0.65473954  0.70902043  0.40007217  0.35843915  0.04417292\n",
      " 32.          8.         40.         25.          3.        ]\n",
      "number_layers: 3\n",
      "0.39452514221154944\n",
      "0.6547395369280231\n",
      "Using HyperParams. units:32, batch_size:8, look_back:40, look_ahead:1, layers:3, epochs:25, learning_rate:0.044172920736170594\n",
      "dropout_list:[0.39452514221154944, 0.6547395369280231]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 40\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59960, 40, 17)\n",
      "y look_ahead: (59960, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.39452514221154944, 0.6547395369280231]\n",
      "build model...\n",
      "dropout l0: 0.39452514221154944\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_56 (LSTM)               (None, 32)                4352      \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "repeat_vector_21 (RepeatVect (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59960 samples, validate on 59960 samples\n",
      "Epoch 1/25\n",
      "59960/59960 [==============================] - 149s 2ms/step - loss: 71224.9708 - val_loss: 0.1004\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10037, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/25\n",
      "59960/59960 [==============================] - 150s 3ms/step - loss: 0.0664 - val_loss: 0.1353\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.10037\n",
      "Epoch 3/25\n",
      "59960/59960 [==============================] - 152s 3ms/step - loss: 0.0106 - val_loss: 0.1395\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.10037\n",
      "Epoch 4/25\n",
      "59960/59960 [==============================] - 152s 3ms/step - loss: 0.0076 - val_loss: 0.1398\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.10037\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.004417292028665543.\n",
      "Epoch 5/25\n",
      "59960/59960 [==============================] - 151s 3ms/step - loss: 0.0543 - val_loss: 0.1040\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.10037\n",
      "Epoch 6/25\n",
      "59960/59960 [==============================] - 151s 3ms/step - loss: 0.0533 - val_loss: 0.1031\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.10037\n",
      "Epoch 00006: early stopping\n",
      "evaluate model...\n",
      "59960/59960 [==============================] - 19s 324us/step\n",
      "number params: [7.23937501e-01 7.26848371e-01 2.88625054e-01 5.39483084e-01\n",
      " 4.52599097e-01 1.15337285e-02 1.60000000e+01 3.20000000e+01\n",
      " 5.00000000e+00 2.50000000e+02 4.00000000e+00]\n",
      "number_layers: 4\n",
      "0.7239375012303313\n",
      "0.7268483714708558\n",
      "0.2886250540358979\n",
      "Using HyperParams. units:16, batch_size:32, look_back:5, look_ahead:1, layers:4, epochs:250, learning_rate:0.011533728468874446\n",
      "dropout_list:[0.7239375012303313, 0.7268483714708558, 0.2886250540358979]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 5\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59995, 5, 17)\n",
      "y look_ahead: (59995, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.7239375012303313, 0.7268483714708558, 0.2886250540358979]\n",
      "build model...\n",
      "dropout l0: 0.7239375012303313\n",
      "dropout:0.7268483714708558\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_57 (LSTM)               (None, 5, 16)             1152      \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 5, 16)             0         \n",
      "_________________________________________________________________\n",
      "lstm_58 (LSTM)               (None, 4)                 336       \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 5         \n",
      "_________________________________________________________________\n",
      "repeat_vector_22 (RepeatVect (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 1,493\n",
      "Trainable params: 1,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59995 samples, validate on 59995 samples\n",
      "Epoch 1/250\n",
      "59995/59995 [==============================] - 16s 264us/step - loss: 0.1085 - val_loss: 0.1228\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12276, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/250\n",
      "59995/59995 [==============================] - 13s 213us/step - loss: 0.0708 - val_loss: 0.1233\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.12276\n",
      "Epoch 3/250\n",
      "59995/59995 [==============================] - 13s 215us/step - loss: 0.0698 - val_loss: 0.1253\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.12276\n",
      "Epoch 4/250\n",
      "59995/59995 [==============================] - 13s 221us/step - loss: 0.0698 - val_loss: 0.1174\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12276 to 0.11744, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 5/250\n",
      "59995/59995 [==============================] - 15s 256us/step - loss: 0.0664 - val_loss: 0.1200\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.11744\n",
      "Epoch 6/250\n",
      "59995/59995 [==============================] - 13s 225us/step - loss: 0.0670 - val_loss: 0.1171\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.11744 to 0.11713, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 7/250\n",
      "59995/59995 [==============================] - 15s 254us/step - loss: 0.0665 - val_loss: 0.1254\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.11713\n",
      "Epoch 8/250\n",
      "59995/59995 [==============================] - 13s 215us/step - loss: 0.0658 - val_loss: 0.1184\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.11713\n",
      "Epoch 9/250\n",
      "59995/59995 [==============================] - 13s 215us/step - loss: 0.0654 - val_loss: 0.1227\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11713\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0011533728800714016.\n",
      "Epoch 10/250\n",
      "59995/59995 [==============================] - 14s 232us/step - loss: 0.0549 - val_loss: 0.0750\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.11713 to 0.07504, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 11/250\n",
      "59995/59995 [==============================] - 15s 243us/step - loss: 0.0506 - val_loss: 0.0745\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.07504 to 0.07445, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 12/250\n",
      "59995/59995 [==============================] - 14s 231us/step - loss: 0.0501 - val_loss: 0.0760\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.07445\n",
      "Epoch 13/250\n",
      "59995/59995 [==============================] - 14s 239us/step - loss: 0.0501 - val_loss: 0.0759\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.07445\n",
      "Epoch 14/250\n",
      "59995/59995 [==============================] - 14s 232us/step - loss: 0.0499 - val_loss: 0.0751\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.07445\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00011533729266375304.\n",
      "Epoch 15/250\n",
      "59995/59995 [==============================] - 14s 238us/step - loss: 0.0481 - val_loss: 0.0733\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.07445 to 0.07326, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 16/250\n",
      "59995/59995 [==============================] - 14s 225us/step - loss: 0.0476 - val_loss: 0.0730\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.07326 to 0.07303, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 17/250\n",
      "59995/59995 [==============================] - 13s 214us/step - loss: 0.0483 - val_loss: 0.0727\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.07303 to 0.07270, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 18/250\n",
      "59995/59995 [==============================] - 14s 231us/step - loss: 0.0475 - val_loss: 0.0724\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.07270 to 0.07239, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 19/250\n",
      "59995/59995 [==============================] - 15s 244us/step - loss: 0.0472 - val_loss: 0.0727\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.07239\n",
      "Epoch 20/250\n",
      "59995/59995 [==============================] - 13s 215us/step - loss: 0.0470 - val_loss: 0.0726\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.07239\n",
      "Epoch 21/250\n",
      "59995/59995 [==============================] - 13s 217us/step - loss: 0.0472 - val_loss: 0.0724\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.07239 to 0.07238, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.1533729411894457e-05.\n",
      "Epoch 22/250\n",
      "59995/59995 [==============================] - 15s 247us/step - loss: 0.0470 - val_loss: 0.0723\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.07238 to 0.07228, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 23/250\n",
      "59995/59995 [==============================] - 14s 227us/step - loss: 0.0474 - val_loss: 0.0722\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.07228 to 0.07220, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 24/250\n",
      "59995/59995 [==============================] - 15s 246us/step - loss: 0.0472 - val_loss: 0.0721\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.07220 to 0.07214, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 25/250\n",
      "59995/59995 [==============================] - 14s 228us/step - loss: 0.0470 - val_loss: 0.0721\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.07214 to 0.07212, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 26/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59995/59995 [==============================] - 15s 247us/step - loss: 0.0470 - val_loss: 0.0721\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.07212 to 0.07210, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 27/250\n",
      "59995/59995 [==============================] - 14s 226us/step - loss: 0.0473 - val_loss: 0.0721\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.07210 to 0.07205, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.1533729775692336e-06.\n",
      "Epoch 28/250\n",
      "59995/59995 [==============================] - 14s 227us/step - loss: 0.0469 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.07205 to 0.07205, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 29/250\n",
      "59995/59995 [==============================] - 13s 211us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.07205 to 0.07205, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 30/250\n",
      "59995/59995 [==============================] - 13s 213us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.07205 to 0.07204, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 31/250\n",
      "59995/59995 [==============================] - 14s 236us/step - loss: 0.0469 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.07204 to 0.07204, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 32/250\n",
      "59995/59995 [==============================] - 14s 237us/step - loss: 0.0477 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.07204 to 0.07204, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 33/250\n",
      "59995/59995 [==============================] - 14s 234us/step - loss: 0.0468 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.07204 to 0.07204, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.1533729775692337e-07.\n",
      "Epoch 34/250\n",
      "59995/59995 [==============================] - 14s 235us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.07204 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 35/250\n",
      "59995/59995 [==============================] - 14s 235us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 36/250\n",
      "59995/59995 [==============================] - 14s 237us/step - loss: 0.0477 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 37/250\n",
      "59995/59995 [==============================] - 14s 235us/step - loss: 0.0468 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 38/250\n",
      "59995/59995 [==============================] - 14s 235us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 39/250\n",
      "59995/59995 [==============================] - 13s 211us/step - loss: 0.0477 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 40/250\n",
      "59995/59995 [==============================] - 14s 230us/step - loss: 0.0478 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 41/250\n",
      "59995/59995 [==============================] - 14s 242us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 42/250\n",
      "59995/59995 [==============================] - 14s 229us/step - loss: 0.0466 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 43/250\n",
      "59995/59995 [==============================] - 14s 240us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 44/250\n",
      "59995/59995 [==============================] - 14s 230us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 45/250\n",
      "59995/59995 [==============================] - 14s 239us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 46/250\n",
      "59995/59995 [==============================] - 14s 230us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 47/250\n",
      "59995/59995 [==============================] - 14s 239us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 48/250\n",
      "59995/59995 [==============================] - 14s 230us/step - loss: 0.0468 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 49/250\n",
      "59995/59995 [==============================] - 14s 238us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 50/250\n",
      "59995/59995 [==============================] - 13s 212us/step - loss: 0.0475 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 51/250\n",
      "59995/59995 [==============================] - 13s 218us/step - loss: 0.0467 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.07203 to 0.07203, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 52/250\n",
      "59995/59995 [==============================] - 15s 254us/step - loss: 0.0466 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.07203 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 53/250\n",
      "59995/59995 [==============================] - 13s 223us/step - loss: 0.0469 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 54/250\n",
      "59995/59995 [==============================] - 15s 252us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 55/250\n",
      "59995/59995 [==============================] - 13s 216us/step - loss: 0.0477 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 56/250\n",
      "59995/59995 [==============================] - 13s 214us/step - loss: 0.0468 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 57/250\n",
      "59995/59995 [==============================] - 13s 224us/step - loss: 0.0476 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 58/250\n",
      "59995/59995 [==============================] - 15s 251us/step - loss: 0.0475 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 59/250\n",
      "59995/59995 [==============================] - 13s 224us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 60/250\n",
      "59995/59995 [==============================] - 15s 249us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 61/250\n",
      "59995/59995 [==============================] - 13s 224us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 62/250\n",
      "59995/59995 [==============================] - 15s 248us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 63/250\n",
      "59995/59995 [==============================] - 13s 224us/step - loss: 0.0468 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 64/250\n",
      "59995/59995 [==============================] - 15s 245us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 65/250\n",
      "59995/59995 [==============================] - 13s 211us/step - loss: 0.0475 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 66/250\n",
      "59995/59995 [==============================] - 13s 221us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 67/250\n",
      "59995/59995 [==============================] - 15s 257us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 68/250\n",
      "59995/59995 [==============================] - 13s 218us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 69/250\n",
      "59995/59995 [==============================] - 15s 255us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 70/250\n",
      "59995/59995 [==============================] - 13s 216us/step - loss: 0.0468 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.07202 to 0.07202, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 71/250\n",
      "59995/59995 [==============================] - 15s 251us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.07202 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 72/250\n",
      "59995/59995 [==============================] - 13s 217us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 73/250\n",
      "59995/59995 [==============================] - 15s 253us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 74/250\n",
      "59995/59995 [==============================] - 13s 217us/step - loss: 0.0477 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 75/250\n",
      "59995/59995 [==============================] - 15s 255us/step - loss: 0.0469 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 76/250\n",
      "59995/59995 [==============================] - 13s 211us/step - loss: 0.0475 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 77/250\n",
      "59995/59995 [==============================] - 13s 213us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 78/250\n",
      "59995/59995 [==============================] - 14s 239us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 79/250\n",
      "59995/59995 [==============================] - 14s 239us/step - loss: 0.0468 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 80/250\n",
      "59995/59995 [==============================] - 14s 237us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 81/250\n",
      "59995/59995 [==============================] - 14s 233us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 82/250\n",
      "59995/59995 [==============================] - 14s 236us/step - loss: 0.0475 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 83/250\n",
      "59995/59995 [==============================] - 14s 236us/step - loss: 0.0464 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 84/250\n",
      "59995/59995 [==============================] - 14s 239us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 85/250\n",
      "59995/59995 [==============================] - 14s 235us/step - loss: 0.0475 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 86/250\n",
      "59995/59995 [==============================] - 14s 240us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 87/250\n",
      "59995/59995 [==============================] - 14s 231us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 88/250\n",
      "59995/59995 [==============================] - 14s 240us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 89/250\n",
      "59995/59995 [==============================] - 14s 229us/step - loss: 0.0467 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 90/250\n",
      "59995/59995 [==============================] - 14s 241us/step - loss: 0.0466 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 91/250\n",
      "59995/59995 [==============================] - 14s 232us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.07201 to 0.07201, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 92/250\n",
      "59995/59995 [==============================] - 15s 243us/step - loss: 0.0469 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.07201 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 93/250\n",
      "59995/59995 [==============================] - 14s 228us/step - loss: 0.0469 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 94/250\n",
      "59995/59995 [==============================] - 15s 243us/step - loss: 0.0477 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 95/250\n",
      "59995/59995 [==============================] - 14s 228us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 96/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59995/59995 [==============================] - 15s 243us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 97/250\n",
      "59995/59995 [==============================] - 14s 230us/step - loss: 0.0465 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 98/250\n",
      "59995/59995 [==============================] - 15s 246us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 99/250\n",
      "59995/59995 [==============================] - 14s 229us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 100/250\n",
      "59995/59995 [==============================] - 15s 245us/step - loss: 0.0466 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 101/250\n",
      "59995/59995 [==============================] - 13s 216us/step - loss: 0.0468 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 102/250\n",
      "59995/59995 [==============================] - 15s 252us/step - loss: 0.0462 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00102: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 103/250\n",
      "59995/59995 [==============================] - 13s 212us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 104/250\n",
      "59995/59995 [==============================] - 13s 212us/step - loss: 0.0464 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 105/250\n",
      "59995/59995 [==============================] - 14s 230us/step - loss: 0.0475 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 106/250\n",
      "59995/59995 [==============================] - 15s 245us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 107/250\n",
      "59995/59995 [==============================] - 14s 227us/step - loss: 0.0475 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 108/250\n",
      "59995/59995 [==============================] - 15s 244us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 109/250\n",
      "59995/59995 [==============================] - 14s 227us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 110/250\n",
      "59995/59995 [==============================] - 15s 249us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 111/250\n",
      "59995/59995 [==============================] - 14s 226us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.07200 to 0.07200, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 112/250\n",
      "59995/59995 [==============================] - 15s 245us/step - loss: 0.0469 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.07200 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 113/250\n",
      "59995/59995 [==============================] - 14s 226us/step - loss: 0.0468 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 114/250\n",
      "59995/59995 [==============================] - 15s 244us/step - loss: 0.0476 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 115/250\n",
      "59995/59995 [==============================] - 14s 226us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 116/250\n",
      "59995/59995 [==============================] - 15s 244us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 117/250\n",
      "59995/59995 [==============================] - 13s 215us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 118/250\n",
      "59995/59995 [==============================] - 13s 214us/step - loss: 0.0475 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 119/250\n",
      "59995/59995 [==============================] - 14s 239us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 120/250\n",
      "59995/59995 [==============================] - 14s 236us/step - loss: 0.0480 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 121/250\n",
      "59995/59995 [==============================] - 14s 237us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 122/250\n",
      "59995/59995 [==============================] - 14s 241us/step - loss: 0.0468 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 123/250\n",
      "59995/59995 [==============================] - 14s 237us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00123: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 124/250\n",
      "59995/59995 [==============================] - 14s 238us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 125/250\n",
      "59995/59995 [==============================] - 14s 237us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 126/250\n",
      "59995/59995 [==============================] - 14s 236us/step - loss: 0.0469 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 127/250\n",
      "59995/59995 [==============================] - 14s 239us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 128/250\n",
      "59995/59995 [==============================] - 14s 235us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 129/250\n",
      "59995/59995 [==============================] - 14s 241us/step - loss: 0.0468 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00129: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 130/250\n",
      "59995/59995 [==============================] - 14s 234us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 131/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59995/59995 [==============================] - 14s 240us/step - loss: 0.0478 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 132/250\n",
      "59995/59995 [==============================] - 14s 231us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00132: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 133/250\n",
      "59995/59995 [==============================] - 14s 240us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.07199 to 0.07199, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 134/250\n",
      "59995/59995 [==============================] - 14s 232us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.07199 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 135/250\n",
      "59995/59995 [==============================] - 15s 242us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00135: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 136/250\n",
      "59995/59995 [==============================] - 13s 225us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 137/250\n",
      "59995/59995 [==============================] - 13s 211us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 138/250\n",
      "59995/59995 [==============================] - 14s 230us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 139/250\n",
      "59995/59995 [==============================] - 15s 246us/step - loss: 0.0461 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 140/250\n",
      "59995/59995 [==============================] - 14s 236us/step - loss: 0.0466 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 141/250\n",
      "59995/59995 [==============================] - 15s 243us/step - loss: 0.0467 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00141: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 142/250\n",
      "59995/59995 [==============================] - 13s 218us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 143/250\n",
      "59995/59995 [==============================] - 13s 222us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 144/250\n",
      "59995/59995 [==============================] - 15s 244us/step - loss: 0.0477 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00144: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 145/250\n",
      "59995/59995 [==============================] - 21s 344us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 146/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0476 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 147/250\n",
      "59995/59995 [==============================] - 27s 456us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00147: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 148/250\n",
      "59995/59995 [==============================] - 27s 456us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 149/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0479 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 150/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00150: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 151/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 152/250\n",
      "59995/59995 [==============================] - 27s 456us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 153/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00153: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 154/250\n",
      "59995/59995 [==============================] - 28s 459us/step - loss: 0.0476 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 155/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 156/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0467 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00156: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 157/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0477 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.07198 to 0.07198, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 158/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.07198 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 159/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0469 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 160/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 161/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0469 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 162/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0479 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00162: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 163/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0477 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 164/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0480 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 165/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0469 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00165: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 166/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59995/59995 [==============================] - 27s 456us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 167/250\n",
      "59995/59995 [==============================] - 27s 456us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 168/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00168: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 169/250\n",
      "59995/59995 [==============================] - 27s 455us/step - loss: 0.0463 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 170/250\n",
      "59995/59995 [==============================] - 27s 454us/step - loss: 0.0467 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 171/250\n",
      "59995/59995 [==============================] - 27s 455us/step - loss: 0.0469 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00171: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 172/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 173/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 174/250\n",
      "59995/59995 [==============================] - 27s 456us/step - loss: 0.0476 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00174: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 175/250\n",
      "59995/59995 [==============================] - 27s 456us/step - loss: 0.0468 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 176/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 177/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.07197 to 0.07197, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00177: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 178/250\n",
      "59995/59995 [==============================] - 27s 456us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.07197 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 179/250\n",
      "59995/59995 [==============================] - 27s 456us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 180/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 181/250\n",
      "59995/59995 [==============================] - 27s 456us/step - loss: 0.0477 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 182/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 183/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00183: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 184/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 185/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 186/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00186: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 187/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00187: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 188/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0475 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00188: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 189/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00189: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 190/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 191/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0464 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 192/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0469 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00192: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 193/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 194/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 195/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0474 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00195: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 196/250\n",
      "59995/59995 [==============================] - 28s 459us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 197/250\n",
      "59995/59995 [==============================] - 28s 458us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 198/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00198: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 199/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0470 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 200/250\n",
      "59995/59995 [==============================] - 27s 458us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 201/250\n",
      "59995/59995 [==============================] - 27s 456us/step - loss: 0.0476 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00201: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00201: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 202/250\n",
      "59995/59995 [==============================] - 27s 457us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.07196 to 0.07196, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 203/250\n",
      "59995/59995 [==============================] - 27s 456us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.07196 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 204/250\n",
      "59995/59995 [==============================] - 28s 466us/step - loss: 0.0468 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00204: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00204: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 205/250\n",
      "59995/59995 [==============================] - 31s 516us/step - loss: 0.0472 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 206/250\n",
      "59995/59995 [==============================] - 29s 481us/step - loss: 0.0468 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 207/250\n",
      "59995/59995 [==============================] - 20s 331us/step - loss: 0.0475 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00207: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00207: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 208/250\n",
      "59995/59995 [==============================] - 14s 234us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 209/250\n",
      "59995/59995 [==============================] - 14s 229us/step - loss: 0.0475 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00209: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 210/250\n",
      "59995/59995 [==============================] - 13s 212us/step - loss: 0.0468 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00210: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00210: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 211/250\n",
      "59995/59995 [==============================] - 12s 204us/step - loss: 0.0471 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00211: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 212/250\n",
      "59995/59995 [==============================] - 12s 199us/step - loss: 0.0473 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 213/250\n",
      "59995/59995 [==============================] - 12s 199us/step - loss: 0.0469 - val_loss: 0.0720\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00213: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 214/250\n",
      "59995/59995 [==============================] - 12s 199us/step - loss: 0.0473 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00214: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 215/250\n",
      "59995/59995 [==============================] - 12s 200us/step - loss: 0.0475 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 216/250\n",
      "59995/59995 [==============================] - 12s 201us/step - loss: 0.0471 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00216: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00216: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 217/250\n",
      "59995/59995 [==============================] - 12s 200us/step - loss: 0.0478 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 218/250\n",
      "59995/59995 [==============================] - 12s 200us/step - loss: 0.0469 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00218: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 219/250\n",
      "59995/59995 [==============================] - 12s 201us/step - loss: 0.0469 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00219: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 220/250\n",
      "59995/59995 [==============================] - 12s 201us/step - loss: 0.0471 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 221/250\n",
      "59995/59995 [==============================] - 13s 214us/step - loss: 0.0471 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 222/250\n",
      "59995/59995 [==============================] - 13s 214us/step - loss: 0.0468 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00222: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00222: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 223/250\n",
      "59995/59995 [==============================] - 13s 214us/step - loss: 0.0474 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00223: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 224/250\n",
      "59995/59995 [==============================] - 13s 214us/step - loss: 0.0470 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.07195 to 0.07195, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 225/250\n",
      "59995/59995 [==============================] - 13s 215us/step - loss: 0.0474 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.07195 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00225: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 226/250\n",
      "59995/59995 [==============================] - 13s 214us/step - loss: 0.0471 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 227/250\n",
      "59995/59995 [==============================] - 13s 217us/step - loss: 0.0467 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00227: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 228/250\n",
      "59995/59995 [==============================] - 13s 214us/step - loss: 0.0474 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00228: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 229/250\n",
      "59995/59995 [==============================] - 13s 215us/step - loss: 0.0474 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 230/250\n",
      "59995/59995 [==============================] - 13s 214us/step - loss: 0.0469 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00230: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 231/250\n",
      "59995/59995 [==============================] - 13s 215us/step - loss: 0.0474 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00231: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 232/250\n",
      "59995/59995 [==============================] - 13s 215us/step - loss: 0.0467 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 233/250\n",
      "59995/59995 [==============================] - 13s 215us/step - loss: 0.0471 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 234/250\n",
      "59995/59995 [==============================] - 13s 215us/step - loss: 0.0474 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00234: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00234: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 235/250\n",
      "59995/59995 [==============================] - 13s 215us/step - loss: 0.0471 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 236/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59995/59995 [==============================] - 13s 214us/step - loss: 0.0468 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 237/250\n",
      "59995/59995 [==============================] - 13s 212us/step - loss: 0.0471 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 238/250\n",
      "59995/59995 [==============================] - 13s 212us/step - loss: 0.0474 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00238: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 239/250\n",
      "59995/59995 [==============================] - 13s 212us/step - loss: 0.0471 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00239: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 240/250\n",
      "59995/59995 [==============================] - 13s 212us/step - loss: 0.0467 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 241/250\n",
      "59995/59995 [==============================] - 13s 212us/step - loss: 0.0478 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00241: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00241: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 242/250\n",
      "59995/59995 [==============================] - 13s 212us/step - loss: 0.0473 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 243/250\n",
      "59995/59995 [==============================] - 13s 212us/step - loss: 0.0469 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00243: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 244/250\n",
      "59995/59995 [==============================] - 13s 214us/step - loss: 0.0474 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00244: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00244: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 245/250\n",
      "59995/59995 [==============================] - 13s 213us/step - loss: 0.0476 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 246/250\n",
      "59995/59995 [==============================] - 13s 212us/step - loss: 0.0470 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00246: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 247/250\n",
      "59995/59995 [==============================] - 13s 213us/step - loss: 0.0471 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00247: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00247: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 248/250\n",
      "59995/59995 [==============================] - 13s 212us/step - loss: 0.0473 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 249/250\n",
      "59995/59995 [==============================] - 13s 213us/step - loss: 0.0469 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 250/250\n",
      "59995/59995 [==============================] - 13s 212us/step - loss: 0.0476 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.07194 to 0.07194, saving model to save_stacked_checkpoint.keras\n",
      "\n",
      "Epoch 00250: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "evaluate model...\n",
      "59995/59995 [==============================] - 2s 41us/step\n",
      "number params: [2.12104015e-01 4.88076648e-01 5.97788984e-01 3.28856862e-01\n",
      " 2.96055115e-01 3.40313111e-02 2.56000000e+02 1.00000000e+00\n",
      " 3.00000000e+01 2.00000000e+02 5.00000000e+00]\n",
      "number_layers: 5\n",
      "0.2121040145046649\n",
      "0.4880766475568226\n",
      "0.597788984353737\n",
      "0.3288568617710177\n",
      "Using HyperParams. units:256, batch_size:1, look_back:30, look_ahead:1, layers:5, epochs:200, learning_rate:0.03403131111072706\n",
      "dropout_list:[0.2121040145046649, 0.4880766475568226, 0.597788984353737, 0.3288568617710177]\n",
      "create data Sequence with look_ahead and look_back...\n",
      "look_back: 30\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59970, 30, 17)\n",
      "y look_ahead: (59970, 1, 17)\n",
      "initilize Layer variable length LSTM Model...\n",
      "[0.2121040145046649, 0.4880766475568226, 0.597788984353737, 0.3288568617710177]\n",
      "build model...\n",
      "dropout l0: 0.2121040145046649\n",
      "dropout:0.4880766475568226\n",
      "dropout:0.597788984353737\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_59 (LSTM)               (None, 30, 256)           264192    \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_60 (LSTM)               (None, 30, 5)             5240      \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 30, 5)             0         \n",
      "_________________________________________________________________\n",
      "lstm_61 (LSTM)               (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 6         \n",
      "_________________________________________________________________\n",
      "repeat_vector_23 (RepeatVect (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 269,658\n",
      "Trainable params: 269,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train model...\n",
      "Train on 59970 samples, validate on 59970 samples\n",
      "Epoch 1/200\n",
      "59970/59970 [==============================] - 5646s 94ms/step - loss: 5.4540e-05 - val_loss: 0.1400\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13998, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/200\n",
      "59970/59970 [==============================] - 5599s 93ms/step - loss: 2.4738e-05 - val_loss: 0.1400\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.13998\n",
      "Epoch 3/200\n",
      " 3625/59970 [>.............................] - ETA: 1:27:32 - loss: 1.1783e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-3c94aa687bbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptimized_lstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_optimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\GPyOpt\\core\\bo.py\u001b[0m in \u001b[0;36mrun_optimization\u001b[1;34m(self, max_iter, max_time, eps, context, verbosity, save_models_parameters, report_file, evaluations_file, models_file)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;31m# --- Evaluate *f* in X, augment Y and update cost function (if needed)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_objective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[1;31m# --- Update current evaluation time and function evaluations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\GPyOpt\\core\\bo.py\u001b[0m in \u001b[0;36mevaluate_objective\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mEvaluates\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggested_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_cost_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggested_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\GPyOpt\\core\\task\\objective.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_procs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mf_evals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_evals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eval_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\GPyOpt\\core\\task\\objective.py\u001b[0m in \u001b[0;36m_eval_func\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mst_time\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mrlt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mf_evals\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf_evals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrlt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mcost_evals\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mst_time\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-960cf4438a48>\u001b[0m in \u001b[0;36mlstm_ojective_function\u001b[1;34m(param)\u001b[0m\n\u001b[0;32m     72\u001b[0m                                  \u001b[0my_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_scaled_key_list_stacked_tensor_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_key\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfix_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                                  \u001b[0mvar_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                                  var_epochs=var_epochs)\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"evaluate model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-fcfa725ccbc2>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(fix_model, X_train, y_train, X_test, y_test, var_batch_size, var_epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m                   \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                   \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                   \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                   )\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimized_lstm_model.run_optimization(max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAFNCAYAAABFbcjcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzsvXm8HFWZ//9+um+2G7IYQhIIJGGHgMoSFgHlKjigjqIzOl/0ghuacdQRHccRJm4jZtRRRP2NW0QF5SqiOIrKgKBzQVFANtkCJIbkJhAIa/bt3nt+f5w66bp1a+3u6q6uPO/Xq1/dVXXq1DnV1VWffs7zPEeMMSiKoiiKoijFoNLuBiiKoiiKoig1VJwpiqIoiqIUCBVniqIoiqIoBULFmaIoiqIoSoFQcaYoiqIoilIgVJwpiqIoiqIUCBVnTUJEvikiH293O+pBRHpEZE2726FkQ0R6ReQ37W6HUl5EZJ6IGBHpavFxJ4jIL0VkvYj8JOU+/SLyriYdf6WInN6MuvKmXd9RSDs+JSJXtLMNSXjn6aB2tyMNKs5S4P1Qt4rIRhF5XkT+KCLvEZFd588Y8x5jzEUp6+qIH30aVNi1hrAbsDGmzxjzNy1ux6dE5FOtPKZSPyJyvYh8OmT9WSLyRLsf6DG8EZgJ7GmMeVNwYycIgSBFb3PZnk2djoqz9LzWGDMJmAt8Dvgo8J32NklRFCWWy4BzRUQC688F+owxg61vUirmAo8UuH2Kki/GGH0lvICVwOmBdccDw8CR3vJlwGe8z9OBXwHPA88Cv8cK4R94+2wFNgH/5pX/CfAEsB64GTjCd5zLgK8BvwY2ArcBB/q2HwHc4B3nSeDfvfUV4ALgr8AzwFXAtIj+9QBrgH8Hnvb62+vbPg74IjDgHeObwARgoteXYa8/m4B9vHXTvX0/BgwCk73lzwBfjqvXd9y/Be7xzuMfgRcFvpN/Be71ztuPgfEx3+G7gaXeOXwQOMZbfzjQ7x3jAeB1ac49IMAlwDrv+Pf6roWkfp3l9WuD9/2cGXadAZ8CrvA+DwDGd55fArwd+IO3/ZvAFwN9/gXwL97nfYCrgaeAR4EPRJynsV7b/tlbrgK3AJ/wtelTcdd5u3+v+hrxfU7wrs+X+da9ANgGvNhbfg1wt3c9rnbfr7dtnnfddSVdo97yid5v9XngL0BPTNtCf3vAfwA7gJ3etX5eYL8zA9v/4q3vBy7yrteNwG/w7kN1tG0lcCH2XvEc8D189xfi700fBR7z2vAwcFpUm0OOO2pfb33k/TzkO5qCNRys9er6DFD1HWPUvZDoZ1PkOQP2B27y6rkB+G//tRDoV+S9wtcv1543+PZ7u/d9XuLtuwI4yVu/Gnv/fZuv/GXYe+ENXn03AXN92w1wkPc59j7d7lfbG9AJL0LEmbd+APgn30XhxNlnvS96jPd6KSBRdQHvBCZ5F8uXgXsCF9uzWDHYBfQBV3rbJnk/wA8D473lE7xtHwRuBfb16v0W8KOI/vVgBdSXvLKnApuBQ73tXwauAaZ5x/gl8FnfvmsC9d0M/L33+TfeD+9Vvm1vSFHvMd4P7wSsQHibd+7G+c7j7VjRMQ17s3lPRP/ehL1JHYcVVQdh/5mPAZZjRelY4BXYH/ShKc79GcCdwFSvzsOBvVP063jsw/KV2BvubOCwsGuDkeJsHr4bsLfu7dTE2cuwNyt3nb0Ae6PdxzvOncAnvH4egL3JnRFxvo7EPpAOBxZhr6NqSLnI61xfxXkB3wYu9S3/IyPvMT3AC73r5EXYB9Xrw667hGt0NlY4vNqr65Xe8l4hbUr67e2qN6JPo7ZjxdlfgUOworQf+FzWtvn6eT+wH/Z3fAu1+3vkvQk41Psd7uM7fwem7FPcvpH385Dv6Ofe9onADOx98h+9baH3wojvNvacAX+i9sx4mff9RYmzuGfim6jdp/4f9tnj7qVvxz6b3uGd689gn7tf8477N95x9/DKX+Ytv8zb/hW8e6S33S/OIu/TRXi1vQGd8ApetL71twKLfBeF+/F+Gmu1OChtXb7tU70LaIqvXv+N9dXAQ97nNwN3R9SzFO9fl7e8N/ZfW1dI2R7vBzDRt+4q4OPeD3gzI611LwEe9e0bFGcXAV/FCpongPOxQ8Hj8axqKer9BnBRoN6HgVN95/Ec37b/Ar4ZcS6uB84PWf9Sr30V37ofUbMMxZ37VwCPYP9Z+vdP6te3gEvSXGdkE2eCvWm9zFt+N/A77/MJwEDgWBcC34u5Dj8MPIQVaQdHlIm8zvVVnBdwCvYPwQRv+RbgQzHlv+yu0eB1l3CNfhT4QaCu6/FZNnzrk357u+qNaOOo7Vgx9jHf8nuB67K2zdfP9/iWXw381fsceW/Cip11wOnAmKQ2B7bH7Rt5P/d/R1g/ve2MtNS/Gfg/X59H3QsjvtvIcwbMYfQz44dR/ctyr8BaJM/yPr8dWObb9kKvrzN9654BjvI+X4b3B9pb3gMYAvbzlo13nmPv00V4qc9ZY8zGWlaCfAH7r/A3IrJCRC6IqkBEqiLyORH5q4hswP5AwAoYxxO+z1uwFxzYf3V/jah6LvA/XgDD89gf9xD2xxvGc8aYzb7lVdh/M3sB3cCdvrqu89ZHcRNWtB0D3Ic1MZ+KFTLLjTFPp6h3LvBht83bvp/XJkfUeQkSdZ72AVYbY4YD/Z6ddAxjzO+wZvyvAU+KyBIRmZyiX3HfWd0Ye3e5EnsjBngL1tIH9lzuEziX/070tQBwOfamf60xZllEmdTXudI+jDF/wA5nnyUiB2CtJj9020XkBBH5PxF5SkTWA+9h5P0nLXOBNwWus1OwQiJImt9ePUTdE7K0zbE60DZ374m8NxljlmOtXJ8C1onIlSLiv2dFkrBv2vv5XKxlaq2v7LewFjTIdv+JO2f7EP7MiCLyXiEibxWRe3zHOJKR19+Tvs9bAYwxwXX+e/+u780Yswn7jA5+B/U811qKirM6EZHjsDeSPwS3GWM2GmM+bIw5AHgt8C8icprbHCj+FqwP0ulYX4F57hApmrEaODBm26uMMVN9r/HGmMciyr9ARCb6lucAj2N90LZi/eBcPVOMMe7HEOwPWB+FQ4E3ADcZYx706nsNVriRot7VwOJA+7uNMT9KOikR5yLsPD0O7OePuvXaGXWORmCM+aox5lis398hwEdS9ivqO9uMvWE4ZvkPl6JJPwLeKCJzsdayq33HfDRwLicZY14dU9fXsT4iZ4jIKWEFEq5zpVh8H3grNhDgN4GH2w+xwzv7GWOmYIefou4/cdfoaqylxX+dTTTGfC6knoZ+e6T7PfjJ0jbHfoG2Pe6rK/LeZIz5oTHmFKy4McDn07Y5Zt+09/PVWMvZdF+5ycaYI3zbo+4/wfbFnbO1hD8zovoVeq/w7lXfBt6Pjcydih1OTvP8i2LX9yYie2CHLR8PlEm6T7cdFWcZEZHJIvK3WCvFFcaY+0LK/K2IHORFSG3A/sMZ8jY/ifX5cUzC/piewd70/jNDc34FzBKRD4rIOBGZJCIneNu+CSz2Ln5EZC8ROSuhvv8QkbEi8lKsw+tPvH+23wYuEZEZXl2zReQMX3/2FJEprhJjzBasj9P7qImxP2J9XW7yyiTV+23gPd6/ehGRiSLyGhGZlOH8OC4F/lVEjvXqOsg7L7dhHzb/JiJjRKQHe+O4MqlCETnOa9sYr45twFCKfn0HeId3Y6p42w7ztt0DnO21ZQE2nYDjKazDrv/aGYEx5m6v3KXA9caY571NtwMbROSjYvNHVUXkSO8PRljfzgWOxQ4pfAC43LvJBcvFXedKsfg+9g/gu7FWUT+TgGeNMdtE5HjsH8Yo4q7RK4DXisgZ3jU2XmyqnX1D6qn7t+fxJDAvIO7iyNI2x/tEZF8RmYa1NP/YWx95bxKRQ0XkFSIyDntP2MrIe39kmxP2TXU/N8asxfr5Xuw9qyoicqCInOoViboXuvb57y+R58wYswq4g9oz4xTs9xdKzL1iIlYUPuWVewfWctYIrxaRU0RkLNbF5jZjjN8Kmub503ZUnKXnlyKyEftvYhHWEfIdEWUPBm7ERr38Cfi6Mabf2/ZZ4GNiTan/ir1prsL+Y3wQ68eWCmPMRqyT5mux5vxlwMu9zV/B/hv+jdfuW7HWlCiewPoXPY4dDnuPMeYhb9tHsSbpW8UOvd6ItYzhlfkRsMLrkzMf34Q1r9/uW56EDQggRb13YB8k/+21azlWLGTGGPMTYDHWQrAR6zA7zRizA3gd8CrsP6mvA2/19TuOydgf93PY7+8ZbORPUr9ux143l2D9gG7C/ksG6+N3oFfnf+AbevIE72LgFu88nxjRrh9hH8L+fYew18hR2EjNp7E36SnBnUVkDtbn6K3GmE3GmB9ib8KXhBwr7jpXCoQxZiX2D9JE7H3Bz3uBT3v3iU9g/U2jiLtGV2NHAf4d+7BdjbUmj3rONPjbAxvhDvCMiNyVVDhL23z8ECt0Vnivz3h1xd2bxmH9a5/G3lNneMdM0+a4fbPcz9+KDbJwkaY/xRu+jboXevuNeDalOGdv8drwLPBJ7LMsitB7hTeqcrG37kmsT9ktMfWk4Ydee57F/snsjSgXeZ8uAi5aQlEURVEUpWMRkcuwAWofa3dbGkUtZ4qiKIqiKAVCxZmiKIqiKEqB0GFNRVEURVGUAqGWM0VRFEVRlAKh4kxRFEVRFKVAdLW7AY0wffp0M2/evNTlN2/ezMSJE5MLdjC7Qx9B+1kmsvbxzjvvfNoYU5hM3o2Q5R6m10I50D6Wg3r7mPb+1dHibN68edxxxx2py/f399PT05NfgwrA7tBH0H6Wiax9FJG4aWI6iiz3ML0WyoH2sRzU28e09y8d1lQURVEURSkQKs4URVEURVEKhIozRVEURVGUAqHiTFEURVEUpUCoOFMURVEURSkQKs4URVEURVEKhIozRVEURVGUAtHRec6UJtHXB4sWwcAATJtm1z377MjPc+bA4sXQ25vfsfM6hqJ0MD+/+zG+cP3DPPb8Vqbe9BtE4PktO5kyYcyuz/tMncBHzjiU1x89u93NVRSlCag4293p64OFC2HLFrv8zDO1bf7Pq1bZctA88RQ8dh7HUJQO5ud3P8aFP7uPrTuHAHh+685d2/yfH3t+Kxf+7D4AFWiKUgJ0WHN3Z9GimjhKYssWWz7PYzf7GIrSwXzh+od3CbMktu4c4gvXP5xzixRFaQUqznZ3BgbyLV9PXc08hqJ0MI8/vzXX8oqiFJPcxJmI7Cci/yciS0XkARE531v/KRF5TETu8V6v9u1zoYgsF5GHReSMvNqm+JgzJ9/y9dTVzGMoSgezz9QJuZZXFKWY5Gk5GwQ+bIw5HDgReJ+IzPe2XWKMOcp7XQvgbTsbOAI4E/i6iFRzbJ8C1gF/Qsobene3LZ/nsZt9DEXpYD5yxqFMGJPuNjhhTJWPnHFozi1SFKUV5CbOjDFrjTF3eZ83AkuBOE/Vs4ArjTHbjTGPAsuB4/Nqn+LR2wuf+Uxtec897UsEpkyx60Rg7lxYsqS5jvq9vXDuubXladOafwxF6WBef/RsPvt3L2S2ZxGbOmEML+geg3ifxSs3e+oEPvt3L9RgAEUpCS2J1hSRecDRwG3AycD7ReStwB1Y69pzWOF2q2+3NcSLOaVZ9PTY95//HM46q7b+oYfg8MPhiivgLW/J59gDAzBrFjzxBHz0o50vzDQ1iNJkXn/0bF5/9Gz6+/vpcb9Vj2MvuoEzj5zF4je8sD2NUxQlF3IXZyKyB3A18EFjzAYR+QZwEWC894uBd8KuP4F+TEh9C4GFADNnzqS/vz91WzZt2pSpfCdSTx+n3nMPRwH3rFjB8759J6xZwwnA0vvv58kcztuY9es56Te/YfU//AP7XXUVA/fdx6Mpj1PE73LGjTdy6Be/SHX7drti1SqGzjuPh5cuZd3pp9dVZxH72Wx2hz7mRbUiDJtRt0lFUTqcXMWZiIzBCrM+Y8zPAIwxT/q2fxv4lbe4BtjPt/u+wOPBOo0xS4AlAAsWLDDBf5JxhP3zLA2excYMDCBZLTYbNgBw1MteBsceW1u/YgUAhx9yCIc3+7z19cEHPgDDw8y56SYYN46506YxN+Vx2vZdxlnG3v52cMLMo7p9O/OvuIL5/qHjDJT6mvXYHfqYF9WKMDSs4kxRykae0ZoCfAdYaoz5km/93r5ibwDu9z5fA5wtIuNEZH/gYOD2vNpXCvr6YN486xN27rmwahViTC2Za19funo8ccbkySPXVz1H5CFfniV3zErFvqc9RrDdCxfamQcA1q6Fbdvg7ruz19VKXLtXrYKw86ypQZQWUxFhaLjdrVAUpdnkGa15MnAu8IpA2oz/EpH7RORe4OXAhwCMMQ8AVwEPAtcB7zPGpMu+uDviFwpgxYKfLMlc04qzJHGSlrDks8bAHXdkq6fVJCXN1dQgSouxljNVZ4pSNnIb1jTG/IFwP7JrY/ZZDGgehTSkyeyf1mITJc66vMvDibM4cZLF6T2qXVsLnkAzyTK2ePHI6ahAU4MoudJVEYZ0VFNRSofOENCppBFeaS02GzZYITZ+/Mj1QctZs4btoto1dmy2elpNkmWst9emAql4P6tZszQ1iJIrlYowrD5nilI6VJx1KknCK4vFZsMGazWTgKEzKM6aNWy3eHG4EJw+PVs9rSZN0tyzzgI3zHT11SrMlFypijCow5qKUjpUnHUqixdbYRDG3ntns9g4cRYkKM7CjlnPsF1vL3zkI/azS3B7yimjxWHR6O2Fiy6qLYed5+XLa5937Ghd25RMiMiZ3jRxy0XkgpDt40Tkx97227xcjYjInt60dJtE5L8D+xzr+dMuF5GvekFRuWJ9zvI+iqIorUbFWafihtD22ssuz5plU1MAXH99NotNWnHmjulmDpg9u/5hu1NPte/9/bByJRxzTM33rci88pW1z5///Oi+P/JI7bOKs0LiTQv3NeBVwHzgzb6p5RznAc8ZYw4CLgE+763fBnwc+NeQqr+BzcF4sPc6s/mtH4nmOVOUcqLirJPp7YVvfMN+vu66Wqb/wcFs9aQVZ+6YH/yg/XzLLfUP2znh4vzMJk2CjRtrQ4JFZdu22ud77hm9fdmy2mcVZ0XleGC5MWaFMWYHcCV2+jg/ZwGXe59/CpwmImKM2ewFO23zF/ZSBE02xvzJGGOA7wOvz7UXWJ8zzXOmKOVDxVmn44RTV1e4mEpDFnHmX84qAv0ExZk7/qZN9dfZClxEqQj85S+jt/vFWSAhrVIYZgOrfcthU8XtKmOMGQTWA3sm1Lkmoc6m06XiTFFKSUvm1lRyxAmlanV06ou0bNgABx88en2SOMt6HD9OuATF2caN4UKxKDjL2eGHW8uZMSN95ZYtq80VqpazopJmqrhU08nVU77eKejCprnauGErW4TSTH+1O0zlpX0sB3n3UcVZp+MXZ822nLmUEMH6nMWsEXHmhMu4cfbdHX/DBuvLVlScODvxRPjud+Gxx2DffWvbH3kEXvxiFWfFJs1Uca7MGhHpAqYAzybU6bsQwqefg/qnoAub5uqbj/yJYQM9PS9JVUfR2R2m8tI+loO8+6jDmp1OmDhrls+ZiBVoeVjOwnzOXFuKjF+cwUi/s+efh6efhiOOsMsqzorKn4GDRWR/ERkLnI2dPs7PNcDbvM9vBH7n+ZKFYoxZC2wUkRO9KM23Ar9oftNHonNrKko5UXHW6TRqOdu50/pRRQ0lVqut9TnrFHF2/PH23e935vzNVJwVGs+H7P3A9cBS4CpjzAMi8mkReZ1X7DvAniKyHPgXYFe6DRFZCXwJeLuIrPFFev4TcCmwHPgr8L9596Vaqag4U5QSosOanU6jPmcbN9r3esRZMy1nnSbO9toLDjxwpOXMibMjj7TvKs4KizHmWgJTyRljPuH7vA14U8S+8yLW3wEc2bxWJlMVNJWGopQQFWedjrNe1RutGTWvpiMvceYCAoI+Z04sFhUXrTlhAkybBj//uR36nTMHjj3WDgUffrgto+JMyRkd1lSUcqLirNNpdFizXeKs0y1nP/+5tZo5cbxqFaxZYwWb64uKMyVnVJwpSjlRn7NOp9GAgHrEWTOjNceMse+dFhDwqU9Zfz0/Q0OwebM9Z5WKijMld1ScKUo5UXHW6TTqc9aI5azRgIAxY2o5wsaMsZOhd4I4GzsWVq+O3t7XZ8toElolZyoiDKnPmaKUDhVnnU4nD2u6IU3H5MmdIc7Gj7c+ZlEsXGjf1XKm5IxazhSlnKg463RaIc6CFrJmBQS4YADH5MnFDwhw4mzxYujuDi+zZYsVZirOlJxRcaYo5UQDAjqdsGjNvH3OdmfL2datNlLTTfh+zjnh5YaHVZwpuVMVYVjFmaKUDrWcdTrNsJyJwMSJ4dvz9DkLirNJk4ovzpzlDKxAmzs3vFy1quJMyZ1qRRhUcaYopUPFWafjhFKlUn9AwKRJtXk0g+QZrdmJljO/OIPw4c3ubpukVsWZkjPVimgSWkUpISrOOp2hoZrFrF7LWdSQpqszr2HNMJ+zThNnvb2wZIm1oInY9yVLVJwpLUF9zhSlnKg463TCxFlWn7N2iLPt28MtZ50SEOCntxdWrrR+ZitX2uWxY1WcKblTERVnilJGVJx1Ou20nDXb56wTLWdRqDhTWoBazhSlnKg463QGB2u+ZvX6nMWJs66u1kVrTppkLWpFTt66bZuN1kxCxZnSAroqmoRWUcqIirNOp5N9zsIsZ1Dsoc2tW9NbzoosMpVSUKkIw8PtboWiKM1GxVmn0w5x1qxozbCAANemoqLDmkqBqIowqOpMUUqHirNOp2wBAa5NRUXFmVIgbCoNMDq0qSilQsVZp+MXZ1l9zoaH7RBikQICoNjDmirOlAJRrQgAGhOgKOVCxVmn08iw5qZN9r0oPmeTJtn3olvONCBAKQhOnGnEpqKUCxVnnY4/WtNl+U8rmpLm1YT2BAQUVZwNDcHOneksZ+PGqThTckfFmaKUExVnnY7fciaCqVTSDze2W5x1WkDAtm32XYc1lYJQFU+cqc+ZopQKFWedjl+cASZMTEVRrzhz4q8Rn7NODAhQcaYUjIpazhSllKg463SC4qxS6dxhzYkT7fyURQ0IUHGmFIyq1WYqzhSlZKg463QaEWdOBBVFnInYoICyWM62bwcdblJypFq1t3AVZ4pSLlScdToBcUYn+JwNDdk0HkGfM9eWoouztNGaxjQmYBUlAedzNqx/AhSlVKg463T80Zq0yOes0TxnbrgvaDlzbSm6OEtrOQMd2lRypeoCtNVypiilQsVZp9OMgACXXyyMPCxnbs7JThNnW7fadxVnSkGoVnRYU1HKiIqzTqfRgIDu7hGWt1HkMbdmkuWsLAEBoOJMyRW1nClKOVFx1uk0Ks7ihjQhH8tZnDgrS0CA86dTcabkSEXznClKKVFx1uk0GhDQiDhr1OesUwMC1HKmFASdIUBRyklu4kxE9hOR/xORpSLygIic762fJiI3iMgy7/0F3noRka+KyHIRuVdEjsmrbaWiXp+zvj74+c/hkUdg3jy7HEZay1lfn62nUomvD1obEJClXUlkjdYEFWdKrnSpOFOUUpKn5WwQ+LAx5nDgROB9IjIfuAD4rTHmYOC33jLAq4CDvddC4Bs5ti0d/gf79On2Ffzsf+BHlXdlmikUHPVEa/b1wcKFNbGxapVdDmtPtTraQhYUZ66+Vats+oi4+iA6IKCvDy67zIqzuXPDz2uW85a1XUk0YjnL47svOrtjn1vMrmFNFWeKUipyE2fGmLXGmLu8zxuBpcBs4Czgcq/Y5cDrvc9nAd83lluBqSKyd17tSyT4YH/mGfsKfnYP/Pe+N7r8qlXwjnfAO9/ZPKHgqMfnbNEi2LJl5LotW+z6IGksZ1nqg3DLmTvf69fb5YGB8PPqnbcZN94Y38d62pVEPdGa27c3XyR2Artjn9uADmsqSjlpic+ZiMwDjgZuA2YaY9aCFXDADK/YbGC1b7c13rr2EPZgj2LLFliyJL78zp2jh7gaEQqOMHGW5As2MJB+fZpozSz1Qbg4ixJSYed1yxYOuPTS8LrTHD9qfRL1Ws6aLRI7gd2xz21glzjTgABFKRUxORSag4jsAVwNfNAYs0E8M3xY0ZB1o+44IrIQO+zJzJkz6e/vT92WTZs2pS5/6sBAaIOiMENDmcrv2m9ggJsy9CHIgvXr2dbdzf1eHUeL8PS6dbuWwzhxxgzGP/nkqPXbZszg1sB+B61dy6wdO/iDb32PJ8qeWLOGh/r7M9UHMPXuuzkKuPvBB1nviZio8x11XsetW5f4XWZtVxJzly5lf+Cm226zw8cxTH3wQY4C7rn9dl4c1bcU332Wa7ZIRH6fIX3u1D4WASfOhtVypiilIldxJiJjsMKszxjzM2/1kyKytzFmrTdsuc5bvwbYz7f7vsDjwTqNMUuAJQALFiwwPT09qdvT399P6vJz5tihmJRIluSv/v3mzEnfpjAmTGCPmTN31bGxq4vpU6fG13nxxXaIyW/Z6O5m/MUXj97vl78EqK0fHt61adZeezGrpydbfbDL5+zoE06Al7zEros431HndfuMGcnn7eKL4bzzaj5uSe1K4oYboFrl1NNOSy7ric6j5s+P7luK7z7TNVskMvS5Y/tYAKrqc6YopSTPaE0BvgMsNcZ8ybfpGuBt3ue3Ab/wrX+rF7V5IrDeDX+2hcWLbYLWNHR3W3ESF8U3ZsxoB/jubnucRqjH56y31w4XetnFmTvXLvf2ji7b1TWyvrDPrj433BdXH4QHBISdb3deQ9aveNe74vvo2vWBD9jPIsntSmLbtnSRmjAyz1lU3xr97ovM4sWjh3/L3uc2UFGfM0UpJXn6nJ0MnAu8QkTu8V6vBj4HvFJElgGv9JYBrgVWAMuBbwPvzbFtyTjB4R6qe+5pXyL23Qki98D/+tfhM5+p7T91au3z3Lnwve/Bd78LU6bYdbNnNyYUHGHRmmnyj/X2wrRp8E//BCtXRrcjaLmK+tzba61g++wTXx+E+5y58z19ul3ee+/aeV2ypCaKvPO97vTTk/sIcMop9v37309uVxLbtqXzN4ORPmeub25doyKxE+hmRPtjAAAgAElEQVTthYsuqi3vDn1uA13qc6YopSS3YU1jzB8I9yMDGDUuZIwxwPvyak9d9PbCT38KK1bAX/4yctuJJ1qhdf31tXWveQ18+MPwwx/CCSfAgQfC5ZfDW99aK/PII/DpT8NNN9ntjdLIDAFDQ/FTN0G8OAuKwMHBdMIwKgltby9MnAhveAP8+tdw9NG19VddZYcVV66069L6KLlj1Zsw18/WrfWJM7B9+MIXYNmyWh/KzqteBR/5CFx4Ifznf7a7NaVELWeKUk50hoAkApapXXR1hYsTt83tE1WmGWIBGpv4PKpvfqpVmwrB/TP3tzssijPNseOS0Lr2hKXvqGe6qGaKs3otZ44tW+qf8qoTcX3VRLy5oT5nilJOVJwl0ag4i0pDkZM4I4vlLK04c8fxvwc/u/qyWM7ixFm9VrmoYxVFnDXre+8EXF9VnOWG5jlTlHKi4iyJwcGR4seRJM7cPq22nHWCOIuaIQDixdnw8Iho0VS0W5z5I0Wd5Wx38Q9ScZY7u1Jp7C7XlKLsJqg4SyLKL6vIw5pp625UnDXqcxYmzpJEbdZhwWaLs7TRmlGWM9h9hjZVnOVOzXLW5oYoitJUVJwlUc+wZrXaOnFWz9yaYC1QxoRbBf3kOawZDAiA5p83d6xmCKJGhjWHhmpWtN1laNP1c+fO9rajxLi5NQezWpQVRSk0Ks6SiBJncXNOxlnOXJl2D2v6rXxxZBVnaYbtnGAZM2b0tiRfvawiywmDdkdrunk5QS1nStPo0mFNRSklKs6S6LBozdQBAX4hGUdQnCVFa4atD7Jjhz1uJeTya7aobZfPWbVqX+74/tkTdjfLmYqz3NBhTUUpJyrOkqhXnAVFTbBMnpazNHU3w3JWr/Dcvj3c38zfnmYPa7ZanIHt4+4szjSVRu7U8pypOlOUMqHiLIlGxVlR85zlNazpf49ix45wfzN/e1ScdT4FtpyJyJki8rCILBeRC0K2jxORH3vbbxOReb5tF3rrHxaRM3zrPyQiD4jI/SLyIxHJcLHUR5dazhSllKg4S6JecSZihU1RU2nkKc7SDGtGWc6aLWrbFa0JKs4KKs5EpAp8DXgVMB94s4jMDxQ7D3jOGHMQcAnweW/f+cDZwBHAmcDXRaQqIrOBDwALjDFHAlWvXK64gACdvklRyoWKsyTqjdZMKpPmAd3XB/PmWd+sefPssh9jbNRlPdGawbZGkZflrN5hzXpTabQ6WhOixZkGBLSb44HlxpgVxpgdwJXAWYEyZwGXe59/CpwmIuKtv9IYs90Y8yh2LuDjvXJdwAQR6QK6gcdz7kctz5kmoVWUUqHiLIl6ozXde72O7X19sHAhrFplRdiqVXbZL9BcXcGAgFb4nHV15SvOos5tuyxnxtQnzlz6DLWcFYnZwGrf8hpvXWgZY8wgsB7YM2pfY8xjwBeBAWAtsN4Y85tcWu/DibNBFWeKUipym/i8NNQ7rJlUJukBvWjRyAc62OVFi+wk2hAqznL3OXP7jRvXGQEBzUql4USW+pylp7jiTELWBdVNVJnQ9SLyAqxVbX/geeAnInKOMeaKUQcXWQgsBJg5cyb9/f2pGr1p06ZRZbcO2mY/smw5/YOrUtVTZML6WDa0j+Ug7z6qOEticLC+6Zsg3LqWVmQMDCSvDxNneafScO9jx+5eAQHbttl3FWfpKW605hpgP9/yvowegnRl1njDlFOAZ2P2PR141BjzFICI/Aw4CRglzowxS4AlAAsWLDA9PT2pGt3f30+w7NYdQ3Djdex/wAH0nHpgqnqKTFgfy4b2sRzk3Ucd1kyi3umbksokPaDnzEle34g4a3RYs1Fx1mmpNFScZcf1s3gzBPwZOFhE9heRsVjH/WsCZa4B3uZ9fiPwO2OM8daf7UVz7g8cDNyOHc48UUS6Pd+004CleXfEpQrUic8VpVyoOEuiXcOaixdDd/fIdd3ddr0jalizFT5nQXHmpoPy1x1FI9Ga7Zpb04mzLNGa48bpDAFQOMuZ50P2fuB6rIC6yhjzgIh8WkRe5xX7DrCniCwH/gW4wNv3AeAq4EHgOuB9xpghY8xt2MCBu4D7sPfWJXn3pctTZyrOFKVc6LBmEnHiLMpyFBetmdax3fmVnXOOfZ871wozt95fl799eVnOguJo3Lhoa1ARLWeNCiK1nGWnoOIMwBhzLXBtYN0nfJ+3AW+K2HcxsDhk/SeBTza3pfF48QAqzhSlZKjlLIm4aM0o4dUMyxlYIVapwEknwcqVI4WZv45GAgLqTaURtJxlEWdpAgKKFq3pLF8qztJTYHFWFkSEiujcmopSNlScJRFnOXN5xvxl3Tb33og4Gx62r2DUpqMdPmf+aM16xZkGBDTWlk5BxVlL6KpUNJWGopQMFWdJxIkzt91f1r+tkWhNfxm/v5KfOHGW9E+62dGa/v40MkNAUVNpqDjLTnGjNUtFpaJJaBWlbKg4i8NZruoVZ41azlyZLJYz9zlpIuRGAwKCec6a5XPmws/8dbjvwX/8tLTbchaWhHZ3CwgYGtp9+twGqiLqc6YoJUPFWRxx1qWii7Okh2FacRb0AWuGz1mcOAubkzTqOGloZ7Tm7m458/ezeOk0SkOlIjqsqSglQ8VZHHECJsxxvZnRmlB7oCUNawajNYPtCqPZqTSyBgRE+Zy5NjVbnBUhWtObpHq3FGc6tJkbXRXRgABFKRkqzuKIEzBh+biGhqw4cg/hZlrOwm6+YdGanSDO4ixnrk311h12rHr2C9KMaM099mhOWzoFFWctoVrRYU1FKRsqzuIIm1jcETWs6Rc7ceIszTCPf19nuUlonwnz2YqrO2sqDX+0Zh4+ZzD6vBVBnNVjOfMnod2yBSZPbk5bOgUVZy2hoj5nilI6VJzFkWZYM06cNStaE8KHNlvhcxYXEAA1R30VZ6MJWs6cONtdnOP9/VSfs9zoUsuZopQOFWdxNCrOGh3W9D/QwoIC4ixnSQKgGak0/MtpBdTwsN1erzjLKmyKkErDGLWcqeUsNyoVYUh9zhSlVKg4i6Pd4qwOy9koMZVUd6vFmXtIxwUEBKM1i2I5E4kXlUHGjrXCbGhIxZmKs9yoVkTznClKyVBxFkeagICg47pfKDUarekvE2c587Uvs89ZHuIsThi6h3QWy1m90ZrGNDdac/z4WrBHGlwfd+yw39+kSXZZxZnSRKqiqTQUpWyoOIsjq+VsaKi1w5rtjNZ0lq+w/qSxnLUiWtM/U0IzojWzDGlCrY/bt6vlTMVZblQ1lYailA4VZ3F04LBm7gEB/mjNsPXBz0HqsZzVK878gqAZw5r1irPNm+3xVZwpOaCpNBSlfCQ8mUFE9gXOBl4K7ANsBe4Hfg38rzEmYZ6gDqZI0ZrNDgioN5VGs3zOWhEQUBRx9vzz9n13jtZUcZYbmkpDUcpHrDgTke8Bs4FfAZ8H1gHjgUOAM4FFInKBMebmvBvaFpptOXPO4cH9oqgjWrPwAQFursksMwQUxXKWZeomiBZnajlTmkhXVcWZopSNJMvZxcaY+0PW3w/8TETGAnOa36yC0Gxx5p+MPK9hzbQBAY2m0sjT56xZ0Zp+cdsOy5k7R06caUCAkgMVEYZUmylKqYj1OYsQZv7tO4wxy5vbpAIRN/RXT7RmVpFRT7Rm0S1nrYzW9B+rWdGaWQhazhoVZ319MG+enSJs3jy7XGQGB2HMGPtZxVluaCoNRSkfScOaGxL2F2CtMeaQ5jWpQMRZl+qJ1swqzoocrZm3OGtGtKY7Vnd3e6M1Gx3W7OuD88+HZ56prVu1ChYutJ97e7PV1yoGB+1Q8M6dOkNAjlQrwuBweV1/FWV3JCla86/GmMkxr0nA5lY0tC00OyCgEctZ1mHNvKM180xC2+yAgGaIs2ZYztzE5xn6MOPGG60I8wszx5YtsGhRtja1ksFBe+5BLWc5UhVBtZmilIskcfb3KepIU6YzabbPWV7DmmEBAUn1u+2VhEsgzHJWqYzuf9aAgFam0ogTZ1FDhf7106fDPffAtddmG04MirOJE8PTq8RwwKWXhn/3joGB1HU1leD5mT599Dl0ljNQcZYjVZ2+SVFKR6zZxBizIqmCNGU6lnaLs3rm1szic1atJme8DxNn1Wq0RS3p2GkDAtxclsG66xFnEyaE79fXZ61S7ty6ocJbboHLL6+tr3c40fVx/Xr73t0dL876+qwlbGAA5syBxYsZt25d/DHmtCEeJ3jeos7P0JBazlpApaIzBChK2ag7Ca2I3Jew/bsisk5E7vet+5SIPCYi93ivV/u2XSgiy0XkYRE5o952NZU04ixLQEBWx/Y8hzWD/nFR1CPOihYQEGU5W7RotOjdsgWWLIm3VqUdTnR9fO65WjuCkagOJ3hWrbIpVzyRMxSXvqO7GxYvTm5Hs3DWsnPOSXd+dFizJXRpQICilI6kgIC/i9oEzEqo+zLgv4HvB9ZfYoz5YuA487GJbo/AJrq9UUQOMca0N1tnnDgLGz70D+O4/ZoxrCmSfW7NNJazThFnjabSmDAh/HxEDQmm8QlLM5wYHNaMs5xFCMXIFMFTpsDXvpZ/MICz5q1aZa/DtMNnAwMwa5aKsxagSWgVpXwkPZ1/DPQBYb/8WO9oY8zNIjIvZTvOAq40xmwHHhWR5cDxwJ9S7p8PWYc184rWnDw53HIWF62Zxucsizhz9bk+1utzljYgIMpaVm9AwPCwffl97ObMsaIjSNjMDkHSDCdGibMMQnHXoLMTRjNnwpNPwtVXw2mnJbehEYLDl1n8mubMUZ+zFlGtoHNrKkrJSBrWvBf4ojHmHcEX8Hydx3y/iNzrDXu+wFs3G1jtK7PGW9demhGtCbXks/VaziZPzsfnrB7LmRu6rddy1q6AAH9bHYsX17Y5urutIAmuD5ZJM5wYTEI7YUK05SxJ7BkDc+fC//yPXW5Faoowa14a3PkZHLTfc7Wq4ixHuioV9TlTlJKR9HT+IBCV6+wNdRzvG8BFWEvcRcDFwDvxGQh8hN5tRGQhsBBg5syZ9Pf3pz74pk2bMpWfce+9zAduu/NOtj755Iht4554gpcAD91/P094dS54/nm2jR/P/d7ynIEBDgBu+u1vMWPG0L1yJcd7+29ev54/J7Rl9tKlHAxsrlbZ+fjj3BMov8/SpRwC/PG229gxbRoAVU/83HfPPTwTIzAOGRhg+vAwf0xxPk4VYWDFCh7t7+fg1avZa3iYpfffz4uBu++4g/Xbt7PvQw9xEDBcrbJ21SqWRdS7z/33cwhwy5//zM5ly0LLHP7ss0zauJHbvTrc9wCwdvVqHu7vT/Vd7nX33RwBPLFhA7OAm3/3O4b9FrvZs5nxoQ9xyMUX07VtG9tmzGDFu9/NutNPZ8a0aRz2uc8hQ0Ps9PKTjdm4ke0zZrDiXe9i3ezZkHD8sU8/zUnAzqeeotrVxc233MJLhoZ41uuDnxnnnMOhX/ziru8vDDMwwF333suxwH133cUzWVN7ZOTUgYHQH+aINgE7J0+ma9MmZHiY7TNn7jo/x61fz5bnnmNaVxePLV/OiojzlfV3qYykoj5nilI6kqI1fx+z7Y6sBzPG7FI4IvJt7JydYC1l+/mK7gs8HlHHEmAJwIIFC0xPT0/q4/f395OlvBvyOuHkk2H//Udue+wxAA476CAOc3WOH88es2bVjnH77QCcevLJ1ppw7712fVcXE8eNS27L3XcDMHHvvWFoaHT5+22sxUkvfSnstRcAd3iC54Xz50Nc/VdcAd3d6c5Htcrcffdlbk8P/PCHMH48Lz7mGACOftGL4NRTd/W1MmECs2fOZHZUvV6fTn75y63fVBiXXgorV9batrpmVN17r73Yu6cn3Xe5Zg0Asw44AICXnXRSLUu/o6cH/vpXuPJKxq9cyfwJE6wQ7OmB738fXvEKxl522a7i44H53iuRp58GYMzmzbDHHra93d27+jCqHQccAOedF1mdzJnDsSeeCMALDzss/vttBlHDvo7ubmTJEsb29sIpp8C4cYz/7W9r52f8eHvtjh/PnFmzmBPR3sy/S2UEVUFTaShKycgcrSkid9V7MBHZ27f4BuwcnQDXAGeLyDgR2R84GLi93uM0jbjpm9JGa/rrcWXHj8/uc5Z2WDNLQEBYv8Lw+0m5gIBg/11/kvpWTxJaf+BDvak0/G0M4s5zcKhw587a9EP14E/U66yYcX14/evt+7nnhg+3Ll5ca08rhjXDhn1d6pW5c21UqwtIGDNmdJvc0PnYsTpDQI5UKsKgTq6pKKWinlQaSSMdtpDIj7AO/YeKyBoROQ/4LxG5T0TuBV4OfAjAGPMAcBXwIHAd8L62R2pC/PRNUdGawYAAf5m0AsZfHySLs7BozaT606bSgJEO8sFozax9c4IpTvRE+ZylPW/BYzlxFiVY8xZnMFKcRbXDtfekk6zwmTsXIzJSCLVSnPX22uO6mQ3mzoUf/MD6v61cOTJSNEmcqc9ZbnRVRAMCFKVkpHw6j+DXaQoZY94csvo7MeUXAy1M2pSCZkRr+su493HjsomzSZPSR2s2OyDA1e8XZ11d8dM6xR17+/aR4i6MqGjNceOyRWs6seCEUVbL2eBgY+LMv28ay5k/zUhvL/T2clNwyK+V4gxsO26+GX7xCyvIoogTZ2PGqDjLkWpFU2koStnIbDkzxnwsj4YUkmZFa2Yd+nPs3GlTP+yxR/bpm/ISZ3HRmu5BnGQ5i4vUhGjLWVpR6z8W1C/OGrWc+c9TVnEWRavFGdh2JZ2HKHFWrarlLGc0z5milI9U4kxE/k5ElonIehHZICIbRSQqirM8NGP6Jn+ZeoY1u7rsgz0Pn7NGhjXDfM6cRS0vcVbvsGa7xBnU+uraEHd+iirOdu5M/s50WLNtdOncmopSOtJazv4LeJ0xZooxZrIxZpIxZnKeDSsERRFnEybYB19wnzhx1qwktK7+ND5nLjltkjiLCwZwxwsLCGjU5yyLODMmH3HWiZazNOchTJy5IXAVZ7lS0WFNRSkdacXZk8aYpbm2pIjEibOw4cM8ojXHjKk92IN+Z64+X9b7zBOfpyFOnAUtZ2nEWSstZ04c+OsJEibOXL8aFWdOiGYJCCiLOMvZciYiV4vIa0Sk7jmCy0BVNM+ZopSNtAEBd4jIj4GfA7uyZBpjfpZLq4pCnDgTsaKoHstZloAAZzkDK878ebpCIi5bNqxZrzjbvj2dOAsLCBg/Pvv0TS5Dvb+tQcLEmfuslrPGfM7ytZx9A3gH8FUR+QlwmTHmoTwOVGSqFdEZAhSlZKQVZ5OBLcDf+NYZoPzizImwMMLyceXlcwaj/c7CrF9pLWeNpNLwz62Zp+XMGHv+/ZGgWS1nY8aED0H7UXEWTzN8zuqZBioBY8yNwI0iMgV4M3CDiKwGvg1cYYzZLZKrVTWVhqKUjlRPZ28uzd2PJOtS2PBb2mjNoaGa+Ihi5854ceasWD5y9zkLRmvW43OWRpyBnZPU+Z9VKlYAbNuWrs1QExVFEmfVanQf0oizSsW+OmVY00VrPl/vVLzxiMiewDnAucDdQB9wCvA2oCeXgxYMTaWhKOUj1lfDm8cyljRlOpZGxVmc5QzSDT2OGTNyWNNPmDhrRZ6zRn3OkgICws5bmrrDjlUUcea+w0YtZ65NRRdnw8P2z0e+Pmc/A34PdAOvNca8zhjzY2PMPwN7NP2ABaUiwrABo9YzRSkNSU/nC0Tk6ZjtApyPN9dl6Uhymvf7RvkfRv7trh4YGRDg1scJpKRhzTjLWTvFWVjCXEcanzO/Vc4lnu10cdasgADXpiznoVF27Bg9jVOQoDjzz16R3/RN/22M+V3YBmPMgjwOWES6Ktb6PjRs6KqmmsBFUZSCk/R0vgl4bUKZG5rUluKR5JflFwvuPS5aM2g5S3rANjKsmac4GzMm/2hNV2ew7noCAoomzjrRcpa1Tf5gmpxmCIgSZrsbFSfOjKlryhdFUYpH7G85ztdMRMYaY8qdvChJwPjzcYVFdsZFa/qX446fNKwZbF8rUmmMH99YQECSFSasbmetq8dyVqZoTVdHEYc1jan9YfD/HjTPWa5UPXE2PNzmhiiK0jTSzhDQLyLzfMvHAX/OqU3FIYvPWdgk6Y1azuqI1mxJElr/3JpB61aaGQJ2J5+zsDxnnWg5SyPOXFlQcdZC3LDmoKozRSkNaZM3fha4TkTeKyKLsT5m5Y/gzCLOwixncdGa/uWk42cY1tyV+iOvVBrBaM1WD2t2eiqNRqdvcm1ScYaI/DbNurJTEbWcKUrZSJtK43oReQ/Wv+xp4GhjzBO5tqwINCrOmuFzljFaExgppqIoehJaV2eWuoMUMZVGswICWp2ENk2bYLQ4y2ficxGRacB0EXkBNjAJbD7GfZp5oE6g6vM5UxSlHKR6OovIx4F/AF4GvAjoF5EPG2N+nWfj2k4acRa0imWN1kxz/CyWM2iNOMszCW1wyNQfrZk1IKC7O16cOT8p6Byfs06wnAWjNZsrzvYC7sQKsTupibMNwNeaeaBOoKLDmopSOtIG90wHjjfGbAX+JCLXAZcCKs7qidbMEhAwdmxNzGWxnOXpc9aqJLRZ/dnCjjV1arw4CxNk/s9pz1EU9YizehK+5kkaceb6GTWsuXNnctLl9KwzxiwQkX82xvx/zaiwk+nSgABFKR2pfM6MMed7wswtrzLGvDK/ZhWEvKI1sw5rVip2nzDLWVj70liYOiEJbdDPrd6AgLhozSRx1mrLmf/cRlFEcZbkc+bf1jyeEJFJACLyMRH5mYgck2ZHETlTRB4WkeUickHI9nEi8mNv+22BgKgLvfUPi8gZvvVTReSnIvKQiCwVkZc03sVkqqLDmopSNtIGBOyeNBqtGWZdguzDmmAf7mnm1nTHbXYqjeAQo4h9FT0gIMnnrNXiLCkgIOncuDZ1is+ZX5w1Pyjg48aYjSJyCnAGcDl2MvRYRKSKHf58FTAfeLOIzA8UOw94zhhzEHAJ8Hlv3/nA2cARwJnA1736AL4CXGeMOQx4MbC0wf6lorLLcqbiTFHKgoqzOJIETNqAAL+AEak9rLKIswkTmhsQ0Gi0Joz2uUvyC+vrg02b4OKLYd48uxxGWcRZXx98//v28ymn2OUky1kRxVkWy5kTYK2xnLkL7TXAN4wxvwBSnECOB5YbY1Z4uRqvBM4KlDkLK/YAfgqcJiLirb/SGLPdGPMosBw4XkQmY31yvwNgjNlhjMlnQtEAtVQaKs4UpSxoQuk46pkhIGlY04kM//oo/A/FMMtZO33OgsdJElB9fbDQNw3rqlW15d7ekWXDAinqDQhISqXhX9dMceb6676zNWvs8iteER+tmVacbd9eX7uyMjxsX/UOa1aro4Vb83hMRL4FnA58XkTGke4P52xgtW95DXBCVBljzKCIrAf29NbfGth3NrAVeAr4noi8GBuocL4xZnPw4N58xAsBZs6cSX9/f4omw6ZNm0LLPrTWnus/3Xobq/bo7P/bUX0sE9rHcpB3H+sSZyLyXuAZ4GpjTAZTRoeRR7RmFnGWNKwZJc6a7XPmry8ozoKWsyhhuGjR6PZv2WLXB8VZvcEGQdKk0sjLchbV35tvbo7lbNOm+tqVlbTnISlaE/IQZ/+AHVr8ojHmeRHZG/hIiv3CohKCZqeoMlHru4BjgH82xtwmIl8BLgA+PqqwMUvw5iNesGCB6enpSdFk6O/vJ6zs5nvXwl/uYsFxx3HIzEmp6ioqUX0sE9rHcpB3H+v9myXAKcDPmtiW4pFHtKY/DUU7hzWbZTlL63M2MBBed9j6qPPmjpfW8bmdw5pR/d2wwbYjrA9FHNbMEkEKLfU5M8ZsAdZh70UAg8CyFLuuAfbzLe8LPB5VRkS6gCnAszH7rgHWGGNu89b/FCvWcqfqJgUZ0mFNRSkLdYkzY8zXjDH/bIx5XbMbVCjyiNZs9rBmWPuSxJnL7dWoOIvyOQvr15w54XWHrY/LoeZfn0QwWjNJnIUNcdYrzqL6O2WKfQ/Le1BEceaOU8CAABH5JPBR4ELXCuCKFLv+GThYRPYXkbFYB/9rAmWuAd7mfX4j8DtjjPHWn+1Fc+4PHAzc7iXlXi0ih3r7nAY8WGfXMlH1pmwb1mhNRSkNsU9nEflqijo2GGM+1qT2FIs8ojW7umoPsqzDms89N3J7vZazsLbGETa3plsfJjydn1LFp/0XL4Z3v3uk9a+7264PEiZq/ZOtpx3aDFrOWplKY/HikT5nYPv7N38DP/lJeLCJG4ZNoh3irICWM+ANwNHAXQDGmMddao04PB+y9wPXA1Xgu8aYB0Tk08AdxphrsI79PxCR5ViL2dnevg+IyFVY4TUIvM8Y4y6sfwb6PMG3ghZNcecsZ0MaEKAopSHp6XwW8ImEMhcAKs7SRmvW63MWNqw5OFib2imqXVH1QrZUGmHRmlHDmmDX+8VZby889RR86EN2ee5cK2CC/mau/f52Zj1vYK2D7fQ5c/1atMgOcc6ZY/v72GNWnIUJxSJbzoopznYYY4yIGAARmZh2R2PMtcC1gXWf8H3eBrwpYt/FwKh/FcaYe4AFadvQLNzcmhqtqSjlIUmcXWKMuTyugDe3XTlpljhrR7RmnpazNOJscHD0A/3Vr7birK8P3vKW+OO5Otwxsw5r+ofj2pVKo7d3tPj80pei21JEcVZgnzPgKi9ac6qIvBt4J/DtZh+k6HTpsKailI7Yp7Mx5stJFaQp07EULVqzWQEBYW2No15xFsSlf0g7Q0BQ1Mb5jgXxz1OZ1ucsjyS0QeLOTxHFWaM+Z27ic8gjIOCLIvJK7JyahwKfMMbc0NSDdAAVHdZUlNKR6uksInsB7wbm+fcxxrwzn2YVhEajNd1ds1nRms2ynDVLnEUFBLhyQdJO7N1olCuMFFci0Sk+Wi3O4oSim6g9iU4Y1mxNKg08MXaDiEzHpvfZ7XDTN+kMAYpSHtLmOfsF8HvgRhE5xQoAACAASURBVGpZuctPo9GaIqMFXNZhzWCeM//k0XHRmml8zvJKQus/hh/3cK5nbs2sPmdBIVgUcZZ0fqZOTa6jE8SZ/xpzw23Na/NEEenHOupfBPwAmA5UROStxpjrmnWgTqCqMwQoSulIK866jTEfzbUlRSQsos5PUrQmZBcwDpfuwj30Jkyw6/wTh9ebhLZecebSP/ijNcOS0Eb1zQ1rNjK3ZlTdQYLiLOqcxImzarUmhJtFkmWxaMOazfA5c+eweZazOcD52NxjvwNeZYy5VUQOA34E7JbiTCc+V5TykDbP2a9E5NW5tqSINDp9k1uuxwIUFHtuuMs/tBklHvMa1gwO3Wb1OWtkWDNrQECYOIuznImMFmfNtpq5dsDu43PmTx3TPHEmxpjfGGN+AjxhjLkVwBjzULMO0ElUdeJzRSkdacXZ+ViBtlVENojIRhHZkGfDCkGjAQFuuZ6AgGB9YeKsUZ+zrKk0XJ1Bn7PhYWvVa1ZAQFK0Zr2Wszhx1t3dWeJscDD9TAmNUMxUGv6Ob43ZtlugqTQUpXykMp0YYzp7wrZ6aTSVRliZtI7twYeiy2fmj9iME2fbtkXXXW8qjaA4c0O2/r7nZTlz0zdF1Z10rLKJM1dHHm3006g4839vzRNn3d6fQwEm+P4oCjC+WQfpFLqqajlTlLIRazkTkVlJFaQp07E0Gq0ZVqYVlrO8fM7CxJl/uDPvVBplsJwlRWtmEWetGNqs1+cs32jNO40xk40xk4wxXd5nt5yzWi0eLlpTfc4UpTwkDWtem7A9bZnOpNFoTbecpziLitYsojjLajlrJFozaPFJitYMirO8rFLNCgiA1oizAs+tqVgqLiBALWeKUhqSns4v9g0fBH/5LoytvL5nWSxncdGafpGRlLHe0eiwZp7izO3nLHStGNZsRkBAXLRmJw5rtlKcJZ0L1y8VZy1nl+VMxZmilIbYp7MxJqXHeEnJw+esu7t5lrNWRmsOD4dHa2bxOat3WLNVAQEbNoxcr+IsvThzOf1UnLWcqlrOFKV0pIrWFJHzAstVEflkPk0qCMakE2cwUrjkHa2ZxnIWJUSi6k7CHcM9eNMMazYyQ0DQLytNDrWkYyWJswkT2ms5c9dQp4ozVyZMnLl8cSrOcmFXKg31OVOU0pA2lcZpInKtiOwtIi8EbgXKHcEZTLgahv9BmyVaM43ICD4Ub/CmDHzTm2DePDt5eKMTn2dJpQG1h2ucOEuThLaegICs0zd1WkBAWt8uaE9AQNb8a34rq4jdX8VZLugMAYpSPtKm0niLiPw/4D5gC/BmY8wtubas3aSxLoWJszTRmnFzPYYdv68PFi2qbVu1ChYubO2wJowWZ/X6nCWJHv+cpMPD9hWsO6mOooqzKMtiVhEExbacBf0Tx45tXeLc3QxNQqso5SPtsObB2ES0VwMrgXNFJHaGZhH5roisE5H7feumicgNIrLMe3+Bt15E5KsislxE7hWRY+ruUbNIk6jVbwWJEjxh0ze5cmnF2aJFI4czwfqe7djRumhNCLecZc1zNnZs8pRIzn8pGITQSEBAXLSmiLXmtXNYM4s4Czrf50mjw5ruWhkzRi1nOaEBAYpSPtIOa/4S+Lgx5h+BU4FlwJ8T9rkMODOw7gLgt8aYg4HfessArwIO9l4LgW+kbFd+pEnU6n/QRg0V+qMEs4gz/0NxYCC6XCt9ztywpNuvnjxnacSHqyuL8AsSHCaMi9YcM2b0lEhFFmedYDlzVl0nxHVYMzd2pdJQbaYopSGtODveGPNbAGO5GHh93A7GmJuBZwOrzwIu9z5f7qvjLOD7Xt23AlNFZO+UbcuHeoY1wybKDhvWDK5POv6cOdHlWjms6cRZI3nOsoozv0iuJyDAiYq4YU0VZ9E04nPmv75UnOVG165ozeE2t0RRlGaRNEPAKQDGmFG5zIwxy0RksogcmeF4M40xa7391wIzvPWzgdW+cmu8de0jizhzAiWsbFi0ZnB90vEXL65Fazrccif5nG3fnhwM4D9m0Jcvr4CAVoqzKIFZVHHWiOVMxVlLqKXSaHNDFEVpGklP578Xkf8CrgPuBJ7Czl13EPByYC7w4Sa0I8wJKdRILyILsUOfzJw5k/7+/tQH2bRpU+ryY59+mpOAh//6V9ZG7DNr+XIOA/70+9+z76OPso8Ivw+UffGmTcjQEPf09/OSLVt4Zt06Hunv56ThYZ4eGOCRiLqn3n03RwF3338/6486ihkf+hCH/+d/gjFsnzmTFe98J/M/+1keHRhgla+OTZs28dgTT7DXtm38MaLuvf7yF44A/nzXXWx+7rnEc7HPihUcAtx7xx28CLjvwQd5Zto0Dn/6aSZt2sTS227jWODeBx9k+zPPcBxw/z338PSUKSPqOWxggKnDw9ya4js42RjWrVrFyptu4mRg2aOP8txdd3E88OC997Lp+ONjv8v9li7lQODmW29leMIEXrRxI9Vt27g7sM/BK1eyF/DE2rXsu307N3vbFzz3HNvGjuX+DNdXGrpXruR44IF77+WpPfesrX/0Ubt+2TKeCnyfwX5OfeABjgLu+fOfeT6NUG2A/ZcvZ06lwk033ZRYdsGOHWxdu5YH+vs5aOVKZgF/8Np+3M6dbH7sMR4MOZ9ZfpfKaNzE55pKQ1HKQ1IS2g95TvtvBN4E7A1sBZYC3zLG/CHj8Z4Ukb2NMWu9Yct13vo1wH6+cvsCj0e0aQmwBGDBggWmp6cn9cH7+/tJXd7z8zr0iCM4NGqfVasAeMlxx8Ef/whjx46uf/p02LTJrq9U2Ge//dinpwe6u9lnxgz7OQzPAnH0ccfBySdDTw985zvw2tcyfskS5m/fDp/9LPsfdBD7++ro7+9n9pw5IBLd1yeeAOC4l7wEDjss4UQAy5YB8CKv7AuPOqrWnkcf5dgXv9huP+aYXUOwRx52mC3j51vfgilT0n0HEyYwe9YsZp9wAgAHH344nHQSAPMPOYR1e+wRX88tNpj4Zaefbi06e+0Fzz47ep++PujuZs6BB8LQUG37uHHssffe6a+XtDzyCABHHHroyPPjCdkjjj56xPrQa9azSB115JGjz3Gzufba8Os6jKlT2cN9v1dfDePG1fZ7wQuYOGUKM0LqyfS7VEaxK5WGOp0pSmlIHNcyxjwHfNt7Nco1wNuAz3nvv/Ctf7+IXAmcAKx3w59tI83QXzBaMypyslGfM8f48bBtm/0cF7AQ5fweV3ccWfKcJSWhbUZAQJZoTX/wQtKwpkvbUamoz5kjy3emw5ptwdNmOvG5opSI2KeziPxL3HZjzJdi9v0R0ANMF5E1wCexouwqb8aBAaw1Duzk6a8GlmPzqL0jZfvzI2tAQNQk5M2I1nSEibN2BATERWvGOe03GhCQ1efMn7YjTbSmW3ZpNVScZTsPKs7agohQrYjmOVOUEpH0dHazABwKHIe1cAG8Frg5bkdjzJsjNp0WUtYA70toS2spUkCAY/z4mkCKy8NWhoCAMMtZlmjNnTtHCp00AQFuOU9x1okBASrOCk9VRC1nilIiknzO/gNARH4DHGOM2egtfwr4Se6tayf1pNJIEmf+jP7NGtZspzirNwlt2mM2Gq3pFxVpxZkro5az2jGyiDOXLDlMnG3e3Pz2KYD1O9MktIpSHtLmOZsD+P/27gDmNb01RSLNDAH1iLNm+5y1MgltFp+zPCxn9QxrBusLEmY5869vNlF+c0UVZ83yOdMZAnJFxZmilIuUT2d+ANwuIv+DTXHxBmrJZMtJvUlow8oMDoIx2YY1w3zOxo0Dl/oiyXIGNef2IGmEZ1h9jYqzrD5nWYINko5VNHFWVsuZf25N//Wlw5q5UhGdvklRykQqcWaMWSwi/wu81Fv1DmPM3fk1qwCkmb4pS7RmsL48ozVdu4aGwsVZmr6F1dcMn7NWTd8UFGdpojXdsn99s9ldxJn6nLUUtZwpSrlIaznDGHMXcFeObSkWzY7WDNaXt8+ZKxP2YG1mtGZWn7Osw5qNTN8UtJxFRWuOHds6caYBAUoOVCsVDQhQlBKR1uds96PZ0ZpZLWdRqTTSRGsmWZja5XPWSJ6zegIC6h3WdPnO1HKmec46hGoFTaWhKCVCxVkUzY7WDIqpVlnO4uputc9ZI3NrdnXVhmjzTqWRZT7JrER9N0UVZ2o56wiqIgyqOFOU0qDiLIq8xFkjw5rjxjVPnFUq4f5oYcT5nBkzMht/pWITv+YxQ4BI9PBk0rHixFlXV+vEmUi4/5uKM6UBqlVNQqsoZULFWRTNjtbMKs4anSHAXyZIlJCMIs5yBtG+aEGyBgTU46vnqDfPWd7iLKotWcSZOxcqzhQPTUKrKOVCxVkUacRZlmjNZgUEbN9eS8sR1b40PmdphzRdHyC9OIvqWz0BAY2Is3qjNdspztIc01kQi+ZzNnasptJoExWN1lSUUqHiLIqiRGv6hx7Hj7fv27c3ZjmLamsUcRaysPVhfRseHu0HFkdUIEWUyAqSJVqz1eIsalizWk0/1Oy3UuVJMy1n/j8VSlOpioozRSkTKs6iaHa0ZtaAAPdQdBN3w0hxFufU36phTVdHGnHmHtr1Ws7SnjdHmDgzxorEYLvaYTkLCwhIK1yh2OLMmNHXmKtDrWe5oHnOFKVcqDiLIk1EY9qAABjpNO/ekyxnwfqcONu2rdg+Z436VLm6o4Y16w0IgHDRWJRhzTKIMwj/s+L6puIsF6oVYVh9zhSlNKg4iyJNFv0s4sw58jcizpzVqWjiLE3fnIBrlc9ZWCoNKK44yzLkC60VZ1nynLl9VJy1lGpFU2koSplQcRZFPQEBcQlhs4qzMItFWstZmoCAPCxnzl+qGZazZkRrBgMCoLjirKiWs2DUaxwqztqGDmsqSrlQcRZFs/KcZbEuBY+fZlgz7pitHNZ0echcm6IsZ60MCAim0oBiiLOogIAiirN6hjV37gyP1gQVZzlRFR3WVJQyoeIsimZGa0L6dBP+4+flcxZ8cCYR51vm1vvbGuXwDu0NCICR7TJGAwKSqFecqeWspWgqDUUpFyrOomhmtCbUhE3WaE0/7Y7WDPbBbxX01xdmGcpqOWtFQICrR4c1o1Fx1hF0qThTlFKh4iyKZg1rNjMgoMg+Z0HLWZTPWbuS0IadE78IU3EWTtaJz0HFWRtQnzNFKRcqzqIoojhrV7RmMB1II+Kskbk1o+oOMjxs+67irDGGhuzQbzMsZ3/4g30/7jiYNw/6+pra1N2dighDqs0UpTSoOIuiWdGajQQE1ButmXdAgD8qE9KJs3oCAsKiNaMCAvr67EO/UoG99rLrPvnJmhAIi9aMEmeuzO4eEJBVpEaJs74++MpX7GdjYNUqOOccmD5dRVqTsJaz4eSCiqJ0BCrOokiThDbN3JpxAQHOMhHGzp3FjNasVGpRmVE+Z80c1gz2M6zuvj5YuNA+9I2BZ5+tbVu1ym679Va7XBTLWaMBAV1dxRZn/gCZRYtq17+fZ56BhQuZceONjbd1N8eKs3a3QlGUZqHiLApnCfNPnxRExJYZGqovWhOyCah2+5wFLWRZhjXrTaWRJlpz0SLYsiW6ri1b4Mc/tp/959svPlz7dVizRtah6KDlzH1nAwPR+2zZwgGXXlp/G+tERM4UkYdFZLmIXBCyfZyI/NjbfpuIzPNtu9Bb/7CInBHYryoid4vIr/LvRY2qCMPqc6YopUHFWRRRw5RB/CIijc9ZcF7KqAds3LBmo9Ga9abScJNzB9fnERDgRK87P/7zFuxX3MPf8cwz9j3KcubqV3FWo1nDmnPmxO42bt26OhtYHyJSBb4GvAqYD7xZROYHip0HPGeMOQi4BPi8t+984GzgCOBM4OtefY7zgaX59mA01YowpHnOFKU0qDiLIsoSFiSrOAtazqKsW2HDmkWYvsl/vCw+Z/UEBLi6K5X42QcSHv6A9W+CaHEmUhM8bn2Wc5SF3U2cLV4M3d2Ru22fMaOBRtbF8cByY8wKY8wO4ErgrECZs4DLvc8/BU4TEfHWX2mM2W6MeRRY7tWHiOwLvAZouSlQozUVpVzk9PQpAWkFjPP9abY4yzMJbb3izJhwy9m2bTBlSm19mHWrnmFNt1+S8Fu8GN79bti6Nbyu7m54+9vhC1+IFmfuXWcIqFGvOAte67299v3882sWTEd3Nyve9S6CZqucmQ2s9i2vAU6IKmOMGRSR9cCe3vpbA/vO9j5/Gfg3YFLcwUVkIbAQYObMmfT396dq9KZNmyLLPvXUNjZtHk5dV1GJ62NZ0D6Wg7z7qOIsirQCplodnWIiuB3qE2dOjDm6umx9acVZs33Ooj6H+aI1IyAgbd29vfahf/75dnnPPe37s89aq9rixTBjRnHEWTMCAorscxa81sF+R729NnjjIx+BtWutNfPLX2bd7NmtFmdhjqRBs1NUmdD1IvK3wDpjzJ0i0hN3cGPMEmAJwIIFC0xPT2zxXfT39xNV9pfr/sLAlmcit3cKcX0sC9rHcpB3H3VYM4oslrOgs39wO0QHBMQNa4aJg/Hjk6M16wk2iCNsKNO/Pq+AALB9DR4/7Jy98pX2/cor4emn7Wt4GFautKIgKc+Zex8cVJ8zR72WM2fBDLvGenvh97+3ny++uGZVay1rgP18y/sCj0eVEZEuYArwbMy+JwOvE5GV2GHSV4jIFXk0PoxqBR3WVJQSoeIsirzEWTAgIKt1KyjOWjmsGfxcj89ZVstZWJqOsH5t3mzfJ06Mry8qWtO9a0BAjUbFWVTQifuO3HfWev4MHCwi+4vIWKyD/zWBMtcAb/M+vxH4nTHGeOvP9qI59wcOBm43xlxojNnXGDPPq+93xphzWtEZgGqlogEBilIidFgziiziLGwYx78dmuNzBlactWtuzbjPzbacZcmhBrUHfZTjeVrLmYqzGnlYzqD2HcWlP8kRz4fs/2/v3IPkqO57//nt7ErsrgQrBCvEImkFEjIyxigIzMMQ2eEG7OsKhNjXj8WXuteUDGXHNnZwgVVJ5Salwi5wuE4RbkUlYlPOBCzzcKjYMcSGhYBjXgbEywIh9EKgJ3o/Vrt77h+ne6e3t3ume6Znuqfn96na2u6e06fP6Z49/d3f45yvAg8DBeCfjDGvisjfAM8ZYx4C7gJ+LCJrsRazzznnvioiq4DXgGHgK8aYCIu91pdCGzqVhqLkCBVnYaQtzsLcmpMn1245izuVhoj9CUsIgOiWs6gv+nKWs6B75r7oK1nOVJxFp9qYM/dZhP39pG85wxjzC+AXvmN/5dk+DHwm5NzlwPIydQ8Cg0m0MyoFEYZVnClKblC3Zhhpi7Na3JpJT0LrvU4t4mzSpPKT+nqJkxAA0d2aUcWZO8FwPfD3wZ3EOGvirF6Ws0LB/pORojjLG4W2NrWcKUqOUHEWRpxsTf8Es/7PobHiLGm3prfOWtyacZcncs+L49YME2eV1tZ0f7virF5WM5gYN+e2Iy/irNw/Ky7d3SrOEqTQhsacKUqOUHEWRjNna9ZTnPmFUth2kOUsajKAt76gbM1aEgKyIs687YjrPoRSZmk9qZflDGzcmYqzxGjTSWgVJVeoOAsjzvJNjc7WPHKkdSxnSScERM3WbAZxNjpqf+pFtTFnUcRZd3dqCQF5pCAqzhQlT6g4CyPO8k1pxZy5gfph10wr5sxdF9NLXMtZ3GxN90WfVLZmM4gzqK9rs15TaYC6NROmXdfWVJRcoeIsjHokBIiMXyPSvU4QlbI1y1n2ks7W9NaZZctZZ2fp/obVlwVx5k8IyKs405izhtHWJhij02koSl5QcRZGPcRZUIxWLZazasVZUpazuDFnSSQElMvWDIs389aXBXHmj5vLizhz77GKs4ZTcCzoaj1TlHyg4iyMpLM1/SLDfeE1sziLO5VGNQkBR45ETwgoJ86ylq3ZDJazuO0SsX2LmhCgMWeJUSg44kwtZ4qSC1KZhNZZf24fMAIMG2MWi8jxwE+AfmA98D+MMe+n0T4geOHxIOIkBMS1nFXK1gx7+dUz5sxvxfJf092ut1vTbyE4cCA83sxbn4qz6FQzGW9Hh06lkQKu5WxULWeKkgvStJx9zBhztjFmsbN/E/BrY8x84NfOfnrEcWu6L7Gk3JojI1Z8VMrWrMZyNjoaXnc54lrObABM6Vi1lrOhoeD75s9SPHgwmlszK9ma3vuTN3Gmbs2GU2hTy5mi5IksuTWvAO52tu8GrkyxLfHEWdB2pc/LiTP3WFB93uWbqhFn5eouR9yYM++1IL7lrMJ1xN+3Zoo587tY4y5t5S2bZXGm2ZoNQ8WZouSLtMSZAR4RkedFZKlzbIYx5l0A53dvSm2zZEGchbk1jx61L/Swl587xUa9xVkloeZ33VXj1vRvO9dMVJy5nzXScgal55N1y1ncdsWJOavnPG0thIozRckXqcScARcZY7aISC/wHyLy+6gnOmJuKcCMGTMYHByseE7vr37FqStX8ofbtjE0dSoAHfv2caS3l3XXXsu2Sy+dcM65e/dyYNcuXqtQ/8L33x9TkS+8/DJ7/O09epQ/dLYPj4zwW6e+zo0b+Qjw2urVbDvxxHHntO/fz0eBtevXs9l3/VlbtnAasHXdOnpGRvgv3+f79+9ncHCQS9ra2LRuHW/7Pi8cPMjFIXWX49yhIbqBnXv28LJzXtuhQ1zifL7x3XdZ5xw/Zf165gH/+dhjjDiC6dxduzg4dSqvRrzm1DVrOMfZ9l7Trfvg3r3jnv2527dzoLs7/HmNjLAEePvNN9nglJm7di2zCgWeePxxAE7fsYPpBw5wcPt2ZHSUF2PcnzicsmFD6f50dXH8c89xFvD8yy+zzyc63efp54Q1azgTePY3v+HAtm11aWf/G2/QDww++WTkNVHPHx2lbc8eJgG/W72avSFxj7O2buU04IlHHmH/8HCkv2MlnDbN1lSUXJGKODPGbHF+bxORB4HzgK0iMtMY866IzAQC3zjGmBXACoDFixebJUuWlL9YsQi33z6WGTZp796xj47ZupWFt9/OwjPOgIGB8edNmkT3zJn0Vqr/pJPGNhctXgwXXDD+c8/L9pjubsba+9ZbACw8/XQW+q+xYwcA8z7wAeb5P3vpJQBmTJkCnZ34+z84OGiPdXQwp6+POf7z37c5FvMWLJhYdzkcUTv9xBNL13QDv4HZc+cy2z3utPHiCy+EadPssY4Ouvv6JrQ3lJ6esc3pvb2l81avBmBKZycf9dZlDN39/eHPy3lpzZ01i7lumZ//HCZNKtW9ahU8/TSTu7th8uTobY2Le38uuMDen927ATjnggtg0aJxRceep599+wA49+yzYfHiiZ8nwSOPQHs7Sz72sejnTJky9v39g/POC2/bK68AcMk55zD46qv1u9ctglrOFCVfNNytKSLdIjLV3Qb+GHgFeAi4xil2DfCviVxw2bLyKfsHD9oyfpJya3onRQ1yA1bj1gTb7nLtC5ql31t3EtmacVy21SYEhGxPcGtWSggQmThHmt992Wi3pj/mLItuzbj3IU62JmjcWUKoOFOUfJGG5WwG8KBYM3w78C/GmF+KyLPAKhH5ErAR+EwiV9u4sboycZZvCtp2ced+8ou9cuKsXPanK84OHCgfcF0vcVZtzFm1U2mEbMeOOXPP9WdrpiHOwhICsijO4rQJbLvc/qg4axhjU2loCJ+i5IKGizNjzDrgwwHHdwJ/lPgFZ8+GDRsql/GTlOXMPR5HnFXK1oRsiDM38cA/NUcSCQHl1u3EJ85GR20QehRxliXLWdYTAoaGqrOcuZT7frpz0ulEtIkwZjnTmDNFyQVZmkqjPixfXn5y0q4uW8ZP0uIsrGxcceZazvbvL//yC5oItlLd5QgSZ979KJazerk1Ky167j03S+KsGSxntYgztZw1jJJbU01nipIH8i/OBgZgxQqYMwcjAtOnlzLP5syxn/mTASBZcRZVwLiUm18qS25N7349p9IIsKKJ9yXkvuCTsJyBjZlScabirIkoibOUG6IoSiLkX5yBFV/r1/P4o4/aTLIzzoA/+zNYvz5YmEG8tTWDtr3Uw3JWrThzj5U7N6y+oPOi9G101G7X23JWSZxFSQhw61NxVn3MmYuKs4YxNpWGJgQoSi5oDXHmp6dnbPqCUOrh1oybrVlOnGUhWzPseBLiI444S9pyVm9xlkRCgHtPshZz5u1DpUloQcVZQmi2pqLki9YUZ9Omjc33FUqaMWdR3Jqjo80Tc+aKj3pZzuKIs0rZmtA4y5k/ISCLyzfV262pCQGJ0K4JAYqSK1pTnKVlOfN+7s5/Vm22JjRPzNmRI/Z3vbI1XXGWREIA2OzTRro1jx61x9pi/DnmRZyp5SwR2tRypii5QsVZEKOj9qee4sw7/5mfKG5NyIY4C+qbf+H1erg165EQ4L1Oo2POqo3tynLMWbnvZ2en/a3iLBEKGnOmKLmitcVZWNp5nKD5arM13f1q3ZqV2pdFy1m1bs2gbM1qEgKiWs7820nTLOKs1nnOyn3H2tqsQFNxlggac6Yo+aI1xdm0aVaY7d8f/Ln74k86WzPI6tTslrNmSgiImq3p306aZhFn9XRrgn1eGnOWCK44G9WYM0XJBa0pztxFtcNcm3EETLVuTXe/FnFWrn1hddc6lUYt2ZpxLGdtbaX56BqdrenfTpogt2+rijO1nCVCwRnJ1XKmKPlAxVkQWRFnQS/GrCUERMlErSYhwHutJBMComRr+reTppksZ9W2S6RygoOKs8TQec4UJV+0tjgLm06jWnEW9jKKK87KLXze1lZ6AWZBnNXLremtJ6huvzgTGW9VDKsvC5azZhFntcScRfl+qThLjHZn7FFxpij5oDXF2bRp9neSlrP29pIbrlwZ//Fq5iJzRUiziLNqEgK89URJCOjuDr//3nPzIs4KBdvfrLo1o7jNu7pUnCWE+3+hznOmKPmgNcVZPdya5crGzdYs59aEaOIsAFAgnwAAGEFJREFUi5PQJmg5m+DWrBRv5p6bF3EGpYXa60Ut4iyq5UwTAhJBszUVJV+oOAsijoAJEy1e4mZrlnNrQrYsZ3Fizqq1nCUlzrKSrRm0fFNWxVm17lZ1azaUdhVnipIrYr6hc8Jxx9nfScacVcqcDCpTq1uzkrWu3MLn1YqzKNmaSUxC660zijirlAzgnpsFceaPm8uqONOYs6bh0d9vA+DP73mBb9z7IiPG0NPZgQjsPniUk3s6ufGyBVy5qC/lliqKEoXWFGeFAhx7bOPcmklma0LJAlWL5azaqTQa6dYsl63pXyEgqlszq9maxx4bv55md2tqzFki/OyFd/j+I2+M7btxZ7sPlb4b7+w+xM0PvAygAk1RmoDWdGtC+SWc0hZnSbg1sxRzVm+3ppsQEKW+LFnOhoehWITnn4dHHoH+frsfFVecFYv23La28XWEHY+Kxpw1Bbc+vIYjwyGrnXg4dHSEWx9e04AWKYpSK61pOQMrziq5NeMs35SGW7ORMWdhcXONSAiolK154EApA7dSfVkSZ089BffcUxLjGzbA0qV2e2Cgcj0dHbBmDdx7b0nkuHU89RTcfffE41HrhsaIs8OHg7+nSmS27D5Ul7KKoqRH61rOpk0Lt5zFicvKaramJgQE15cFcebewwcfnGg5OngQli2LVk9HB7zwQnAdK1bUVrcxtSUERPnHxnlmBff7oVTFyT2ddSmrKEp6tK44S8qt2erZmlmZSiNKQoA3W3NkxAqQNC1nu3YFf75xY7R6OjrC3YJh1qiodQfdn6htguiWM6Bw+HC8ayjjuPGyBXR2VBbDnR0FbrxsQQNapChKrag4CyLtmLNKbtUo2Zr+4Hd/3fXM1gyznCUhzsKWb4prOXMFcJriLMwVO3t2tHo6OqAzxBIS9t2JWnfQ/YnaJoieEAC0HVJXWy1cuaiPW676EH2OVazgTMbc09nBtC77PCa3t3HLVR/SZABFaRJaN+Zs2rRsTKURZPkYHi7NAB9E1GzNcgufp2E5i/uiL1P3uGzNOAkBbv+zIM4uv9y6Nr2Wo64uWL48Wj0dHXD66fDmm+O/R11dcM018KMfgVf4xKm7EeJM3ZqJceWivlDh9Rc/fYnH39iuwkxRmojWtpzt21d7RmO9sjXLvRSTcGsmNZVGlJgzdx6vSssr+YmSEDA8bOtvRsvZWWfBddfZbRGYM8fGikUN2O/ogN5euOOO0rHp020dd94JN9xQOj5zZry63ftT70loUbdmvTl9xhS27zvC+weG0m6KoigRaV3LmbtKwN69cPzx4z9LW5wND5evL+sxZ/5JaI8ciZ8M4K2zXMyZO09WM4kz7woBc+bY7W3b4IQT4tXjTqVx7rmlYzfcUBJg8+eXjt91F3ziE9HrrtbaWYU4U7dmfZk/YyoAb2zdx0dOnZ5yaxRFiUJrW84gOO6sXmtrRk0IaIQ4a4v56OOIs7Y2awmqdXmioHvrtHuCOIu7QkAWxNnIiJ3ioqvLWrzi4oqztWtLxzZsKG2vXx98PAq1ujWjLnyOWs7qzQJXnG3bn3JLFEWJSuuKMzcYOyjurF7Zmo10a1YSfnFdjHHEmf/6SVrORKBQqM5y5o3DS1OctbXZn+FhK6DmzIn/PGCiOJs/f6IgO+kkW67R4kxjzjLDzOOOYerkdt7cui/tpiiKEpHWFWeNtJwl7dZ0hU4la12Y5SyuS9OtL+iaUfqWpOXM2R9LCHAD4ZNwazrCb8LxeuC2ZcMGO3t/NXjF2fTpsGjRRHE2dy7MmjX+eBQaGHOmbs36IiLMnzGFNe+pOFOUZkHFWTlxluYKAfV0a9Yizqq1nFUjzsrUXXXMWblsTe9+I8WZG3cWF684mz/f1rNhA7jC1a27vz++5ayBMWfq1qw/p8+Yypvq1lSUpkHFWa1uzXqJs3pla46MxM/U9F6rGnE2NFSbWzMgVq+mhAB39ntIV5zt3m0nok1CnM2bZ0XY0BBs3Wqf88aN9pgr2uLQwHnO0hBnInK5iKwRkbUiclPA55NF5CfO50+LSL/ns5ud42tE5DLn2CwReUxEXheRV0Xk643rTWXmz5jKrgND7NivLmRFaQZaV5y5MWdBlrO0l286erQ2y1mxCCtXWuHiX/C6XpYzf4JBvd2a1SYEgLUspS3OCgV46y27XYs427/fijBXnIF1Yb77bikbdM4cux8ntqsR4uyYY0Ck4eJMRArAPwCfABYCnxeRhb5iXwLeN8bMA24HvuecuxD4HPBB4HLgTqe+YeBbxpgzgPOBrwTUmRpjSQHq2lSUpqB1p9KYMsUKikbGnHmFTbFoF6zev9++VJcvL02BUINbs/dXv4Lbbx+/4PUXvwhXX21f0qedlpw4Kxbtwt1gY5u8fahXQoCzX7XlDGy70hZn7e0lcVZLzNk779htvzhzXZtz5tjvizGwaZMtF4VGxJyJQHc3bY23nJ0HrDXGrLPNkHuBK4DXPGWuAP7a2b4PuENExDl+rzHmCPC2iKwFzjPG/BfwLoAxZp+IvA70+epMjXXbrSj7wsqn6ensQAR2HzzKcZ7tk3s6ufGyBTpZraJkgNYVZyLhSzjVO1uzWISlS8cLqKVL7fbAQE1uzVNXrpy46oAxpets3gxTp5bpUAj+flbqQz0tZ4VCSXzESQjwzi+WBXG2aZPdrsVy5jJvXqme9etLz3zOnNISTxs2RBdntcacRXWdd3en4dbsAzZ59jcDHwkrY4wZFpE9wHTn+G99545TM44LdBHwdNDFRWQpsBRgxowZDA4ORmr0/v37I5f18pstR/nRK6UJaHcfOhq4/c7uQ3z7py/y2uuvceHJdf7+h1BtH5sJ7WM+qHcfW1ecQfgSTvWOOVu2bKKAOnjQHh8YqOzWLJOtOXnbtvLtHRmxKyPExe+ardQH77QVR46UYvzikHfLmTFWtJ50UvV1uMybZ+/BCSdYERYmzqLSCLcmpCXOguYtMRHLlD1XRKYA9wPfMMbsDbq4MWYFsAJg8eLFZsmSJRGaDIODg0Qt62XZdx9laLRyOYChUfj5xgLf+UL86yRBtX1sJrSP+aDefWzdmDNIxnJWjTjbuDG4nHu8Brfmkd7eMo11CEoUqITfclapD97MyGoTAspM3luTOBsZqSzOqkmaiIPbltmz408I7OK2ddq00iS2/f3WcrZhgxVq3d1wyin2GnGm02iUOOvqSsOtuRmY5dk/BdgSVkZE2oHjgF3lzhWRDqwwKxpjHqhLy6tgy+54U5XELa8oSvKoOEtDnM2eHVzOPV6DW3PdtddWDo5PIuasUh+SmEojquWsvT2aiIhqOatmkt64uPexWpcmlNrudVV6xZlbd0cH9PWVLGfFoi3X1mYF3Akn2G1v8kgjYs4gLcvZs8B8EZkrIpOwAf4P+co8BFzjbH8aeNQYY5zjn3OyOecC84FnnHi0u4DXjTF/15BeROTkns66llcUJXlUnPnFWbEIf/u3dvvMM8dnOgYRJ1vT/b18ebCAcq0djz8OTz45MdPSxfVzf/nLE8psu/RSu8C1+2L2i4y2NrtYdhyKRfjmN+32n/6p3Q/qQ1eXPV4s2mD3Vatsf9asgfvvD+9PGFHFWRSrmbeeSuKs3i5Nb1uSFmfutBnuygP+426soOv63LnT/hhjj119tU2WucbRJZdfHu+ZNYE4M8YMA18FHgZeB1YZY14Vkb8RkT9xit0FTHcC/r8J3OSc+yqwChvo/0vgK8aYEeAi4IvAx0XkRefnkw3tWAg3XraAzo5oluDOjgI3Xragzi1SFKUSrS3O/DFn7ovLFWybNtn9ci+naixnAwPjBZSXnTtLFic3yN57/WIRbrmltB9UZmCgFBT+4x+PXx5odDTeAtvuPdmxw+6/914p8N/tg4j9vWKFPb50aUn87Nw5flLUSvfTS5mEgHErBKg4Kx3r74fDh+GNNyaKs/Xrg2MF/Rw4YLOIwU7BEeeZVSHOUnBrYoz5hTHmdGPMacaY5c6xvzLGPORsHzbGfMYYM88Yc56b2el8ttw5b4Ex5t+dY08aY8QYc5Yx5mzn5xcN71gAVy7q45arPkRfTycC9HR2MK2rY2y7p8s+s65JBW656kOarakoGaC1EwL8lrNKQe5BVLu25sCA/ak0e7v/+suW2Zdv1Da61wG7vM+LL8Lq1ROn7wij3D1Zv37i+f395V/+le6nl6QtZ1GyNaO6SGvF7VO102gUi/D3f2+377wTFiwofZ/ACnO/OLv33pJQjkOcZxY3Zq+rS1cIaABXLuorK7q+ds8LPLl2B586a2YDW6UoShitbTnr6YFDh0qTc1YKcg9i1Sr7+x//MdxtV866Vq7uoDLVtBFsu15/vbQf1YoV93px+1OOJMVZsQjf+pbdvvhieOIJu92MljPXmrlnj93fubP0LL1iz789MgInn1xde6M+s/vus7/vuCOaGzudmDPFx2UfPIldB4Z4bkNA9rqiKA0nc5YzEbkc+AFQAFYaY75blwsVi/D979vtmTNtLJbxZ9M7hAW/F4tw3XWlff9cX24ZVxR8+tNw223jLRCzZ1ee4sB7/bDyYW10WbZs4gzxUSwica8Xtz9hFIslN+nChdaVOzBgj7/0EscPDVn37J491hJWzhLon5Ntyxa7ggLARRfB975XqvuZZ6xlMqplsRqKRWu9BFu/e/2olLNm/uVflo5df711Tw4MlCa8dSetjUvUZ3b99aX9oL8Hf/mf/pRJ+/bV934rFVmy4EQKAl+6+1kOHhkZm5z2/YNHKYgwYkzo5LWVtuPU8f7Bo/Q8/khi9dWzjqT7mGabWrmP1dbh72PSkziLCRMkKeAsg/IG8N+wKevPAp83xgTOsr148WLz3HPPRa5/bF4S/8u6HF1dViQEvTTCXJJufE/Qdfz1VWpLlPKeMqFzr4SJT5Hyrq4ofahUvlx/4lzzmmvg7rvj113JdRxWd5S2xiXm/Qx8nuX+kejqCr5vP/zheHe4iK1j+nR73J2SJKzOKPeh0t+Dl7jfKw8i8rwxZnH5xjQHccawes6r9LMX3uGbq15kNDuvA0VpOjo7KsdtRh2/subWHFtWxRgzBLjLqiRLlKBoKAW5h70sKrn2ylk4XLzJASL2ZTl9+vgge+/1/eUrtdGl0tQXYcS9Xtz+BBF231asiBbP5qeSSy6s7rD6aiHKd6ISYc+sUAi/b37XoRuTtmOHta798z/X9swgnqs7ifugJMatD69RYaYoNXLo6Ai3Prwmkbqy5taMsqxK7USJnxGpPGlnJZdf1JeVN2g/CnHLg3UZBVkqli9P/nrVtM9L2H2LMnlu0LlRXK1hdUeNtYpKLbF6LmHPMky4Rulbrc8M4rnAk7gPSmLoxLOKkgxJ/S1lTZxJwLFx/89Vuy4dlNbCOr+3l2O2bi1b9nBvL7+tUHfv1Vez4LbbKHhiuUYmT2bN1Vezrcx1otRdLaHrffX10XvDDZy6ciWTt23jSG8v6669lm19faV50zJC2H0bbWujrUK2YdC9DXpOUetO+lnF/U4EPs+QZ3nqypWx7lvSfav09+Aljb8NJZyTezp5RwWaotRMUpM4Zy3m7ALgr40xlzn7NwMYY24JKl+3mLM4sUbFonXFbNxoLQTeoOYa4mqqJRdrmiUdc+bWuWyZtey48VaV6s5qzFncuhvVN7cNYX8PUdqqMWeh1Dvm7OYHXubQ0SqWdlMUBch3zFmUZVVqJ4m4KG9d69fboHr/vF/Vxoe1OmH37c47x46buM8tbGLegLrr+qzq+Z2IcN/q/j0s9/cQ0lajfxupEzZRLUDBmcDaP3lt1O04dUQt08g2NaqPabaplftYbR3+8n09nclO4myMydQP8ElsxuZbwLJyZc855xwTh8ceeyxW+WakFfpojPYzT8TtI/CcycBYlcRPnDFMvwv5QPuYD6rtY9TxK2sxZxi75Ekmlj1RFEVRFEVpNFlzayqKoiiKorQ0Ks4URVEURVEyhIozRVEURVGUDKHiTFEURVEUJUOoOFMURVEURckQKs4URVEURVEyhIozRVEURVGUDJGp5ZviIiLbgQorWo/jBGBHnZqTFVqhj6D9zBNx+zjHGHNivRrTSGKOYfpdyAfax3xQbR8jjV9NLc7iIiLPmZysyRdGK/QRtJ95ohX6mAStcJ+0j/lA+1g76tZUFEVRFEXJECrOFEVRFEVRMkSribMVaTegAbRCH0H7mSdaoY9J0Ar3SfuYD7SPNdJSMWeKoiiKoihZp9UsZ4qiKIqiKJmmJcSZiFwuImtEZK2I3JR2e5JCRGaJyGMi8rqIvCoiX3eOHy8i/yEibzq/p6Xd1loRkYKIvCAi/+bszxWRp50+/kREJqXdxloRkR4RuU9Efu880wty+ixvcL6vr4jIPSJyTB6fZ5LkbQzTsSs/3/VWGLfSGLNyL85EpAD8A/AJYCHweRFZmG6rEmMY+JYx5gzgfOArTt9uAn5tjJkP/NrZb3a+Drzu2f8ecLvTx/eBL6XSqmT5AfBLY8wHgA9j+5urZykifcDXgMXGmDOBAvA58vk8EyGnY5iOXfn5rud63EprzMq9OAPOA9YaY9YZY4aAe4ErUm5TIhhj3jXG/M7Z3of9o+jD9u9up9jdwJXptDAZROQU4L8DK519AT4O3OcUyUMfjwUuAe4CMMYMGWN2k7Nn6dAOdIpIO9AFvEvOnmfC5G4M07ErH9/1Fhq3Gj5mtYI46wM2efY3O8dyhYj0A4uAp4EZxph3wQ6CQG96LUuE/wt8Gxh19qcDu40xw85+Hp7pqcB24IeOC2SliHSTs2dpjHkHuA3YiB3g9gDPk7/nmSS5HsN07GrqZ5n7cSutMasVxJkEHMtViqqITAHuB75hjNmbdnuSREQ+BWwzxjzvPRxQtNmfaTvwB8D/M8YsAg7QxK6AMJzYkyuAucDJQDfWXeen2Z9nkuTx+w7o2OXQzM8y9+NWWmNWK4izzcAsz/4pwJaU2pI4ItKBHdyKxpgHnMNbRWSm8/lMYFta7UuAi4A/EZH1WHfOx7H/jfY4JmbIxzPdDGw2xjzt7N+HHfTy9CwBLgXeNsZsN8YcBR4ALiR/zzNJcjmG6dgFNP+zbIVxK5UxqxXE2bPAfCezYhI2kO+hlNuUCE78wl3A68aYv/N89BBwjbN9DfCvjW5bUhhjbjbGnGKM6cc+u0eNMQPAY8CnnWJN3UcAY8x7wCYRWeAc+iPgNXL0LB02AueLSJfz/XX7mavnmTC5G8N07MrHd71Fxq1UxqyWmIRWRD6J/Y+lAPyTMWZ5yk1KBBH5KPCfwMuUYhq+g43dWAXMxn6xPmOM2ZVKIxNERJYAf2GM+ZSInIr9b/R44AXgamPMkTTbVysicjY2cHgSsA74X9h/oHL1LEXk/wCfxWbsvQBci43XyNXzTJK8jWE6duXnu94K41YaY1ZLiDNFURRFUZRmoRXcmoqiKIqiKE2DijNFURRFUZQMoeJMURRFURQlQ6g4UxRFURRFyRAqzhRFURRFUTKEijOlIYjIfud3v4h8IeG6v+Pb/02S9SuK0tro+KU0GhVnSqPpB2INbiJSqFBk3OBmjLkwZpsURVGi0I+OX0oDUHGmNJrvAheLyIsicoOIFETkVhF5VkRWi8iXwU7aKCKPici/YCeqRER+JiLPi8irIrLUOfZdoNOpr+gcc//LFafuV0TkZRH5rKfuQRG5T0R+LyJFZ+ZnRVGUcuj4pTSE9spFFCVRbsKZKRvAGaT2GGPOFZHJwFMi8ohT9jzgTGPM287+/zbG7BKRTuBZEbnfGHOTiHzVGHN2wLWuAs4GPgyc4JzzhPPZIuCD2PXQnsKug/dk8t1VFCVH6PilNAS1nClp88fA/xSRF7FLt0wH5jufPeMZ2AC+JiIvAb/FLgQ9n/J8FLjHGDNijNkKPA6c66l7szFmFHgR665QFEWJg45fSl1Qy5mSNgL8uTHm4XEH7Vp0B3z7lwIXGGMOisggcEyEusPwroE2gv4tKIoSHx2/lLqgljOl0ewDpnr2HwauF5EOABE5XUS6A847DnjfGdg+AJzv+eyoe76PJ4DPOnEhJwKXAM8k0gtFUVoRHb+UhqBqW2k0q4Fhx7z/I+AHWJP875yg1u3AlQHn/RK4TkRWA2uwrgGXFcBqEfmdMWbAc/xB4ALgJcAA3zbGvOcMjoqiKHHR8UtpCGKMSbsNiqIoiqIoioO6NRVFURRFUTKEijNFURRFUZQMoeJMURRFURQlQ6g4UxRFURRFyRAqzhRFURRFUTKEijNFURRFUZQMoeJMURRFURQlQ6g4UxRFURRFyRD/HzRuklfF/DGCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimized_lstm_model.plot_convergence(filename=\"convergence.png\")\n",
    "optimized_lstm_model.plot_acquisition(filename=\"aquisition.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized Parameters:\n",
      "\tl1_dropout:\t0.410919405544983\n",
      "\tl2_dropout:\t0.2026549324920149\n",
      "\tl3_dropout:\t0.20132766526762616\n",
      "\tl4_dropout:\t0.20337507747060973\n",
      "\tl5_dropout:\t0.7990748026192123\n",
      "\tlearning_rate:\t0.0994845076611638\n",
      "\tunits:\t32\n",
      "\tbatch_size:\t32\n",
      "\tlook_back:\t30\n",
      "\tepochs:\t200\n",
      "\tlayers:\t3\n",
      "\n",
      "optimized loss: 0.0016024886248923308\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "Optimized Parameters:\n",
    "\\t{0}:\\t{1}\n",
    "\\t{2}:\\t{3}\n",
    "\\t{4}:\\t{5}\n",
    "\\t{6}:\\t{7}\n",
    "\\t{8}:\\t{9}\n",
    "\\t{10}:\\t{11}\n",
    "\\t{12}:\\t{13}\n",
    "\\t{14}:\\t{15}\n",
    "\\t{16}:\\t{17}\n",
    "\\t{18}:\\t{19}\n",
    "\\t{20}:\\t{21}\n",
    "\"\"\".format(parameter_bounds[0][\"name\"],optimized_lstm_model.x_opt[0],\n",
    "           parameter_bounds[1][\"name\"],optimized_lstm_model.x_opt[1],\n",
    "           parameter_bounds[2][\"name\"],optimized_lstm_model.x_opt[2],\n",
    "           parameter_bounds[3][\"name\"],optimized_lstm_model.x_opt[3],\n",
    "           parameter_bounds[4][\"name\"],optimized_lstm_model.x_opt[4],\n",
    "           parameter_bounds[5][\"name\"],optimized_lstm_model.x_opt[5],\n",
    "           parameter_bounds[6][\"name\"],int(optimized_lstm_model.x_opt[6]),\n",
    "           parameter_bounds[7][\"name\"],int(optimized_lstm_model.x_opt[7]),\n",
    "           parameter_bounds[8][\"name\"],int(optimized_lstm_model.x_opt[8]),\n",
    "           parameter_bounds[9][\"name\"],int(optimized_lstm_model.x_opt[9]),\n",
    "           parameter_bounds[10][\"name\"],int(optimized_lstm_model.x_opt[10])))\n",
    "          \n",
    "          \n",
    "    \n",
    "print(\"optimized loss: {0}\".format(optimized_lstm_model.fx_opt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build, Train and Predict Values with optimal LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Append Dropout for 1st hidden layer: 0.410919405544983 to list\n"
     ]
    }
   ],
   "source": [
    "#best parameters\n",
    "best_dropout_list = []\n",
    "optimized = True\n",
    "if optimized == True:\n",
    "    best_learning_rate = optimized_lstm_model.x_opt[5]\n",
    "    best_units = int(optimized_lstm_model.x_opt[6])\n",
    "    best_batch_size = int(optimized_lstm_model.x_opt[7])\n",
    "    best_look_back = int(optimized_lstm_model.x_opt[8])\n",
    "    best_epochs = int(optimized_lstm_model.x_opt[9])\n",
    "    best_layers = int(optimized_lstm_model.x_opt[10])\n",
    "\n",
    "if optimized == False:\n",
    "    best_learning_rate = 0.07495698010515113\n",
    "    best_units = 512\n",
    "    best_batch_size = 16\n",
    "    best_look_back = 10\n",
    "    best_epochs = 100\n",
    "    best_layers = 3\n",
    "\n",
    "\n",
    "best_look_ahead = 1\n",
    "best_features = 1\n",
    "\n",
    "for layer in range(0, best_layers-2):\n",
    "    print(\"Append Dropout for %sst hidden layer: %s to list\" % (layer+1, optimized_lstm_model.x_opt[layer]))\n",
    "    best_dropout_list.append(optimized_lstm_model.x_opt[layer])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.410919405544983] 0.0994845076611638 32 32 30 200 3\n"
     ]
    }
   ],
   "source": [
    "print(best_dropout_list, best_learning_rate, \n",
    "      int(best_units), int(best_batch_size), int(best_look_back), int(best_epochs), int(best_layers) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look_back: 30\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59970, 30, 17)\n",
      "y look_ahead: (59970, 1, 17)\n"
     ]
    }
   ],
   "source": [
    "X_key_list_stacked_tensor_dict, y_key_list_stacked_tensor_dict = create_Xy_key_list_stacked_tensor(tensor_dict, train_key,\n",
    "                                                                                                       best_look_back, \n",
    "                                                                                                       best_look_ahead)\n",
    "\n",
    "    \n",
    "X_scaled_key_list_stacked_tensor_dict, y_scaled_key_list_stacked_tensor_dict = create_Xy_key_list_scaled_stacked_tensor(X_key_list_stacked_tensor_dict,                                                                                                                         y_key_list_stacked_tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.410919405544983]\n"
     ]
    }
   ],
   "source": [
    "best_lstm_model = stackedLSTM(var_units=best_units, \n",
    "                                 var_epochs=best_epochs, \n",
    "                                 var_batch_size=best_batch_size, \n",
    "                                 var_look_back=best_look_back, \n",
    "                                 var_look_ahead=best_look_ahead, \n",
    "                                 var_layers=best_layers, \n",
    "                                 var_dropout_list=best_dropout_list,\n",
    "                                 var_learning_rate = best_learning_rate\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout l0: 0.410919405544983\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_179 (LSTM)              (None, 32)                4352      \n",
      "_________________________________________________________________\n",
      "dropout_179 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "repeat_vector_83 (RepeatVect (None, 1, 1)              0         \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = best_lstm_model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59980, 20, 1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_key_list_stacked_tensor_dict[train_key[0]][:,:,0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59970 samples, validate on 59970 samples\n",
      "Epoch 1/200\n",
      "59970/59970 [==============================] - 49s 809us/step - loss: 36299.2248 - val_loss: 0.0038\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00381, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 2/200\n",
      "59970/59970 [==============================] - 44s 740us/step - loss: 0.0211 - val_loss: 0.0030\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00381 to 0.00302, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 3/200\n",
      "59970/59970 [==============================] - 44s 741us/step - loss: 0.0191 - val_loss: 0.0029\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00302 to 0.00291, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 4/200\n",
      "59970/59970 [==============================] - 46s 764us/step - loss: 0.0182 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00291 to 0.00222, saving model to save_stacked_checkpoint.keras\n",
      "Epoch 5/200\n",
      "59970/59970 [==============================] - 46s 760us/step - loss: 0.0179 - val_loss: 0.0024\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00222\n",
      "Epoch 6/200\n",
      "59970/59970 [==============================] - 46s 760us/step - loss: 0.0173 - val_loss: 0.0024\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00222\n",
      "Epoch 7/200\n",
      "59970/59970 [==============================] - 46s 770us/step - loss: 0.0169 - val_loss: 0.0025\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00222\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.04974225535988808.\n",
      "Epoch 8/200\n",
      "59970/59970 [==============================] - 47s 777us/step - loss: 0.0167 - val_loss: 0.0024\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00222\n",
      "Epoch 9/200\n",
      "59970/59970 [==============================] - 47s 787us/step - loss: 0.0167 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00222\n",
      "Epoch 00009: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1da36b2bf60>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(X_scaled_key_list_stacked_tensor_dict[train_key[0]][:,:,0:1], \n",
    "               y_scaled_key_list_stacked_tensor_dict[train_key[0]][:,:,0:1],\n",
    "               epochs=best_epochs,\n",
    "               verbose=1, \n",
    "               validation_data=(X_scaled_key_list_stacked_tensor_dict[train_key[0]][:,:,0:1], \n",
    "                                y_scaled_key_list_stacked_tensor_dict[train_key[0]][:,:,0:1]),\n",
    "                  shuffle = False,\n",
    "                  callbacks = set_callbacks(),\n",
    "                  batch_size=best_batch_size\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59970/59970 [==============================] - 14s 233us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = lstm_model.predict(X_scaled_key_list_stacked_tensor_dict[train_key[0]][:,:,0:1], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values shape: (59970, 1, 1)\n",
      "actual values shape: (59970, 1, 1)\n",
      "predicted values shape: 59970\n",
      "actual values shape: 59970\n",
      "residuals shape: 59970\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzsfXm4HEXV/nt65i7ZF5JAWEJYAsi+hE0WQUEIICAuHygqnwqK4PKBfgH8KQiIIC6ACIiIyE4+FkFkE2VVAoQdAgkhBMhGQhLIcm/uMl2/P6qr19PTNT3T1Xcm/T7Pfe5MT3efqu6qU6fec+oUCSFQoECBAgVaC1beBShQoECBAo1HodwLFChQoAVRKPcCBQoUaEEUyr1AgQIFWhCFci9QoECBFkSh3AsUKFCgBVEo9wIFChRoQRTKvUCBAgVaEIVyL1CgQIEWRDkvwWPGjBETJ07MS3yBAgUKNCWee+65D4QQY5POy025T5w4ETNmzMhLfIECBQo0JYjoHZ3zClqmQIECBVoQhXIvUKBAgRZEodwLFChQoAVRKPcCBQoUaEEUyr1AgQIFWhCJyp2IriWiJUT0aszvRESXEdEcInqZiHZtfDELFChQoEAt0LHcrwNwaJXfpwCY5PydBODK+otVoECBAgXqQaJyF0I8DmB5lVOOAnC9kJgOYCQRjW9UATnYwka/3a/Kl6WoAJZ1LcPrS183LhcA7n7jbqztX+vKXtmz0ojcZV3LMOuDWbCFbUSeH6t6VhmXCQAVu4LeSq9xuat7V+OGl26AEAKre1cblf3w3Icxff50t133VfqMyBVC4NF5j6Knv8eIPD/e+fAdrOpZhe6+bleftBIasYhpIwDv+b7Pd44tCp9IRCdBWveYMGFCKmG9lV50nN8ROX7y5JNxxeFXpLqnLsZcPIY9XvlpBRZl5754ftHzOPq2oyPHv7/n93HJoZdkJheIr7M4O9vB7aG3HsIhNx4SOf7uD97FJiM2yVR2+Ty+W2Rd5+/f/31c++K1+Opfvxo4vvrM1RjSPiRT2QffcDB7POs6T/7jZDy/6PnIcfunNogoM7lCCEy8dCL/W8Z1NoVGaCTuDbBPRwhxtRBishBi8tixiatnWSxZs4Q9fuWM/NigrK2sssUrm0ufvjRTuXniqhlXsccnXJLOKGgGfNTzEXv89IdON1wSc5j34Tz2+LTXpmUqtyIqmd5/IKARyn0+AL8ptTGAhQ24L4vOcid7fKNhG2Ul0sVhkw5jj3f1dWUqN44COnX3UzOVCwCHTzo8cxkcTtrtJPb41H2mGi6JORy19VHs8QMmHmC2IAZx/oHns8fHDkln/Okirk9tO3bbTOWaRCOU+z0AvupEzewF4CMhRISSaTR+N+V3EGcLiLMFNh+1uZEOMKpzFDYftbkr98rD5Wwha/5dOBOhO794pyt7SNsQdJSj9FSj0VHuwHZjt3PlnrXvWShRKXO5Ck994ymIswXWnLUGgHwHWWPr9bbGF7f7olvnkyefjDGDeXqqkVDvec5350CcLfD8SZKuuOuNuzKX3VnuxP9+/H/dOh+19VHYaf2dMper6rz49MUQZws8fsLjAGDMx3P+gee7dd5+3PbYZsw2RuSaQCLnTkS3ADgAwBgimg/gbABtACCEuArAfQAOAzAHQBeA/86qsI7MaBlBbiPJVDYEyMdCqc9Zy1Z1DnOQJpy6QoiA3LZSGyqiEjmehVw/lE8jj/dcopIRZRN+z6MHjQYAHLw5z4c3ErawI+/ZhFM5XOeSVXLLk6lcph1ZZOUSOJAVEpW7EOK4hN8FgFMaVqIEqJcSULJEuSg69dmU5R6pcw6Krs1qAwD02X1oL7VnKhcI1hnIZ0AzjXCds3TWKwjBGy6Zyw29Z1XXip0tJ84ZTK2m3JtuhSr3UtZFy51gZkALy3Ut6KwHtFCdTSkbIPqe/eXJWq4fJgcYgeiAZqRPhd6zal+mLHf/e7bIynxQMYnmU+4xVqwR2TlZdHnXOQ/EWu6GFA43QzMhl5NnbLaSR/sKvWdjMwbmWZui30yh+ZR7jh0A4BWsaSvWPZ7DbMWU7IjlbtqKzWNQyUnRKdm5UlGGZcdZ7oVyzxGsFWuQlvHDGC0TV+cc/QyZy42x3I3IDtc5R2sSMDdbCXP7ptoXezwHqrNQ7jmD5Z9b3aEaU2cTiLPcM5ebJ0WRV53DlnuOs5W8HKp5Gg+Fcs8ZeVvuuThUBxD/7D+eqdwcKQogP947D9lxck1gINXZIqulVq42n3LP04odSJa7KVomL4suZ4rCD9NKz7hzMU/jIS+Hao5BCqbQfMo9z/jnATJ1BfKPEDI9W8nboWpKbi3HGyY3Z4PJpLy85ZpE8yn3POPc8wqFdOrc1U2+Y2YsK9sWsCsDp9Mbo0dE/jHfpvnnZcvMP2tb5DNrUPfv78sn6s4Emk+5Oy/lG18nEAFEwGuvElYZSP09Z47Aq694ck84QR63M24QPb3y/scc7clesZww753sG+KzMwSef86TO9VQ3q5Vq2XddtpRym4ry05YMUCJLlwk8Je/eHW+9BJCxUC67zlvyTpvtKGUvZGTC69im7HcL7vEcut86y2E5dV2cWgQnn9Byh4+XNZ58mSzxsPUqd57fvwxwqxZRsQbQfMpd1eRBhvB9OnZK7p33wtbdPLz4sXZyvUUmk+2ILz1VrZyAaCrSyD8rAFp0WeJpUv59/zsjEzFAgAqlajlvqYr+/a1alWozk4ZXnoxW7lehEiwzsuWZSsX8NU59Lznz89WrjszCMnNui+bRPMpd+el7LkH4Re/AI529rDo5DMBNxRjx0lF9+ijwAsv+MqUcb9XM4PttgMuvRS44QZ5vD271C4uRo2Wiu5UlV3Y6Qx2xhFjgoKd74gj5Ne+vuyVbHu7fM8bbghsvz3ADW5ZoK3dq/PkycAhzl4l3WvNUBQQhGOPBX76U++XrNHZKWX87W+E//wHbjt7/30zsxWAcMcdwH33yW9DhhS0TG5QL2X99QlnnAHcdRfQ2UkoN2JPqWTpIBA+8Qlg552Bb35THq1UzDTEffchfO97wPHHy2mkKSdyRwfhd7+Tg9ghzm66mSt3p26XXkoQApg2zcygAgAggfXHERYsAF55Bdh5FwMy4Q3i995LePZZ4Oc/d6goQ8962+0It9wC/OxnwIQJhDYDxoMaWLbeirD33sAeuzvvOeOmreTuuw/hmGOAKVOAESMI5bZs5ZpE8yl3FUWR01J8vxVnkZnOpzjXYJ0p8xkDACbnSLBMWcqV8qTAkpNCPutO70hHxFonEw/bEaWSaDm9M5eso/4CZSk7nDjMqXPWfiw+MKNwqOYKwXrXTTphfI3BVOeLiRzJugM40sENaNlb7kF5phQdICmhXNIPuIZLSG7WVmyOfcp2KmdZFPifdXJG3khsrbDIJlTu8n8kTMyIYRV0tLkrVLPm3B1z1Qo3RFN1Zh2q2cp16+y0UFV3M4YVF+duLixQ1VW9bjtj2Z7lbl4diJg6m5IbeM+tpdubT7m7cbER3W7GiuUagymHangKaQY8LZP1rCFcZ3KNWPOzFdOrcr3ZihnLXUXLBAwmMiAYvjpbwWdsbJFcZDQpaJncwC16MDttjipYY/xgbsvDo4uYsh7QohadGUWnMBCSpblVzqF9kSBDwyj/nk21r2iOqtZB8yl31rkImLMyzCu68HTdkZ5rQ8w6zt2OsaxMKhwFIhhxqKo6l6yQostYLmvFGqZHIrSMqUG8sNwHDvjlygatDD/nbqgDcLSMU6DMEc6zYhmqdLjTh49nLD0XR5tw/Qw5We55GEzg65z5bDjkvAbyma1liaZT7nGd3pB0tgGY6nymeUlPNldnE3LzSuyUk0M1ZEEbG0jZZHymBnH53zJd5ziqs3UM9+ZT7hwtYyo+VTUHT64ZWiYuzt0M+AHNlEM1YrmbGNCIH9CyFyz/mZ6tuMaDTx0QYISK8gbxzEUF5cY5VE2sZzCE5lPurEVnjCAMfDU2hXQ738BxqJqKvQ4oOmEm/DMaIWTYmnTEqZla1lVmo2WMhtr6ZqWG+1QkMKN1dHsTK/ecFB2/QXa2ctkBLS9Fp47mUWeY6ntcamdztIzrXFSSc1uhmj0iA5op4yE2FLJ10HzKnaVlcgqFNOxQzcPPEFnEZCjePHeHap5x7qHVmplHy3grA4M/mKBlnCAFd1Wu6dlwKELIzDoKM2g65Z6nQzVutWbmlhU3oBlriDH7xubhUBXmBvF1Ks49V4eqVO7hlciZy2Us93UyWoaIDiWiWUQ0h4jOYH6fQESPENELRPQyER3W+KJKsM5FMmTRhZNomUo/EKPojFQ5znI3Fdtv5dDhSASsWNPOxUjkSB7+DZiNxjLuUI2hd1uJdE9U7kRUAvB7AFMAbAvgOCLaNnTa/wMwTQixC4BjAVzR6IIqxC3oMYG4OHdTVmwpoujysNwlsl7EFF656B7PgZYx2r7AxLmbWoqfR26ZEC2jYC7Ofd223PcAMEcIMVcI0QvgVgBHhc4RAIY7n0cAWNi4IgYR51A1A55zz7oh5hkKGZt+IGu5eUZFRRzn8ljmUiOhkGbqy0XLmI0Q8pS7qXw68Sk9Wgc6W1xsBOA93/f5APYMnXMOgIeI6LsAhgA4qCGlY8DyzzAZLeOPBTbMP+ey4CKk3ENlygpqZhCerZjaoCRPyz3sXMwjt4wqUdaIc6hmbjzERsu0jnrXsdy5lh1+AscBuE4IsTGAwwDcQESRexPRSUQ0g4hmLF26tPbSIt849/homYwVXawTOT9Fl5ufIVuxHnLIwBmXFTIfRZeP5W4s/DPXmaEZ6Cj3+QA28X3fGFHa5RsApgGAEOIpAJ0AxoRvJIS4WggxWQgxeezYsakKnKeiC1uxphRdXEM0FfPNDWiZ1zmGczdSaRLRbm7SoZpXbpk84twjlrth/0aoba9roZDPAphERJsRUTukw/Se0DnvAvgUABDRxyCVezrTPAH89ljmIke43ObZKzoJKw9rMmy5G4tDlv/ziOCIcu6muO9QnLup9Lc5UhSRaBlD7YvPLruOWe5CiH4ApwJ4EMDrkFExrxHRuUR0pHPa6QBOJKKXANwC4ASRkbnhrrfILQ7ZvEPV9iodLE8OtIxlfLbiP2puSIvG5pkPhTQWLePKjfqTskbYcjcV/skxAK2l2vUcqhBC3AfgvtCxn/o+zwSwT2OLxiNuJ6Y8aJm8Nq5wpBvy/fBx7plLjUs/YGKKRjmtUA1RUeYUHZdbBgapKPMOVT4CzYRkc2i6Fapx6QfMqHY+5tuYQzWHlL8Ar9xMrZoM1Nng7kD5zAzl/7zi3IOJ6fKx3E1HY+URFWUKTafc899YgFN02cqN3SDbAKK7EpmNlhkQefsJZqzYSCikyYgVc/KisnNwqHJ1JlPhxWbQdMpddfpSXotbAitU84yWMYWcVqjmnTgst7BAhnPPJbeMOZ8OG+eeQ6gtAUYGcVNoPuUew5WZmq7n6VDNS9GxoZBZS80zDplEtNMbQCQrZI6rgc2GJEbTD2ROy8S1r9bR7U2o3NmGCJhS72yIXA6Wu0knXx7ORTe3eU6Jw6Lhn3nQMs7xnCx3ExACwWgZUxuUKKozPCstLPf8kOeIG7HcsxcJIH/+mUvZ19q0THhBj0FaJgdFpxAdSE3NDKMDWuZSVXRxeGFg6+j25lPueW45B4DV6Ma2BMtB0eW1+1Req3Lz3LjCjuWfzbQvP8xuUJJDNFZsSHXroOmUO+8IyYmiyDVyxFz6gVxW5cakOW5ly12+UGZBjyHkEeduO7MVBVOrcrnssuboXTNoPuXOhgUaQqghmrascpk2Uz6WFTugGdiJyeOffTCm23lr0lg+94CbwaxD1ZVL/uPZgV870lpmfNMp91iKIoel+MYs95zj3PPIsxI3oOUSOQKYW63p33Iu59zmJhDdAMdMn6qwi5gM0rsG0HTKPU9aJhIW6DuaJWzwCscULZPHwq3YZFa5RI6YpArMc+4KuSzFj8nXlLnYmJXurYSmVe7hXctNRcvw/HM+kSMmXPsitKAnz9h+E52Pc6iai+AIOVTVoJK1XJaKyoeWcY/nEufeWmg65R5LyxiJTw03xNaOHHGks85FY8nSDOfTydNyj1vQY4opyMOnowY0twyGFm7FGokFLZMf8o5z56zYrJH3BiXcbMXUBsaRCKEc+GeXfjMhm9ks2lTKXz/ydqi28sJAU2g65e6GyEWWh7ewosspcgTgOp8ph6r8bzoqKn7jClPcdzSJlqmFNXlEoEUdqs7xXLJCtpLd3oTKnc/DnK+iMxbnbgU7QR60jHvUmGUVLo2hxS25LtyK/JKtXHX/8LM2FCGU565X0RxCraPem065u+0wp8gRdrqedUxuXGx/LhtXKNGmYq9Ds5W8onQMIJrb3JBDlaWi8nWo2ln3qZiFga2EplPuee7EFEtR5KJwzG1ckcdshXWoGuh71dIrm6HfzGdIVMhlM/IQLWNq9ymW6iwcqvkidss5M9JzpSjySRwWUnQuJ5qtzNjEYTlEyxgb0MKWu2GKIhIgZHCbPVesofbF0buFQzVnxKX8NbJClfiGmL1FF5Tn+yVTuZwMU1QUGy0j8o6WaU3LnXNeU24Oe//xDOXGrnRvHTSdcs97s+iBks+91ePcc983lpkZGrfcDXHunhUbLVHWCEfLmJqd8ovkZIlaBU2n3PNdWRYTk5sx2GgZwIhDNbJZtGFapmQ4sRO3WtNcJkwp2bzlzs2GzVnueVCd7I5uLbZatWWUu5GwrZiGmNuCnkylKjEiuBTf+W9ss47wqslcZklKtKkIIcSWIRO5LuWeg3JzBjQF0yk9csmnYwhNp9w5WoYM0jL+hmgq93R8yl8TyCkrZEjRASrkNQeHqmFaxrKY4xkiNgLNkMEUEGk6/UAO4Z+m0HTKnUuyL5Gf5W5qNV0e6QdyTZYWWYpvgJbhLDqj1mSUc89+tuLIyyFyJC5aJus6F4nDBiBiLXdDoBwsd3ez6FCdjTlUc3Auhrec80rTwpZ7TrllvKX4kRJlKldKyDdaJuxQLfK55wg2isIUKwMRjQVG9rJj+cFcIoTMiGbDAk2sUI1dlm6IlvFb7oYoirwWjDnSA2GX5iLQQvLQela8lnInokOJaBYRzSGiM2LO+SIRzSSi14jo5sYW08NAoijc43nlczeh3WPSDxhxqIatWAODOJvy16Q1yVnumec2l/8jqR4MIHYRU15+hhZCOekEIioB+D2AgwHMB/AsEd0jhJjpO2cSgDMB7COEWEFE47IqcL6pOoOKzlRMLu9QbfH0A2Asd+QUFmiIlnGlGVY4HC0jy2CeljFGdeZoJJqCjuW+B4A5Qoi5QoheALcCOCp0zokAfi+EWAEAQogljS2mh7jc5sZomRyWSsc7f8yo9zxy2LNWrNENss2bdLEbZBvKLZNbplU25W/Gcot87gCAjQC85/s+3znmx1YAtiKifxPRdCI6lLsREZ1ERDOIaMbSpUtTFZhNomXQyhgoce7GGiLlREXFWO5Zo2risDyoKAMDmh27QtUEeJ9O1v05bicmMzu6mYGOcudeefgJlAFMAnAAgOMAXENEIyMXCXG1EGKyEGLy2LFjay0rgPz21pQQgR5gOoNdKbwUPxeHamtHjnDGQ+5UlCn+OexbMRbnzlBgGcvl87kbi8wwAh3lPh/AJr7vGwNYyJxztxCiTwjxNoBZkMq+4ch1ZVnYuWjc+ZMPLYNwpzckOazoTHY+NlrGkEM1eNBEhJD8n0tK6ZBD1VQDqzZDaxXoKPdnAUwios2IqB3AsQDuCZ3zVwAHAgARjYGkaeY2sqAKnHPR5MYCwSgKg1Ys8nOo5regh+GfDcU/B8Watdz54xnKdduXd8yUQzXOj5XPZh0wMlsxhUTlLoToB3AqgAcBvA5gmhDiNSI6l4iOdE57EMAyIpoJ4BEAPxJCLMuiwLyVYY6iYK1YQw7VXDLYRUIhDSq6yFJ8gw7VPJzIIeeiI91A+1KS8nGoUo6bdbQyLZMYCgkAQoj7ANwXOvZT32cB4DTnL1PwlhUMjbg5LsVHfluC8VkhTVhWXChkpmLZfO7BMmUqHHlY7nk6VGNXqBppX8xOTOuS5T7QEOtQNfBOonlWzNjPcft6Ggv/9Im1TFnu66RDFYgodxPhnzn6dOLi3DOXmyO9awpNp9zjlkobyQkRTn9rMuYbXFZI83HunuWetVTOcjeo6HJciRw5njnnLv/nEvMtYkJtMxbLbbPnFKdl0HTKPTaEyQjyTT9QClsZJhpiThFCcZZ71pVmLXfDfoYgsn/PeS7Fz4uWYROHFbRMvlCdLxzzbUp2Llkhc16Vm5tDlQmFNBaZlNsMLYdoGed/xHAxFefud6hahuPcc3Aim0LTKXd2eywAuVAUqkyZWxnyfzRrn5nOlwstE2u5ZywXXKeXMLESOSrXQJy7yrPiD4XMaTZsKp97fDK+1kHTKfd887mHLHelbI1YsSEL0lDWvriskHmlHzAXOWJ+tgKWlsm+ziz/bMp4EK4wpwzO8RwWBhoLLzaEplPu7GpNU+GpxMe5Z424aAYzGwsMpPQDJvnn6Gwla7C0jMieilLIy48VKAMMGUws/dZaVnzTKfc8c5vnljgsz9kK8VkhzYQkcpZ7tvDS3+ZBy0jJ0eOGrNhMpfCI3SMha7m5pvQwg+ZT7siTlgFvxWYsk6szyIBgR3pwtpJvnHvWlc49MV0IJmS7Pp0cluLHplwwtYgpFIHWOqq9CZV77km0/PrVkPOH3VoQMJSedADFuRtJfyv/c1N0YSLlL6vMDcXXG6bAHOkBwaa3FsyDfjOFplPu7MoyMrhZNOP8MUXLDIg491CZsoIKkaPQYGoscVhArrnwT8ohtwxnuZuK+Y6jZUzN0EoFLTNwkGsIU2RBj2HnTw6x/bEO1aylcrllDKS/ZXObm3KoxlEUxiJH/EdNUlF50H6OvHV8J6YBhTxpmXDMt1nnYlTJmEq5kEeGRBUWaFmRo5kifh2FgcRhnHI3uBNTNOto9oi13I1FRRkWbBBNp9zzdajmY2XYcWGBJhCzzV7WW86xWSENxF5XS29hZnWseediXBptUwZTLqtyi1DIgQeWljG04CKOc88jcsTkggsuQ6IpGHeoVqFlTKQB4Dj3rMFvjGIsuD8gy+g6CrSeQvej6ZR7riFMxKcfyCfmO/s68xsaqN8MyOZS/hpyXoe3nJO/ZSo6PitkDpa7UVrG1GprH2I3wCkSh+UHNs+KOels+gEjVCwX820kHJG3Yk3kWQkPaCYGcTZyJGOZPumMNJOzlbBoQ+HFedIyVnQQbxU0oXKPNkRTFEVks47MJXpygfycP2yEUMZgN4s2gGq+hKz9DHnzz3nQb9E+ZdCPhXCds5VpGk2r3PMacVlaxghVQJGsfeZomehvpix3/nh2YKfrJuPcQ3U2skLV+Z/XZjC5DGjcqm/1S4swM02n3ONGXGM7Mfm/GnP+IErLGHAic7SMqQ2MVShkENnHubP8s8k4dy4rZNZL8Zk9VHOz3HNcO2KqP5tC0yl3fsQ1R5Cwce6arWF593JU7EoKqUzn06xzxa7gw7Uf1iwTqB5RYGaRifl5Mr8UXyKXDbKF3gztyXefxN9m/S29WAwkP0P2UGkm8sinYwrlvAtQK/gMiXo47KbDcP+c+wEAJSrhnuPuwWGTDqtJPrsTU8I1QghY5wbH0U9s+gk8/NWHUbaSXwG7WlMT5fO8+28yfBPM/u5svPfRe9hi9BawqPrYXo9Dta/Sh/bz2wEAP9n/J9hu7Ha47JnLcN+X7sOIzhGJ5WZnYpoztHEXj8PSrqUYN2QcnvrGUzj9odOx9Xpb49Q9TsXGwzeueq1LyzA8cBL6Kn345PWfxDMLnsF5B56HKVtOwcbDN8bQ9qFoK7UlXh/j1oSOGbvfn/dzP9/42Rux7dhtcfF/LsYNn70BJatU9VqbG9A0Z4Z3vX4Xjpl2DABgq/W2wqWHXoptxmyD9Qath2EdwxKvj80KqTGQ0s+C1z3530+iq68LB29xsJZcgDeYkkQfdetRuGfWPQCAy6dcjv99+H9xyBaH4ObP3YzOcmeibFNoXuWeYlcipdgBoCIqOPzmwwEAn9/28/i/L/yfjnTOk5uIxasXR4499s5jaDuvDWvOWoPBbYOTpPJhgTXOXd9b+R4G/XyQd/3Z1a+vx9H27MJn3c/nPX6e+3nkRSPRdVYXBrUN4i4LIcQ/azpYl3YtBQAsWbMEW1y2hXv8on9fpFFn+Z+LxkpSOD/6x4/w5LtPAgCmPjwVUx+e6l2bINc5i2lSteeWOf6u493Pryx5Ba+c/Ep1qXUMaEqxA8DsZbMx5aYp3n016hzZQ7WO8Ih9/7wvAODkySfjisOvqC6Xod/Cv8VBKXYAOPX+UwEAd71xFwb9fJDmezaDpqNlbDAKR7MhHjjxQPb47TNv1xMes1ozqdPPXDoz9rcFKxckio1dZJJDKKRXpurXzlk+J/a3PrsvWXZczHfGla4wETG6XOyi1Yvqkh3nRK7nRY8dPDZZbqyiS5Z71r5npShVEI1OP6Bj9fMJ4qTgpMu3Xm/r9IUziKZT7vUkDntk3iMAUDMV40eaDbLHDxsPAPj6zl/Ht3f7Nu770n3e/TTqEbdxhQ7GDB4DADh2+2MDx0/f+/RkuVXCxZI60KYjNgUA/PEzf4y9b1XZDXCobjRso8D30/Y6LVlu7AroZBy/w/HJJyXKjtZZp8rDO4bjO5O/g3985R+B4+H3Hi83nUP1F0/+AoBee4qRzj5fnUG8bJVx4q4n4uojrg4c16JlWD2i158/v+3nYZGFPTfaM1FOnmgZ5a7TGHYdvysA4O9f+jvE2QLibIFdx++KwycdriecBDPSJzcGVeZDtjwEVx5xJaZMmoIbP3tj4LfqNwDCSbQI0MrnvudGe2K38bvhls/d4tZ5UHlQIt8uxXKcu14HsIX0WE0aPcmV+9tDfhv4LQn1hAGO6hyF+afNd2WXqKRFBVWbrifFuVeEdJY/f9Lzrtyp+0xFe6lds9Tp099aZKFklXDQ5gdBnC2w8LSFsswaz7qehYFf3+XrAIALD7rQrfMJO5+ACSMmaF0fGy2jgYpdwQZDN8CJu50IcbZf3N4NAAAgAElEQVRw6SedoAX1RCOrcinZcreFjRKVMP2b0906HzDxAOw3Yb/qFxqGlnInokOJaBYRzSGiM6qc93kiEkQ0uXFFDIINYdJUApNGT4pMqSyyaprq87llEnhcN8LHe9yqUel0vtiUrBrFtoUdUeQWWZqdnqPA1G/JcpUsv1xAbyCuh5bZZsw2OGjzgwLHiKimZ+1vUpZm+1L39zswLbK0o6m42Qppcu5CCP5Za1zMtS/dfO5KiQdkw6ohKoynonTal4AIBCSoMtTWtr1jugNLXJ8ys6exPhKVOxGVAPwewBQA2wI4joi2Zc4bBuB7AJ5udCH9cBVlCocq91IIep3eu4CxYjXkAiHl7jqOdBtEui3n4hqiTuervoqvuuxqyl3Psop2et1BPKzolOyarFhGVlKEUFydlUWfKLuOBT3h95zGeOAyreoO4v53U7JK2n0qfg/V6oL77X4ACCj3EpUCZaoGrs6ubB3LPRSBVLMeMQAdy30PAHOEEHOFEL0AbgVwFHPeeQB+CWBtA8sXAZ97us4RtxYHjO9YPYrOozdSOn80uViuzrqdj8ttXmunT2u5y1Mao+jknaimZw3Goku6XA1aftkl0ld0kn6OxrnrIFzn2mZJ8n+a7RRd5e67tpYBLWy5u/VPkMsp91osd3X/NCtU69EjJqGj3DcC8J7v+3znmAsi2gXAJkKIextYNhas5a4JttNrTtdZ/tmQouO2nNOlo+uhZbj0t+pjGitW3aeW5x1EfVSU3sCQ3qHq0jIUpGWAGpzInA9Zi1rhlXtNFEVQLHQUHTdLqmVAq9dy5561VttW/dnvx6rBn5RWj5iEjnKv6uEhIgvAbwEkusuJ6CQimkFEM5YuXapfSh9GlDYE3tsbZf+0qE5apibOnY0cSZYL1KHoHCu2kbRMWstdN0Ko6oCmqejSOlRtYUf401ppmTQ7MVWlorQsWSZCqAbLnTM80ta5FsMlLe3nSEfActfkvdX9U3PucStUkb7OTce5Q1rqm/i+bwxgoe/7MADbA3iUiOYB2AvAPZxTVQhxtRBishBi8tixyfG3HD4+5HjgT/8JrASrhZbhOn1aasSbumZMy1RZTZeERljuAf5Zb9ZcVdFpT5vDio30ZzppLSsu/W2tDtUALWPp88D1bBYthMDq1SkH0ri2rRk5Ei5zQzj3OmgZnYGU71PQrnPdvjsD0FHuzwKYRESbEVE7gGMBuEu0hBAfCSHGCCEmCiEmApgO4EghxIwsCuxZGd4x3Zfy9tsCLz5vgQju36xZhP6Kfr6XNOkHKk4ii8MP82R/4fP6DtXYjSs0yjvnLRuPPRqs85LFFpZ/qOFQZTl3t1BVr+3rl3WevJsn+6ST6qNlSHOD7A+W2bj+umCdV6+0sGKFxrOusodqkqLsc6zJLbeQsidNAn58Vg3WZIwTWec99/bZuOJyr84jR0i5i5foO87TOFSXLRfo6w0+60svsdz3n4x0TmSl3E/9TtmVO3FTWee1PfpUVJpt9t6bb2P5B8E6338fYdWqJrPchRD9AE4F8CCA1wFME0K8RkTnEtGRWRcwWh75P2hl6L2UlatsQASr/MESC6/N1Leeg442LbFSLhCU7Xx+9z3dThDdLFpH0634MFpniBLuuz8d5667PHzR4vg6r1qjX+fwV53u09XN1dnCPx/RUbCOKC4qKkH4W2+pOktrfc4cuOXQs+rSR8uAbPZZP/5EtpTjO+/yz7qrW5+WSePHUsodti+DivPcn5peD/2WbCTOe4ev87x3ms9yhxDiPiHEVkKILYQQP3eO/VQIcQ9z7gFZWe3y/vI/p1iTXkqpbAOwcOGFwK67AltuCUBY6OuvJVqmdouu27EkvvENwmOPAU89BQwdKu/TvVbfyRehZTT67eAhNjraLdx+O3DjjcA22wAQFsZvmJKL1ez0K1fL+//61xa+/GXgyCPhdoiurhqiVqK/JF7b0WFj8BDCQw/5jxLGjKnFzxC4VAurnUHrlxdZmDoVOO44uHXWDf+Mti/NEc2yscP2Fp57Djj8cE/umLFp11FAazasBpXrrgOOVwt07ZI8roH4lAvVoZT7/vuV8NJLwN//DgwZLOvc0Zkyzl1zYCm3yTrfcw9w2mnA6afLAI9Sqcks94GGWFpGJ4QJkiubOhV47jngzTeBEcMt7YYo5dbuXOztlSeMHGFh//2BvfYCTj1FXlupaDYIjpbRWGQiYKNkWfjc54Avfxl4/XWgXLLQX6lhQY8fmn4GRUUNGWzhxhuBu+8GTviabG46U3ZW0Wk6FwVstJUtHHywfDdCABA1LlZLs0LVqfP220kD4uabgUM+La1JneedNvzTVVQWYdddgXvvBd5f7MwYbP1BPM0GJRVhA4Jw0EHADTfI83fa0QKsdJa7/3h1ufL+QweXseOOwGGHAQ89WHu0THS9jE6dK4CwcMQRwK9/DfzqV8DYMc3pUB1Q4JW7vmefEHaEWBCoJRQycLEjt7rgnl55/442n6OtJD/31zJr4IQnXQu+zraG06meaBmlzMol79qS05G0FJ1T0sC3WhyqkTprOlTddRTeMcs1H6qj37HOSz7+TH3WHdCiSH7P6rqSz8nXVlbORX0rlvGbJ4dC2gIQFkq+4DWLarPc06S3cEMh/Q5VS82S6ohz13SoQlghHWRBUO17NWSJllDu8oBOTK4NElFFBx3lzgjWdagq5d7e7u/0+oouNnGYhqHADmiiBFujztVivpOUrOpgvKKrwccRgP6AFsmdIyytzTYqzKpc7Th3p85tZU/TqTr39OmFQqah/bgoHVe5a1nuVd6zluUeVu41WO4k2FlSmjj3sqUsdw0KrA5aRj5vTo8UlntdYC133dhr8C9Fx3L3zmciRxKglHsno9y5FLMRCERoGflZj5bhLfdaFrfU7vBSdFO55MlWn7UGNBHDxWo9Lka5a9Y51qKDPi3T5q+zEwqpb7lHo2WSwCn3crmWyCQ419f+nm2bU+4l51rdWWkKuSJqPLQ5hdCZrbiGS2CbPUCngSnL3Q8Z596EDtWBBJcfZEquZbnXS8uwjtzqgvv6HMs9QMsoy13fig3TMjp2gkA0tl9Xubu0TKQDILEPKEXnV+6qI+mGyaVZuQgAgrg615g4zH+tZlhUv6pzOR0tk5Z/5hSdosNqsdy5SDC9WUMwkqtUy3qGmD0SkqDaZmBmWKplVa78nyblrx3DABS0TJ1QbZVbraljuUeUO1kQGvwgZ8UqZaXNPzOdvlKDFZtmhWo9lruaVQR36JFIojg45a4+6ziROaWirwREgH+Wh/RomeoJpZLqLDt3G1PnXg1aho0c0dhD1bPcff6NEgFCN7WGRPD56io6jnOvcVVuigGt36X9vGtdWkZnQItbxAQ9IzGsOi2UBpzl3oTb7Mn/aWiZeix373yu01e/huOfa6IoAETzuevyz5xzUdOhyvDP2g6vSrxy11/gktJyhw2KrCC0nE6ZeLE83+9b0aWi3AHNxwM7n3vTRghpUAVeGmz/CmgAwqrJoRql/TTpkbByr2NVrjbF6jq+o5a7Xp2dsoa1u+6q3LDlTnq+O5NoOss9PhRSj3Ovl5bx9z1luSc5J1Vj81uTqtP3aayO5aNl9MAruhIEaoiWCXQ+Pbku/1xOO6Cld6gCdtRyr5GWSRPnztZZ0TJ9KUMhRXI+d5tpX/JavQGtnnw6FYZzV+XQzS/D02/VoQwEzmBKnc+9Jr4/mPLXgh4DYBItodzlGmAdhRHNg6EbLeOJ4sIC9SgKv/NGOX90Uh/Eb7OXlpYpwdZR7lzkiLpvClqmFs49Ns+KpkM1wrnXSMukUnTObMhfZ6V8evv1850EoBH+6dIyYUdUXZa75mwYMs49oNxrsNwBEcPKJNAyjoFQ8oXali39QcWjZWqPELIRtdylQ7Xg3OtCbJy7Viik4C33mjj3kFz5q9a1QSvDUe461o0TLeMHkV42Sy5yxNJU7txOTF7yr9qVe5truetHCEUPa1xLIvCsAX0/A0vLaE5XVJ3b26K0TJ9WbH90QCONhVucQ1XeTm9AY/OsqHsnRAgpzp1zqOom8Arma7Lc49Wg/DbB8M8a6CA3MCMUqaOhC7jw4lpi+02hdZS7hkOVs2KtmqNlfE4rqzZFx4VtadEyTEPXjasVsBnOvcbNOhBVdEnXs5Ej7sItzecdoZ9rqDNDy+hQFFxWyPBvsddyoZDKz6BBy8Qvxde03CNLmC1N56JEGt+KCoX0K3cVCqm1mChUZy93kWb7qjNCKJUeKUIhswGv3C09y53p9FTjS+E7QALnrpQ7Gzmia2VEQ/v0qah0U0g2W2CNVFRQ0anwT01Fx+0nqmMdEZeSVdeKlf/TxHwrCoRzIuvQMpzlrhPyqqKa/O1L3q7WvXL9UtVv1a/lYr6VEaO3Ejmd5a7ubflpmbKMENJS7ohp2zoOVVTYPlVY7nUiNlqG7FQOVavGFar+rueGQiZNIRkro60GWoa/v24ui6hDVZeW4fhn1XETeWCGlvE6vU65o462Wiz3CEUB0puhMTHf+g5Vh3MvR30raRcxOYWqep3rXGQHtDoiR6C7QUmwzLX4GcJx7toGk1okF4hzRw0DmvwfHcR1c9gzBlMR514fOOVuOZ0+TSikpRvnztEymvwzu4KwBocqYqxYXcud49xripZhrNikDpRJtIzQU9BgFzHVuEFJWieyXQo4F8s1UFEs565huXORI/J2NW6nGOlTGpw7on4s5VCtJbWGgu7G8a7lzih3LYdqnBNZkwFgOfeClqkP9VjubORIrbSMfwrpWu6aoZBpLfc4WkbHiiWec9dR7mz6gRppmXJKWoYLC9TO3xHjUNVbDu+cn8KhysV8K6pE37dSO+fuRo6kVO6c41x9Toq2iVuKDwB9ulQU47DXVe7+RUyucq9hR6TgegZLjwGow0g0iZZYxATdaBkmRE7XEVI9LrZ2ikJ59tPSMqRJy7CWO9VPy+g6kdsYh6rWCtWYOHfd9LfsNmgp37P3W/VrOeVe68KtNJy7und403htWsb5bzFKNslyF8w2e7Xk0wnPVlzjIclgsmNoGbuk5URm90ggTYMpxo810Cz3llDu8sFqOlTDU0jNl+Kl/OUaYopomXINce5MEi1dWiaOc9fhB6s5VBNpGRFV7upz6rBAJDut4sICdS13dvcpZcUmKI2KXYlEjtTCuccqlsSBlB/Q6nOo6tWZy5Do5dOpfbZSOy2TznL3VvWGnCva+8Zyyn1gce7Np9xtgX3xJCzaF6pRKEWX3Hej6UV1c8sAwO7zgQltS9zvrnNRk3MvMYuYdPhBy7ax/dL+8FFAZ7MOJnKkRHp5MIQAdl4EjMRH7jHd2H7bjRDyRTPUkE9nSG8/tl64JnCMNCz3uLBAgqVluavbs9EySYO4iHLuakDToaJ2X7gS+89bESq3Rm4Z9azrcKgePAco79HtXau5wbYtRGRBj+LcdQbxMd392KRrtU8ueYWqAjX780cIqZQLuhtVb748OCzVE1JdKjj3+rHna9fiCeyPtr/+n+8o6Sk6RrnrTqeEEHjmGuCaKy/0rrX0rFg3DwZnuWso9xOffBGvXL8EeOkl95hsiHqWe3i2Ukso5At/AH7xi/91j2nTMozlrqJIdDr9FX97Hv+6YxawbJl7TGc1sbtxRcRy149zX3QxsMu/vB0kLW1FF0/L6ESOPHzrG/jp9DeZXxKsWHcFdDrLfb33F+GhG4HJv/p/7jHd2QqXREsNMjqz0revWoAn/nxn6KbJFBoXgSav1Yvt32ruXLx1GTBy2l/cYzKcUU+5s7OVAca5N51y33+D2QCA9gVvu8f0aZnoVL+kablzysyz7nQtd+9xt5f1F3rssHCp/LBggXtMe4UqY7lbpEfLCGdQ6uxZG5AL6AxoXPinnCj2VcKzkCi2X7JSfujp8R1NHsS5yCR5pZ6PwrZtbLAGmHLtb7xra6lzmJYp60eO8Eh25roUBTNb0ZmhtXd1AQCGvevrU9AbxG0mpYdLy2jUuZNrhiKZ++ZoGQlLa2Xshu+/DwAY/OJz7jHtkGrBG0yFcs8AurSMgIgqOkvXco8ec51OOtN1hFfTOdaNZnKlMEiTlkGMQ7UWzj14rR5FoaxcvxN5UHs7AKC3vzdRNq/SkkMhYzl30suECcaBSJqKTu2t6Uct4Z9xSF6Kz1vu2ikX3Atq9zNwKT3cMF/t7J9hJA/EnENVFqjWOvs/pqdlpHKvaFDD5tAayl1zOoW4aBkdRcd0ei9xmJ4V6+cHPctdJ7dM1IusS8vEce46zh9eueuFqnGrNTvbOgAAPXYPe40fxNY5ORSS828AgCXaYVPyoCIYZeZFjuiEBQazBba50TJpnW2UGMARG5svSrCRPEvi6Cbd91xtEZNO+CdfoORBnEspDUBuIamVQyjGiaxF70ZXqJaoBFg20lY5C7SGcq/Jcg82xDbqAMprY67wXctZ7u5mHbVb7rVw7pwVW5PlHrJulpVegT16duKV7IYZuhQFo9wHt0vl3ltJVu4cdNIPxGVIbLOHolJazV0SlMG8Dzefjs6sIWS510vLOLZk1XPc9APhQbwyBH20hrskCHV7brFakuXOWLEqFFLHcc5B+kc0I9BK4QGtVsvd75DVj7qjUMrfN+l+AHqbsphC8yp33xtQ8alpomXaaQjQ1q2hrOKtmyRaxmZCIZXl/kHlbfaaYKk56OVZ4Sz3ZaWZidcBSbMVTQva1/lcy70W5V6j5e5l4AzFXouhsEvJik4wCkkpr+RBPGrRKYdyT39fomweyZy7UnThtl2qDEMfrUq8vprlnkRFcbSMVbflrk/LpM7+6SbF82DVsBgyzLlPtD4OAFjTk2womkLzKvfIyrKgcn/5/ZdBPyP09Pegp78H/XY/61DttIYAALr7vDCwh+c+jMWrFwfO4zay9u4VbEyvLXkN9715H15d8qq8VkRpmUGOFftY38VY2bOyelVjaZlgmYQQqNgVzFw607sn2ZHZyu6V7wMAVvasxJI1SxAHdkCLCf8UQgSeoS1swA46F5Xl/mz3Teju68bSNUvR06+v6Inh3JesWYLnFz2PeR/O8+TCC8dzy2N1Y+2IVxNl8LRM9dmKEAJdfV2S0w/RMq99+CwA4OalP0qUHVumGEVnC9t551zMNrBm9H+wbMQ/0dXXVf3+KeLcp8+fDiEE61DthRxQ/vb2rVprC6KIvueuvi5093WjYlewtn+tOysIJ0uzhy7Ae23/wKJVi6pKcJ9pJOVvvJHYb/ejp7+HXQw51BoDAPj49bti6ZqliTU0gaaLc+fAebl3umonAEDnzzu9g4OB3v6gMvtXaSoAYOgvhkbuK872bsi9cI+i8H5csHIBtr9y+5hyeueNHDTM/TziwhGxcoM/+GcrUSv2E9d9Ak+8+0TwmjIgeoO867OlS1m5B0w8AI987RH3ezWHaljRWecydoIVVBjDOgcDAGb1P4zBFwwOnKpVZ4aKWv9X67OXVaygQnt/uJw208+CnfK6o67D13b+mieDs9wZK/b+N+/HYTcfFjyxw/nzYfsxsh2+1/tKQPamIzbFvB/MY8sOIdwHx21YEq6DQh94JT7kgiGB74tPX4z1h/LPTYGz3KfPn469/7R38MShzp8PH1TmAgD+9MbF+NO5F7vHv7TDl3DTMTdVlSsRNVzCdVAoR6JlgNXld7HhbzYMHFtz1hoMbhscOZcCfSrKAJzx8Bm46N8XBS8aDoQJvmf6/wgAmLNiNsb9alzgt9i2nTGa13L3QdfLDXidXGFbOkZLBkdRcOkH5iyfE3sPv+PWnzkwjNtn3h68jjmHW6EaUewOXhr821hZfjw679GgXM1omUumXxJ7T7/lvt6QkbHnzfpgFv9DwHLUje0H/omztM474e4TAt85y51Llnbxfy6OnMdhj/H7sMff+eid+Isiz11POdyw/Dta523w6w0C37kn6houvucRUewx2HfUf7HHb37lZq3rKUTLVLP+e4SGTwHAvtfuGyMslHIhZDxEFHsM9u/8rtZ5JqGl3InoUCKaRURziOgM5vfTiGgmEb1MRP8kok0bX9R4cDGmcRtIb7ryS4Hv31v/Di0ZupEjwzqGRc5T2GDIePdzqQTg57yDb/zQ8exxP7STaAHYpP/AwPfzSnoKsmqdfb+VqBQ5zz3f18JKJQALdmfPC9MoXiG8surGqgPAV8v3BL4f93ayM1XK4xyq0Tp/ZcevaN1uSEdH8kmRMgTfj67dd+mm7wW+7/8Yz/9G+obwflHgLPeNh2+sVY4dN9waWDNW61weQVqmtxIf5fSxEZMD3zd94c/seZ/a7FOB71yyNCTQMn609Y0OfP/ixj8E3j4w5ux8kEjLEFEJwO8BHAxgPoBniegeIYTfK/cCgMlCiC4iOhnALwHww3e9YJ989KUI4YQ1nRM8f6uDg1duuSWAb6tzhLzXlw9D28ggb1aNlvGHQr7/kbN8/IYHgLcOCZzfcYr/WgB9Q4Ll2+Yu4NhjUOltD8pmxinOct+8vA/mzu4Ern9Y/kY2YJex73HBaysVCj0XAfx4MDBnSuA8m6UoolbsxsMmyA9XPQ8s3gVo6wL6OwBRAp3tq38HgD8+E7zhTn8BPnsCeuKiDALKPWq5W6s3hD17CnDPNfJAeS1Q6sVOFwwPnPfJfYfglhMFYPUDtjOQnMPYNtwMjanzrHc+lB8uXA6sHRW8wFfnUaPgPGunbQHAlO8Ce17uZ19i61w1FPIcId/xkCXA6g3Qd2nw5761HcH3bPUBP23HiVv/v8B5upz70RNOxOWvng38LBrP76/zhAkALg75cg79PrDzX6CFkOX+QdcH8sOKzYBL5wZObf9X8NJ37j4BuPdLwAYvAAv2BDZ8FjhpDwzp3TIowvnP1VnO0uXng8d9Bf9452/ARcG0EH0AcL73fcIEAH8JFeborwETH6te1wyhY7nvAWCOEGKuEKIXwK0AjvKfIIR4RAihCL/pAPSG+HoQyWAXVO6b9n0amL9H5LKvfz34/cDAYOvcs9KGvv4gT82GBTI5Rx6Y+R/5YcR7kfNHhliJ558PndAnOcFX3whGVrh3D/DP0RWqiz5YC/Q7PgZhAbYcuw84ICjmtNOiNcEH2wCheP9qzkX/85g1b02g/OgbLBV7SHFZFnDkkaEbOte89XY4Htu5f9hyDw1ottUjBxKF/k6gZzj22y94t2OPVReUId9zzDIpJsKDs2IXLHbeUaW6Zd7RARxzDILyuqTzbeWqmBlU5Lkz2v2JM52fLGC1pFkOOih4yg03hO/bBlTa8OrMUNtm7s/NVt580ylXwr6um2/OHLTLkfYVh3CqiPaSY+xM/0Hk3J12Cn6/+WYAlXap2AHgI2l4/OepsOxqg7j328JFANZG6cRDDw1+32cfYNMwX2G3yQE1J+go940A+DXVfOdYHL4B4H7uByI6iYhmENGMpUsb51Fm49wtGd8tBAJ/bidXp1mInLPhBmV0DAo2Bj4sMErLbDlkVwDARaftFLlvOTRP2mWX4O+/+mUbAKB7bfLCE2L4QattLUaP6HDvV6lIPXHSScFrhw2L1nnwoBLGrB/q9Iyi43ZiWrRUjut3ThsMIYBVqzzZYdx9d1Du6f/jpCToj6lzhHMP1blcwZZblCL12T3E/gwdGq0zC2a2wi1WW9MlP8+eFW1jYdxxB7BmjXwmQgCfPEC+51VdMR0/NFsJzLEcAXvs1u7KW75c/t922+BtttgiWK41awBU2tAvQnKr0G/+DIv9FRsQhDVryL2nbfN1Dj+TvfYsyVmTFoLveW2vLMPhU9oi9x0dZEdw3HGyTIsXy99nPCtnacNHhNqymq0wdfb39b7+ilzRHZJ7f0jDEQHz5gXP2XrLNlhtA1u5c8M02zWI6HgAkwGw3iYhxNVCiMlCiMljx9bDySEUORKlZbi0nLooWWUIClvu0fM4WiZ2cYUGhg6Riq67N2xZMbIZWkbADvDfVihapRpKVIosz+cWsLjWDfydXl43uEMqraFDg1x7NXS0yzqv7UtW7nzK32hK43rArTbm0g+4idHa9B7w4MHeMxk9Qj6nNd3Jyt0ple+T/OyP7x4VYoWqlQGiHMnto3IIJaUfqIhKJOuldvuyyoCluTw/RMv0Osq9vU3vPRMB6zvBQGPW4xcLunaR7zlyG5Rw4a26KFttEDla7jqhkPMBbOL7vjGAheGTiOggAD8G8AkhRLoliDpgYr65bfZsZhMBXZStMuxQB+CcixwtE7e4QgeD22Wn7+mLaRDhsMBwnDuzWlAXFpWiCZeqRI74p+tevvran3dHm+w4a3s1lDuzmYKgCiyk63xJ8oJyEXDyqYGvLbwptQbay84MrUdvthJu1wCTu10TZLehH8H2peYG/rBAjpbhEqPpQi7P74dtIzA4xJQy8Kxjd5rSQGe7Sj8cohxVnQN6JNq2bVRA9Sh3GtiW+7MAJhHRZkTUDuBYAIFQBCLaBcAfABwphIhfFZMRuDh3buWcLspWScty57hYbjWqLjodK7Ynzor1Ic5yT63cEbXcBZNoi8thr+pfLtcuu6Mtoc5+5S6YDJ5kZ67cuQyJbp1TKXdZZ23Lnfxy07cvQCr3Pjuk3LnZCmu5S+WerJyjaLPaABLo6dVJtxEcxPsq6Qc0ZTyEF2MxNiLAKXdRh3KnAc65CyH6AZwK4EEArwOYJoR4jYjOJSLlHrsYcinD/xHRi0R0T8ztGgcmPjWg3Jmcy7ooW+VIwiV2hSpj0bkJjVJ0vkEOrbG2N4YTZRZcBE5jMkDqokTlyNZ7gsnqxy1iqs9yl4ouNieH3+KKs9xTKjpeHlNnlcPe9k/XHcs91YAm33NXjx7njkZa7iij3w4NpEqe7/Wxhouj3HWpGD/aLGe2Em7bHEK0TKUOy73DtdyTDSa2zqiHlikDpYFNy0AIcR+A+0LHfur7fFDkIoPgHKqSlkm3ALetxHHu8YJYWucAACAASURBVLQM0BiKQlnuvSHnos2GQjL8M9VHy9gIxhOzFp0VpaKUAyq8FFwHqs5hWsa9u0/RWTF1LjXScmejZaJcbD2+FaXc16yNid/2W5kUetaNoGVClrsX/hlObxEcxOtR7uVSG9CvlHtnwtlBWkZZ7uHEaDpoL7U59whHoEVpGW62YotKJEmYLtpKcrZii/RGVz1ojRWqzLJhweS80EXZKkc8+9Udqo2Zrg/udCiKuMiRUChkOFqGy3mhC86hylnuJYaL9dL7ph/QdDh3MOGfoEpDOw4xMzRlMfo39la5c9JQFIPaZGhfrBXrr3Mo7DBuG0FdWCijIoLPmlTiMf95TOQIl/VSF8pyj8xKGYRpmXos95JVAmwrdiFUsM7RUMh6OHc1sMT60DJG8yl31oKOhgXWw7m3laRy9+vYaqGQdoOcPyrapLcvJs494FzkHKrRzUh0YVmlKC3DrNbknYvOgJaFn6FKtIwQAiABq8oK2VrBRcso6zxg0dXhXOx0Ni3p1qRl/M9aDTBpqShL6IZCxlvuaaAUXWydA+UJRcs4ufBT0292W9Ryd0l37xgXLCBzt9en3GPpt4zRfMpdITydCodCMmk5daGUu3+Ht6rRMpxDNYXlPqiDp2Vc+CgDzqGKOhyqZSpDIBznXo1zZ5x8dVjusbOVyArVxlEUSfIUynGWOyiV5T7YUe5dPcm0DCG4a1Z/HRQFIC33MC2zqm+xEuadF8e5p2xfHhWlo+hCA5q7ZWM6Jcs5kaNEVNxMvA7LvVxLnRuP5lXuPgSXDUuIOuLclXLv9fU9bkGOZ8V6cvvr4NwHdzrTON0FPWEnMpO7XRdt1CFXe/rAWrFMtIy75VkdnHtvCsvdU+6NjJZhaBnGcleRI2mMSbXdYFevnnIHwz+nnqGhDZXQIP70apmpsae/23eUt9zTKrrO9oTY/iqrr+uZDQMAiahyd3+L7BcQinOvx3J3lHtXodzTg/dyp1/c0lYuRSz3ahsagHEuplF0blhgpZs/wWe5q2RpYT9D6tmK1QlRCi1PqJZbBlElm2ZAU7OVMC3j3ilCUfi5fme63lDLPT78sxIwHkTqsECl3NfGKffQegbOck9Ny6AMO0TLWK64hDh3Zo9YXXQmWe6BhhykHOtW7pwTuQoV1ag4947Cck+JSFhg0LKqx6HaXpZ5MHRpGS4sMI1yV/d7EdcFjrOcO6JOZNQRCtludUCELHc25ltZOgwVlSbOPS5CiCtD2M/gDiqNtNwZx7my3P1b5VXqiBwZ3KEcqhqWuygBDaRlStSGCnjlbgVWN3ORI+k5984awz+58OK0AxqJqHJ3X7OfcyeGAWiA5a7lZ8gAzavcfeAs93ocqoPKnYBlY1WX1/lE1Th3LhSydtmjB8lEGZv0xUSWBjh3Jraf0kfLkCWA4QuCfotq6W8DSlatyq1ddlubvPbd3pf4E8IDGvkHUjOWu+Lcg+0rvXIvlWUdZq95hj+hGufeX5+iWz3oVawa80jg2H6D/hsA0FHykqAp46Q/5GdI26d6rY8AAMu7VvAnBAyJ4CDu9qm0VJTdiT4RSn8c9aeyicPqUe6JA1rGaD7lvtde8r/z4AA/5x603NN2+lfWPgAA2GGa19jtKtkC2aX4KZyLKqf5c50XY1WPt/elG+ceXorPOFTT1vlFuhYAcO1zf8HbK96WB6s6VKOzlTQLetYfvh4A4NG+X+L5ReE0mYhy7hlY7i8sesFd2MNmwrQYy91Ob8Uu7H4LAHDnyjP5EwKx/aWgFVunousry1TFlz9zOXr6e7CqZxXa4WRd9LVjNVD39zfGcu8WUqmf8tx+/HaFkZBXJgItxWwYAPoGLcCC9W7Gy++/7JMd1e5xi5islLTMXSt+BgA45N6tWFo3azTfNnsHHCD/+1Is8iNueivjgBH/jTe6npT3drY0K1WAMHHg5Zbxd4D0YYF+DL/Qy0f+V1XVcLRMmJah9LMVhW/+/QT38yYfAqFU8K6i42L701ju6w/30vrtdvVu7udF3ICG4KbgLude57Pe9epdvc8Lge+FfldWbHC6nl7RHTHpSJz2mPzs3zKPp9+sIC1Tp+Wu8N37v4vv3i93D/rMauBHCFqxjbbcNx2+ufu5dK6nLNk6ixiHasoBTZSk1a623gSAbytZvvO86De/kZject988E5Y3Ct3ZgtvQ7li6gqM7IzfmawRaD7LXTVqv3XD0TJ1LOj5wUHRfUYsZuCtSsuktDI4xHd6xqGasgPs1v/9yDHu6XEpf213EVPtshOv0eDc09Z58tu3RY4R855Zzt2WoZBpsOWGCRlRQ5x7I0MhJ77458gxd521T6m5dQ7NhlMPaB/7dPUTQrQM16fStC8AQHc0bab7npkVqpFFTCnV5F+/dHvsb6Mu0kzlWQeaT7mrlxFQ7lGqoJ4FPVtvPgT4/atBscx5XLSMS1GkbIh378I5bh1EaJmw5Z5euT98+iXAdUEutvqAxsxWquwLWw2nrxLAS8cHjrkLM/10mAhSUWr2YKWYMQDAv6/+YuQYV2fXcg8YDyK1oiMCdnjsjfgTIpw7Q8uktNxfu+kE4KJlwfIIJda3x6+KEOpvjOW+/aThwDkMHePePBws0DiH6ktfWg4smxQ4pt6zP1u+FyDhbwQiteU+diyww995H8Mu9jdT3bMWNB8tw1juQJQqqCcsEADEku1w6aUCP3A2f7HQDSC4ezqbf8NWDTGdwjnySGD2xwT+9Ce5ycOcOQAG7Qvg38FQSCbNcT3KfeRIoPv1A3DuuQLrrQf86U9Az+tvAtgqcF7VhVsp6/yrXwEX9N6Ajo4b5JZxXetBWOMALGdmaCEFi/RL8dvbgTeOFfjrX4Hf/AZYsgSwSo8CODBwnqpXpeIf0GxQSuUOAC//a2u89prA9ts7B6w+QHHfGdIygwcDoms0fv97gVNPdUQPvRzAdzGk3dvekZut2LCRNokWESAEYdo0gf/62gq5W1LfELimS5U0E94ipnR13nFHYNFZs3HSScDf/iaPWet/HUBwFsP50OrVIy8/MxLHHCNw113B47fNTn1LbTSf5a5eMJN7OsyJ1ruJw/e/D3dXle41mtEyTrkCzsVKBXjsMW25kyYBF14IvPmmlH3Up+W2bJxDtVG0DAB0dgIXXACcfjowcybwFtMAlcVoM9EyaS13QCpaIQCxehyEXcKG6zvJpSILt/wDaX3KHQC23hqYOhV4/30pf/rj7ZFzXP45khWyvva13XZe+xIVL0CgarRMnOU+ezYQF1rJ4JRTPNl3Xic3ZPc/RlXnSoMWBip88YuA6B4F0TskaJiEUjs3Ms4dADbYALjnHq/Ol53p8O8BPaKoKO951+O7U7jzTt97dv4mTUq+rl40r3L/4Q9l1n8i/OXbP8ToruDKMpZzP+AA4DOfSSc3kKmPACJYVgninBBFwXW+Cy+Usv/1r3SyFU45xZV962+uAYU2iwYXCnnVVcA//5lOHlPnzcZuhN88EORnbdsG7NBqzZtvltesXp1OtsJhh7myz3rwGQjyW3QxtMz55wPPPptOnl/jOHKn7L8vdlwcNB7YyJGnngLOOCOdXD923NGV/bN/zgaQwLkvXSpHqVNOQSqo9/zKK67cHxx5CI5+PWS5c7OVJ58ErrkmnVw/1ltPyt5lF+zzXhebfiDgx1q2TFojTzyRTp56z1df7db5d//9bey8CIGYeBleHKrz+efDndIPYDSfcvdvROpTPst+GV5FKqLTqcceA+69N53cKqFMVPFiaFnnz6xZ8v/8+elkM5bphis+gn3B2lCxGD/DySdHd07WBZdzAcD/TA8vSxcAKKjcz3e2hn/nnXSyF0Y2+8KXZ7wB8VvvfiqSI2K5/+QnwB7RzdG1wOWW6e7GS1eFFslxiu7jHwcuuqhqW6kVJz43Dxc9+a77veLyz746r3E2KH/ooXRCYsp7122haBnOubjffsCJJ6aTy+HFF/HQbfOwy/ueX4Ad0J5+GujpkVPNNIhp2y/8Aei1vcV8LC3zk58Al16aTq5BNJ9y74jfbT68QjXrhFIKnb1rvNO49LdK6zGx8lrojM9/3SjOnUWVOvs3M1FWbGApvlqHkDbd6cSJ7OG+ANtVn0OVRZU6V8KWe1z36epqXHkAjPKlyWVpGWXwpH3W1epcCQ5oplTG+l1en2INprWOQfXAA+kEVBmAeytB5V4vLZMXmrPU774LPPqoJIYXLAAAXLML0BN+KY1UdP7G8OijksA7TkaBd/T5LXcn5tvfEOtV7mpAmzZNTp0vuQQAcPnuQaoAZKcOkWPhr/PxxwMLF0I4Srtie3W2hQBEyHKvV7l/7GPy/913A8uXA48/DgCYsb7HTTeCc4/Ar+i++11gxgx0T5BbCFd8XKxdLb1FDdx3ALs68fY33CA96VddBQB4d5g3W3UHNP97VvVPO6io9/zVr8o2drsM4XtnRGhAq6bo0s5WxjphoStWAK+/Dlx/PQBgtc/1wXLualfwjTdOJ1eV9/bbJYXn0Du9VlC5o46Q6rzRfNEyALDJJvLPwYpRo0BYEVmhmomiu+QS4BOfkJ+fegoAYNleZ3a3X/Mr90GD5P81njVSs+xNNwW+8AX5ffvtserMMyGo29k+zFF4VoMHNPU877gDOOYYAEDfj3+M9nPOAXwdgN2hp17lTgRMnizDhwB3ZfK/N2rD3s4pLC1TLyWirn/sMWD//QEAH+2/P9bccVNgezqVOCyAIUPkO+7q8pRPrbI/8xk5kAKu0n57hNdN3W0c/e1LvaePPpKfa3U8qjr/+MfAVjI6at4BB6Dr1UfR1x/sU7ERQr29VWfVsbAs4FvfkuFaI0fKgRwA+fLfsCk91DRxWTCsUxvqmR16qHxvAHo7OvDMuB70+GhWAdHYPmUQzVnqEIRlwRJAj48ru+Oupfjm86/yFzgOFPdPx+Ho7jPpUyROR6j4tuTb5s03Mf2PQMmfo0RZ7D/4QVS2DoSInCscq7HXGViEEBjdBXTETbHDcm++OVmuupevU5UcCsCfWXDUypU4aG5I7ssvy//77BOVraOAbZt9Ph92+izJrm70nAvs88oM7wTGIer+TZumJxcI1LlsWSAAfb5B/H+efB5P3RTyCyjltvHG6d5zWDE71/ljscWq1XjoemDs0vejZQbcIAP3b+ZMPbk+eUBM5Ei1aJnOzsbU2VHalvCedUXY2HRFjHLv7o7K1ZnBMDtkL9l8a/SWgpz7iS+uxFFvzuPvEZY7eXKyXINoCeUOIlgC6PNtpXXknLX48ROaERM6DkfVGBirSPgU3SnT/g97LgA6F/ucpzH8MQC9CBpGuTtrmNxty2xhY9kvgZ/dFb8qLoAvfzn5HKbTqw7W69vZ5pprb8I/bglREdtuG3/fpUuTZYfrrD5bvhQMi99Huw18+YE7omXm8F/RlccRcANaSQ5ofuV+yvTXsPv7oSyajtWZGjHK3R/+Oe4/j+PgucARd93knVdtsNxuu2S5TNu21IDme8+P3vA45lwTk+AtLcJ1VqG2PoNpy6f/jXmXApvP+Ld3Xns0ZNXFRhsly2WUO5Gss5+WufyhFbjiIc0w5uee0zvPEFpEuVsgEex8LHxUTs3gLHdl3ficixWncZb8yZFGVskhwUSFRMAod/V95RpZZ0UH7T+rysrHWsEoOoW+fq/Tj1+5yiunwiGHxN/Xl/QtFkKwcsnX6StO87X8Cr2aX0MnDLbKgJbYvk4+Ofn+SbKZ9uUPeXXbl7/O1QY0nWgSznJ3ktj5/VgfW7Ya49c0OMNhDI3k3xVs7FyZn2XcW7OC18XB8VUkygVCA5qsc1dvD3eFhx13TL7/AEBLKHdBJVgi7AhhMGSIs4rCt5pg8mRgyhQNIfGWe8Vn3dhO5EZZMJ1v8WJProIOD88qOunO85R7jPU2YkRwNVYtnDRj3ajP4T0pASCw6Wyl4luZJCQ3qmLA09AyruVuY22PfJ79Dv9LwTzF8v+FFwbrbFl6nZJ5z2Wr5BgPCe2LSDoI/XJPP12fiw6/Z7fOPv7ZOWb5HemqzNdf78ld7GydV82wCF/ve95lSxpMPUl9yn8PIWQbOOYY6SPSQZxyF0ydBTOIP/CAJ1utbShruBLZOhNIAKvXJtS5s1MaL2n6lEG0hHInS9IysduWKXANqVTSi2KpYrn3+Rxt/c6xdv+uD+r+3LY9Osqd45+d7yrnfCXOkknjYFPglLsDv+XuHfQd6+8P1nf0aG+6rPO84yx3ASz/yBnQHL8Da8Vy77mataegyuZX7mVZj94ky5171u3t+lFSs2ezDkJBnrLpV3XmjAf/e6olQosZ0BQVFbsrWBxKJWlQ6DxrQDqBX/X5xtSqb98MTd0p0HvU/f1trJNb1RwDpm2XnPe8Jkm5x7TNgYaBX0INWKUSCMCHqxJeCqckLau2xsC81F5a6X7ud6IY2uyQFQukV+5VOPfV3VLh9PfHdOJGK3fncxc8ftl9ev7BtVKJ1ld91x1MOcsdwLKPZDRDv2O9tvkHmjjlrvuef/tb+f8//3EPlZ32Nb8nwTlZT/sC5ID4yCPed3Wvcg/6+mRdlfFQCiRUY9pmrc/aLw+yzgDQ3buWu6I6LKu2sF8uoMGn3PtdKooJUuDqnLI/qwikROUe4+wfaGgR5S4t9/mLNV5KIy13B/3wNtZY60wJLX+n4JS7cgilVO4qxHrJChkZUFW5p22IVSz3d4d76XJ71DTYH6VQqUSnx7V2Ps5yB7B8pXzPdp+sc7niG0jj3pOuwpnhRN7Mm+e7VJbjjfYbo+f779nXF3X06c4YqoAgsOgD2Z76HH9FuT+hzrUod0bRdXZYzrNOodxrrfPQod5npw429bvFcpW7vy7cIM4mFYwBO6BJqnPJ8sJyHzCoWP0gAVw5/ZrqUVi6yn3VquixKlZs9+p2V+77q2WnKvX6prOqI/qVuxNbK9M+JoBR7jb6QABOvexeEAHjxoZmCqq8ts3PGNRv1aisKsqdeoaASNLJa8hR4v6BKkzLAN6zX7hQlnH+fLlwJa5sMZb7AZ9/HUTAFz4vO2h7X5/MYdPf71FD3KxBR9Gdc47878T1u+IFgBnfirav7m6Pa+7tjTqLa1E4YfgEbbrlahABP/+NfNbtfT1S3gcfVLfcV60K+kI4sIpOyvnXE13ROq9YIf/iYFlS5tq1ybmFOjuB73wncpg6PoRlSbn/eMzpU/5BnGubqv69vd4K1jgw17e3SzkPPjM3WY/E/SiETIvQXSOdlQG0lDsRHUpEs4hoDhFFMiMRUQcR3eb8/jQRTWx0Qauho70TJQFg2zsR2XrOH4c6b55c/efHY4/J1Y/+84YPj1qdVWgZGu9tDzfUcUCVDjvEu59yJHLK/Y47ovGyYQcNo9zbyo6FuN8FTjV9iqtchtszenqAv/41Uma3PB0d8bHJVQY0tEtF3tsLjFErdLfayrvPFVdE+WNV/913l2XcZBMZMkkUVUDcbAWOkt3tavnZnzht2DCpWNWKx/feC164erWkXMLPet99g+dNmCD/jxgRrfPkP0TKg2HD5LNuawNuugl4663g72++6dU9KQ58003lKtFwnQFgRzlrqDiXjX9rtnx3Y8d6YacffeRdpNrpOefIsvnl3ndfUADHP5N0ImMLJl/N6NHyL07BffihHHQGDZLPp1qdw23df84gSf21OwuatrjzFu8+n/qUPMcffqrqfMIJUrZf7kuhEE6mzpbjRMZeTN4Y/71efNHLHRyGZckBa/Dg+DobQqJbmYhKAH4P4GAA8wE8S0T3CCH8BOQ3AKwQQmxJRMcCuAiARlBxY9DZNsgjfs92FMg5Dbgx92L8Ssj/+zny89P3A3s/HXM//4BRzbLQmPKVrbJsiOUe4BySFOX5MSfPmhXzAwOuzpySBdw6az9rJ20CCy48csMN+TLscAuwwy2wVgCIy9/061/LRPFJ+Pe/tersfqu1zjfdFP8bJ9c/k/P/fuhpwKGnwXoPwJ9i7vejHwEnnSQ/x83WAODww/njXKTOxMdrr/Ott8b/xtU5bvY4Ve6vO/8VAHfwpwSc+NX6zc47x/+mLvevRK21ztXA1XnVqiAdlQF0LPc9AMwRQswVQvQCuBXAUaFzjgLwF+fz7QA+RQYTMlhE7PZoLMJZ82vFt75V9edlg6v86FfuSVPlBJRLQWXI7SDkIm0mTAW/Ncm81t/vrnmfalN5DswaAIr5HMGSJbXJCsNPFzF1/uvW9d0+Fj5Hrivev+i22rX+2Uo15R6HhFQRb6wX84PKA5QWzKDvr2d/NS31Rd9uWg1QOeE7fDAo5sQLL6xP0LBh9V2vAR3lvhEA/xx3vnOMPUcI0Q/gIwBxTaHxEALHvAG0+/RlrwVcsC+CsahCAEcfHXsPrZhVJ8eJHzv6VoK7HbG/Pyrbj802k/833ND7fdddoQuybRy5bIz33fl/2/G7ROXGWWq6dQ5TFwDW8/lOSQBLByMqN3xfdZ8bb/R+/+Mfq8sO4dgVXtNTA9p9Pzk2KndszD6lunUOWZMjeoCJvrFJEPDyOCTX2UlRgZkzg2srdOAoq59v7A2uqn09fMWPonKHe5uqB5S7bp1Ds8mtlgODfI/hw07ggS0QlRuX4qC3t/Y4cKfOW6z1rCT1npc/+0T1Z60Mps03935/5pmaZO/3LjDU509d1QHctmtHVO7Uqfw9dJ91yKeTBXQSh3HDYbjkOueAiE4CcBIATFDcZiPw2msAgJ7zAQiBt1e8jbafbYGzDvhxffd94QUZm10uSyfKsmXAFlt4vzsN8c5pwCNz/4WyVcb4ZX8AHr0pmVo5+GB5f7XXGSBlPe/w9y+/DKy/PjBmjJzmvv9+8PqlS7HhUkC89gXMueoCjBGDgJ9vjP/a4dj66vy730nud6+9ZAO99lq5ECdU53cvAYRtwxY23nn9MIx594Xke48eLf/7F9YoJ+z220tO/BOfkOkJnnpK1j+ErV9eAHH7bMwaZePSG08F8DAO2ypm8NLF7NnSUh82TJbx8su9JG2Aq+jfvhRY8NF8zF0xF6Mf+gY+1hafitnFzjvL+6/ns3VU2xg5Um52sckmkpb65z+BXXaJ3GLsVddDrFyJhViFDV+aC1y7Hw7a8uDqcpOs2NWr5UKnjg6ZZfWBB4Att/R+f0OudO66QL7n3kovxD17oH3s+OQ6K3BUm0oBsc02MqvqvfcCn/tc5LTf/LULv7mtBy8sew0b/f0J4PbvY/TQMZHzAuhxtLI/Rfa4cd7nDz6QA1hnp6S/Xgi1WScVyKpfABACfZU+WNdugc12+GRCRRNw552yH+++u+eMHRQ3JWgghBBV/wDsDeBB3/czAZwZOudBAHs7n8sAPgBA1e672267iYaBt5+EOOOM5GsvukiIU0/l75eEG2+Ml52EDz4Q4phjhFixwjt2xhn618fJPe+85GsffliI6dP5+yXhl79MX+fVq4W48kohbNs7dv758tqpU5Ovj5N7003J144YIcQ3v8nfLwnHH19fncPP+ogj5LU331z92oUL4+U+/HCy7E9+Uojbbgse0y33Djvwctvbk6/929+E+MpXeLkzZ1a/9oUX4us8a1b1a1eskOddfbV3bPXq+vvUVlslX3v22UJ85jP8/fr6kq+vAQBmiAS97UhPVO5lAHMBbAa5g+9LALYLnXMKgKucz8cCmJZ0XyPKXeeFVrtfEm66qbFyH3hAXnvccfplNF1npYwbJfcnP5HXnnOOfhlN1/nooxsrV93vjjuqn7doUbzcb30rnWzdck+a1Ng6d3bKa/3GDIdqyv3FF2uXa9v1K/es21fNt9VT7om0jBCin4hOdazzEoBrhRCvEdG5jpB7IP33NxDRHADLHQWfP3bX9fSFMHVq/U7INPj0pyUloJOxMQ5XXpnuuvvv14sDb3T87g9+IKN5vve99Pc477x0133ve3rJ5OrdBzaMyy6T/HicL0QHP05JOY4bB3z+88nnNXg3Kdxyi3xPfr8Ah2p00uab1y6XSDpsD06gsapB5devFQ88IDebyQs6I0AWf5lY7mvWyO+VSnDqnxWuucaT/fbbQqxd68nPGhlZBYn4n/+Rcn/1K++YiWctRLDOK1YI8a9/CdHfn73crbaScq+5RtbVtiVlkrXsxYvze8+jRkm5YUopa7z4opRbLsvvptqWEN6znjfPnMwUQKMs96aCcqSYWhrsbPEHIJizvQmWJqeGiqjwO4TyWKgxciRw4IFmZA12IjcmTPDqOr4Gx2IzQs3QBleL7c0Qqi/n0bZMODsNoLW0kGmlqtKqrku45Rb5P+2u880IFf+dRCk0Gnkmp+IGcRNQdW40FVYL8hrQGozWUu6mscEG8v+3v51vOUxCrfQ75ZR8y2ESStF1aoQ+thpUmox1CYXlXsCNq9XZ1qtVoBTcTjvlWw6TyNuKzRN5Kbp6V73WgzSrewcgCuVeD5RyT7Pre7MiL0WXJ/K23HV2FsoKpikKRYGtS+0rIxTKvR5wK+JaHUrRFQNa9nAzcFbZDDpr6Ox320goR+661KcyQqHc64HKP2K6A+QJZVmtS3XOe4aWp3I3TQ3l/axbCIVyrwdxW7q1MoSQ/9elOnP7dZqAetZ50jKmsS72qYxQPMF6sC42xKLO5uUOBMeqKayL7SsjFE+wHqyLDXFdrLNCXpb7uvSs1+X21WAUT7AeFJ1v3UJhuWePdbFPZYTiCdaDdVHRrYsKR8H0e14XFd262KcyQvEE68G6qOjWRYWjYPo9r4vtq1DuDUPxBOvBuqjo8u58ecY/m1ayRfsqUAeKJ1gPVL54/9Z7pnDJJTIHu2nceqvcKs2/JZsp3H478Oqr5uWedpp5mYDcmg0AfvhD87Lvv7/+TaDTYN99ZT6buD1KC2iDhLIODGPy5MlixowZjbnZXXfJRTVHHNGY++lCCLnRxDbbmJVboECBxmPxYrkwsZH7O2cAInpOCJG4y3prrI747GfzTjTWVAAABZhJREFUkUtUKPYCBVoFKstri6CgZQoUKFCgBVEo9wIFChRoQRTKvUCBAgVaEIVyL1CgQIEWRKHcCxQoUKAFUSj3AgUKFGhBFMq9QIECBVoQhXIvUKBAgRZEbitUiWgpgHdSXj4GwAcNLE6eKOoyMNEqdWmVegBFXRQ2FUKMTTopN+VeD4hohs7y22ZAUZeBiVapS6vUAyjqUisKWqZAgQIFWhCFci9QoECBFkSzKver8y5AA1HUZWCiVerSKvUAirrUhKbk3AsUKFCgQHU0q+VeoECBAgWqoOmUOxEdSkSziGgOEZ2Rd3kAgIiuJaIlRPSq79hoIvoHEb3p/B/lHCciuswp/8tEtKvvmq85579JRF/zHd+NiF5xrrmMKLv93ohoEyJ6hIheJ6LXiOj7zVofIuokomeI6CWnLj9zjm9GRE875bqNiNqd4x3O9znO7xN99zrTOT6LiA7xHTfWHomoREQvENG9TV6Pec77f5GIZjjHmq59ObJGEtHtRPSG02f2HjB1EUI0zR+AEoC3AGwOoB3ASwC2HQDl2h/ArgBe9R37JYAznM9nALjI+XwYgPsBEIC9ADztHB8NYK7zf5TzeZTz2zMA9nauuR/AlAzrMh7Ars7nYQBmA9i2Gevj3H+o87kNwNNOGacBONY5fhWAk53P3wFwlfP5WAC3OZ+3ddpaB4DNnDZYMt0eAZwG4GYA9zrfm7Ue8wCMCR1ruvblyPoLgG86n9sBjBwodcmkwhk+yL0BPOj7fiaAM/Mul1OWiQgq91kAxjufxwOY5Xz+A4DjwucBOA7AH3zH/+AcGw/gDd/xwHkG6nU3gIObvT4ABgN4HsCekItHyuE2BeBBAHs7n8vOeRRuZ+o8k+0RwMYA/gngkwDudcrVdPVw7j8PUeXedO0LwHAAb8PxXQ60ujQbLbMRgPd83+c7xwYi1hdCLAIA5/8453hcHaodn88czxzOdH4XSIu3KevjUBkvAlgC4B+QFuqHQoh+Rr5bZuf3jwCsh9rrmAX+f3vn89pEFMTxz0BFpYq14kGooLmIFKRK8VIRjxqL/4MKHuzFk5eCeNZLDkovBUEQxd9nQRAKgj+Kv+pBjD8OoWIEEcGT6Hh4E7tbkm0L3WTfMh947Huzm7z5Jo/ZZOaF1ICzwF8bbyFOHQAKPBCRWRE5ZbYY11cF+AZcsXTZtIj0UxAtsQX3dvmm2Lb7dNKwUnuuiMgG4A5wRlV/Zl3axlYYPar6R1VHCJ989wO7M+YvpBYRGQeaqjqbNGfMXUgdCcZUdR9wBJgQkYMZ1xZZSx8hHTulqnuBX4Q0TCe6qiW24N4AtifGQ8B8j3xZiq8isg3Ajk2zd9KQZR9qY88NEVlDCOzXVPWumaPVA6CqP4BHhFzngIi0/hw+Of9/n+38JuA7K9e42owBx0TkM3CDkJqpRagDAFWdt2MTuEe46ca4vhpAQ1Wf2Pg2IdgXQ0teebWcclx9hGLDThYKP8O99st820E6536RdFHlgvWPki6qPDX7ICF/t9naJ2DQzj2za1tFlWqOOgS4CtQW2aPTA2wFBqy/HpgBxoFbpAuRp60/QboQedP6w6QLkR8JRciur0fgEAsF1eh0AP3AxkT/MXA4xvVlc80Au6x/3nQUQktuizDHF7NK2MHxAZjstT/m03XgC/CbcLc9SchxPgTe27H1Zglw2fx/A4wmnucEULd2PGEfBebsMZdYVMBZZS0HCF/9XgMvrVVj1APsAV6YljngnNkrhF0IdUKAXGv2dTau2/lK4rkmzd93JHYsdHs9kg7u0ekwn19Ze9uaK8b1ZXONAM9tjd0nBOdCaPFfqDqO45SQ2HLujuM4zjLw4O44jlNCPLg7juOUEA/ujuM4JcSDu+M4Tgnx4O44jlNCPLg7juOUEA/ujuM4JeQfC52K6F9k2GoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"predicted values shape: \" + str(predictions.shape))\n",
    "print(\"actual values shape: \" + str(y_scaled_key_list_stacked_tensor_dict[train_key[0]][:,:,0:1].shape))\n",
    "#print(\"residuals shape: \" + str(residuals[:,:,0:1].shape))\n",
    "\n",
    "prediction_value_list = predictions.flatten()\n",
    "actual_value_list = y_scaled_key_list_stacked_tensor_dict[train_key[0]][:,:,0:1].flatten()\n",
    "residual_value_list = actual_value_list - prediction_value_list\n",
    "\n",
    "print(\"predicted values shape: \" + str(len(prediction_value_list )))\n",
    "print(\"actual values shape: \" + str(len(actual_value_list)))\n",
    "print(\"residuals shape: \" + str(len(residual_value_list)))\n",
    "\n",
    "residual_list = []\n",
    "for index in range(1,len(prediction_value_list)):\n",
    "    #print(index)\n",
    "    #if index+1 >= len(prediction_value_list):\n",
    "    #    break\n",
    "    residual_list.append(actual_value_list[index-2] - prediction_value_list[index])\n",
    "    \n",
    "#print(actual_value_list[59979-1])\n",
    "#print(prediction_value_list[59979])\n",
    "#print(prediction_value_list[59979] - actual_value_list[59979-1])\n",
    "\n",
    "plt.plot(prediction_value_list, color=\"blue\")\n",
    "plt.plot(actual_value_list , color=\"green\")\n",
    "plt.plot(residual_value_list, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for feature in range(1,2):\n",
    "plt.plot(prediction_value_list, color=\"blue\")\n",
    "plt.plot(actual_value_list , color=\"green\")\n",
    "plt.plot(residual_value_list, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(train_error_vectors, axis=0)\n",
    "cov = np.cov(train_error_vectors,rowvar=False)\n",
    "p_values= multivariate_normal.logpdf(train_error_vectors,mean,cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot features\n",
    "def plot_features(feature_dict, feature_list, ceiling=10):\n",
    "    \n",
    "    print(feature_list)\n",
    "    \n",
    "    for i in range(0,len(feature_dict[feature_list])):\n",
    "        print(\"%s of %s\" %(i, len(feature_dict[feature_list])))\n",
    "        plt.subplots(figsize=(20,10))\n",
    "        plt.plot(feature_dict[feature_list][i])   \n",
    "        plt.show()\n",
    "        if i == ceiling-1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_plot_features(feature_dict, feature, ceiling=10):\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "\n",
    "    for i in range(0,6000):\n",
    "    #plt.figure(figsize=(20,10))\n",
    "        plt.plot(feature_dict[feature][i])\n",
    "        if i == ceiling - 1:\n",
    "            break\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cycles(tensor_dict, firstkey, secondkey=None, begin=0, end=6000, feature=0):\n",
    "    \n",
    "    if secondkey == None:\n",
    "        \n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(tensor_dict[firstkey][begin:end,feature])\n",
    "        #plt.plot(second_tensor[begin:end,feature])\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(tensor_dict[firstkey][begin:end,feature])\n",
    "        plt.plot(tensor_dict[secondkey][begin:end,feature])\n",
    "        #plt.plot(second_tensor[begin:end,feature])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cycles_all_features(tensor_dict, firstkey, secondkey=None ):\n",
    "    \n",
    "    if secondkey == None: \n",
    "        if firstkey in tensor_dict:\n",
    "            #print(tensor_dict[firstkey].shape[1])\n",
    "    \n",
    "            for feat in range(0,tensor_dict[firstkey].shape[1]):\n",
    "                print(feat)\n",
    "                plot_cycles(tensor_dict, firstkey, begin=0, end=60000, feature=feat)\n",
    "         \n",
    "        else:\n",
    "            print(\"combination/tensor doesn't exist\")\n",
    "\n",
    "                \n",
    "    if secondkey != None:\n",
    "        if firstkey in tensor_dict:\n",
    "            #print(tensor_dict[key].shape[1])\n",
    "    \n",
    "            for feat in range(0,tensor_dict[firstkey].shape[1]):\n",
    "                print(feat)\n",
    "                plot_cycles(tensor_dict, firstkey, secondkey=secondkey, begin=0, end=60000, feature=feat)\n",
    "        \n",
    "        else:\n",
    "            print(\"combination/tensor doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    def build_fixed_length_model(self):\n",
    "        \n",
    "        if self.fix_stateful == False:\n",
    "            self.fix_model.add(LSTM( units=self.var_units, \n",
    "                                    input_shape=(self.var_look_back, self.fix_features),\n",
    "                                    activation=self.fix_hidden_activation,\n",
    "                                    return_sequences= True))\n",
    "            self.fix_model.add(Dropout(self.var_l1_dropout))\n",
    "        \n",
    "        if self.fix_stateful == True:\n",
    "            self.fix_model.add(LSTM(units=self.var_units, \n",
    "                                    batch_input_shape=(self.var_batch_size, self.var_look_back, self.fix_features),\n",
    "                                    activation=self.fix_hidden_activation,\n",
    "                                    unroll = True,\n",
    "                                    return_sequences= True))\n",
    "            self.fix_model.add(Dropout(self.var_l1_dropout))\n",
    "            \n",
    "        \n",
    "        #hidden 1\n",
    "        self.fix_model.add(LSTM(self.var_layers, return_sequences=True))\n",
    "        self.fix_model.add(Dropout(self.var_l2_dropout))\n",
    "        \n",
    "        #hidden 2\n",
    "        self.fix_model.add(LSTM(self.var_layers, return_sequences=True))\n",
    "        self.fix_model.add(Dropout(self.var_l3_dropout))\n",
    "        \n",
    "        #hidden 3\n",
    "        self.fix_model.add(LSTM(self.var_layers, return_sequences=True))\n",
    "        self.fix_model.add(Dropout(self.var_l4_dropout))\n",
    "        \n",
    "        #hidden 4\n",
    "        self.fix_model.add(LSTM(self.var_layers, return_sequences=False))\n",
    "        self.fix_model.add(Dropout(self.var_l5_dropout))\n",
    "        \n",
    "        #output\n",
    "        self.fix_model.add(Dense(output_dim=self.fix_features))\n",
    "        self.fix_model.add(RepeatVector(self.var_look_ahead))\n",
    "\n",
    "        #add activation\n",
    "        self.fix_model.add(Activation(\"linear\"))\n",
    "        \n",
    "        self.fix_model.compile(loss=self.fix_loss, optimizer= Adam(lr=self.var_learning_rate, decay=.99))\n",
    "        \n",
    "        self.fix_model.summary()\n",
    "            \n",
    "        return self.fixed_model\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "variabel_length_model_parameter_bounds =[{'name': 'dropout', 'type': 'continuous', 'domain': (0.2, 0.8)},\n",
    "                   {'name': 'learning_rate', 'type': 'continuous', 'domain': (0.001, .1)},\n",
    "                   {'name': 'units', 'type': 'discrete', 'domain': (16,32,64,128,256)},\n",
    "                   {'name': 'batch_size', 'type': 'discrete', 'domain': (8,16,32,64,128)},\n",
    "                   {'name': 'look_back', 'type': 'discrete', 'domain': (5,10,15,20,25,30,35,40,45,50)},\n",
    "                   #{'name': 'look_ahead', 'type': 'discrete', 'domain': (1,2,3,4,5)},\n",
    "                   {'name': 'layers', 'type':'discrete', 'domain':(3,4,5)},\n",
    "                   {'name': 'epochs', 'type':'discrete', 'domain':(25,50,100,150,200,250,300)}\n",
    "                 ]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fixed_length_model_parameter_bounds =[{'name': 'l1_dropout', 'type': 'continuous', 'domain': (0.2, 0.8)},\n",
    "                   {'name': 'l2_dropout', 'type': 'continuous', 'domain': (0.2, 0.8)},\n",
    "                   {'name': 'l3_dropout', 'type': 'continuous', 'domain': (0.2, 0.8)},\n",
    "                   {'name': 'l4_dropout', 'type': 'continuous', 'domain': (0.2, 0.8)},\n",
    "                   {'name': 'l5_dropout', 'type': 'continuous', 'domain': (0.2, 0.8)},\n",
    "                   {'name': 'learning_rate', 'type': 'continuous', 'domain': (0.001, .1)},\n",
    "                   {'name': 'units', 'type': 'discrete', 'domain': (16,32,64,126,256)},\n",
    "                   {'name': 'batch_size', 'type': 'discrete', 'domain': (32,64,128,256,512)},\n",
    "                   {'name': 'look_back', 'type': 'discrete', 'domain': (5,10,15,20,25,30,35,40,45,50)},\n",
    "                   #{'name': 'look_ahead', 'type': 'discrete', 'domain': (1,2,3,4,5)},\n",
    "                   {'name': 'epochs', 'type':'discrete', 'domain':(25,50,100,150,200,250,300)}\n",
    "                 ]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Optimization stateful objective function\n",
    "def stateful_lstm_ojective_function(param):\n",
    "    \n",
    "    #validation_loss_list = []\n",
    "    param = param.flatten()\n",
    "    fixed = True\n",
    "        \n",
    "    fix_optimizers = 'adam'\n",
    "    fix_learning_rate = 0.5\n",
    "    fix_loss = 'mse'\n",
    "    fix_shuffle = False\n",
    "    fix_patience = 3\n",
    "        \n",
    "    if fixed == False:\n",
    "        var_dropout = float(param[0])\n",
    "        var_units= int(param[1])\n",
    "        \n",
    "        var_batch_size = int(param[2])\n",
    "        var_look_back = int(param[3])\n",
    "        var_look_ahead = int(param[4])\n",
    "        var_layers = int(param[5])\n",
    "        var_epochs = int(param[6])\n",
    "        \n",
    "        print(\"Using stateful variable length Model HyperParams. dropout:%f, units:%s, batch_size:%s, look_back:%s, look_ahead:%s, layers:%s, epochs:%s\" % \n",
    "                 (var_dropout, var_units, var_batch_size, var_look_back, var_look_ahead, var_layers, var_epochs))\n",
    "        \n",
    "    if fixed == True:\n",
    "        fix_layers = 6\n",
    "        print(\"fix_layers: \" + str(fix_layers))\n",
    "        \n",
    "        var_l1_dropout = float(param[0])\n",
    "        var_l2_dropout = float(param[1])\n",
    "        var_l3_dropout = float(param[2])\n",
    "        var_l4_dropout = float(param[3])\n",
    "        var_l5_dropout = float(param[4])\n",
    "        \n",
    "        var_units= int(param[5])\n",
    "        var_batch_size = int(param[6])\n",
    "        var_look_back = int(param[7])\n",
    "        var_look_ahead = int(param[8])\n",
    "        var_epochs = int(param[9])\n",
    "        \n",
    "        print(var_look_ahead)\n",
    "        \n",
    "        print(\"Using stateful fixed length Model HyperParams. l1_dropout:%f, l2_dropout:%f, l3_dropout:%f, l4_dropout:%f, l5_dropout:%f, units:%s, batch_size:%s, look_back:%s, look_ahead:%s, layers:%s, epochs:%s\" % \n",
    "                 (var_l1_dropout, var_l2_dropout, var_l3_dropout, var_l4_dropout, var_l5_dropout,\n",
    "                  var_units, var_batch_size, var_look_back, var_look_ahead, var_layers, var_epochs))\n",
    "    \n",
    "    print(\"create data Sequence with look_ahead and look_back...\")\n",
    "    X_key_list_stacked_tensor_dict, y_key_list_stacked_tensor_dict = create_Xy_key_list_stacked_tensor(tensor_dict, \n",
    "                                                                                                       train_key, \n",
    "                                                                                                       var_look_back, \n",
    "                                                                                                       var_look_ahead)\n",
    "\n",
    "    \n",
    "    \n",
    "    X_scaled_key_list_stacked_tensor_dict, y_scaled_key_list_stacked_tensor_dict = create_Xy_key_list_scaled_stacked_tensor(X_key_list_stacked_tensor_dict, \n",
    "                                                                                                  y_key_list_stacked_tensor_dict)\n",
    "    \n",
    "    #print(X_scaled_key_list_stacked_tensor_dict[train_key[0]][:16,1,1])\n",
    "    #print(y_scaled_key_list_stacked_tensor_dict[train_key[0]][10:16,1,1])\n",
    "    \n",
    "    \n",
    "    #prepare for Stateful LSTM Model\n",
    "    n_train_batches = int(len(X_scaled_key_list_stacked_tensor_dict[train_key[0]]) / var_batch_size)\n",
    "    n_validation_batches = int(len(X_scaled_key_list_stacked_tensor_dict[train_key[0]]) / var_batch_size)\n",
    "    n_test_batches = int(len(X_scaled_key_list_stacked_tensor_dict[train_key[0]]) / var_batch_size)\n",
    "     \n",
    "    length_training = n_train_batches * var_batch_size\n",
    "    length_validation = n_validation_batches * var_batch_size\n",
    "    length_test = n_test_batches * var_batch_size\n",
    "    \n",
    "   # plt.figure(figsize=(20,10))\n",
    "   # plt.plot(X_scaled_key_list_stacked_tensor_dict[train_key[0]][0:2000,:,1], color=\"green\")\n",
    "   # plt.plot(y_scaled_key_list_stacked_tensor_dict[train_key[0]][2000:4000,:,1], color=\"blue\")\n",
    "   # plt.show()\n",
    "    \n",
    "    if fixed == False:\n",
    "        print(\"initilize Layer variable length stateful LSTM Model...\")\n",
    "        stacked_lstm_model = stackedLSTM(var_units=var_units, var_epochs=var_epochs, var_batch_size=var_batch_size, \n",
    "                                         var_look_back=var_look_back, var_look_ahead=var_look_ahead, \n",
    "                                         var_layers=var_layers, \n",
    "                                         var_dropout=var_dropout, \n",
    "                                         fix_stateful=True,\n",
    "                                         fix_features=17\n",
    "                                         )\n",
    "    \n",
    "    if fixed == True:\n",
    "        print(\"initilize fixed lenght Layer for stateful LSTM Model...\")\n",
    "        stacked_lstm_model = stackedLSTM(var_units=var_units, var_epochs=var_epochs, var_batch_size=var_batch_size, \n",
    "                                         var_look_back=var_look_back, var_look_ahead=var_look_ahead, var_layers=fix_layers,\n",
    "                                         var_l1_dropout=var_l1_dropout,\n",
    "                                         var_l2_dropout=var_l2_dropout,\n",
    "                                         var_l3_dropout=var_l3_dropout, \n",
    "                                         var_l4_dropout=var_l4_dropout, \n",
    "                                         var_l5_dropout=var_l5_dropout,\n",
    "                                         fix_stateful=True\n",
    "                                            )    \n",
    "    \n",
    "    \n",
    "            \n",
    "    print(\"build statefull LSTM model...\")\n",
    "    lstm_model = stacked_lstm_model.build_model()\n",
    "    \n",
    "    \n",
    "    print(\"train stateful LSTM model...\")\n",
    "    train_model(lstm_model, X_train=X_scaled_key_list_stacked_tensor_dict[train_key[0]][:length_training,:,:], \n",
    "                                 y_train=y_scaled_key_list_stacked_tensor_dict[train_key[0]][:length_training,:,:], \n",
    "                                 X_test=X_scaled_key_list_stacked_tensor_dict[train_key[0]][:length_training,:,:], \n",
    "                                 y_test=y_scaled_key_list_stacked_tensor_dict[train_key[0]][:length_training,:,:], \n",
    "                                 var_batch_size=var_batch_size,\n",
    "                                 var_epochs=var_epochs)\n",
    "    \n",
    "    print(\"evaluate stateful LSTM model...\")\n",
    "    validation_loss = evaluate_model(lstm_model, \n",
    "                                     X_scaled_key_list_stacked_tensor_dict[train_key[0]][:length_training,:,:], \n",
    "                                     y_scaled_key_list_stacked_tensor_dict[train_key[0]][:length_training,:,:], \n",
    "                                     var_batch_size=var_batch_size)\n",
    "\n",
    "    \n",
    "    \n",
    "    return validation_loss\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One_to_One Vanilla LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_1 = RandomUniform(minval=-0.05, maxval=0.05)\n",
    "\n",
    "#path_checkpoint_1 = '23_checkpoint_h512_s50_PS1.keras'\n",
    "#path_checkpoint_1 = 'checkpint_h512_s50_PS1_optimzed.keras'\n",
    "#path_checkpoint_1 = 'checkpint_h512_s50_17to1.keras'\n",
    "#path_checkpoint_1 = 'checkpoint_2h512_s50_17to1_stacked2.keras'\n",
    "#path_checkpoint_1 = 'checkpoint_3h512_s50_17to1_stacked3.keras'\n",
    "#path_checkpoint_1 = 'checkpoint_3h512_s50_17to1_stacked3_rmsprop.keras'\n",
    "#path_checkpoint_1 = 'checkpoint_1h512_s50_1to1_bidirec.keras'\n",
    "#path_checkpoint_1 = 'checkpoint_2h512_s5_17to1_sigmoid.keras'\n",
    "#path_checkpoint_1 = 'checkpoint_2h32_16_s5_17to1ps3_sigmoid.keras'\n",
    "path_checkpoint_1 = 'checkpoint_2h32_16_s5_17to1_sigmoid2.keras'\n",
    "callback_checkpoint_1 = ModelCheckpoint(filepath=path_checkpoint_1,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "\n",
    "callback_early_stopping_1 = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=10, verbose=1)\n",
    "\n",
    "callback_tensorboard_1 = TensorBoard(log_dir='./23_logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)\n",
    "\n",
    "callback_reduce_lr_1 = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1,\n",
    "                                       min_lr=1e-9,\n",
    "                                       patience=0,\n",
    "                                       verbose=1)\n",
    "\n",
    "callbacks_1 = [callback_early_stopping_1,\n",
    "             callback_checkpoint_1,\n",
    "             callback_tensorboard_1,\n",
    "             callback_reduce_lr_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look_back: 5\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59995, 5, 17)\n",
      "y look_ahead: (59995, 1, 17)\n",
      "look_back: 5\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59995, 5, 17)\n",
      "y look_ahead: (59995, 1, 17)\n"
     ]
    }
   ],
   "source": [
    "n_steps, var_look_back=[5,5]\n",
    "var_look_ahead=1\n",
    "train_key=['c100_h130_pl0_v100_s0']\n",
    "#val_key=['c100_h130_pl1_v100_s0']\n",
    "val_key=['c100_h130_pl0_v90_s0']\n",
    "\n",
    "X_key_list_stacked_tensor_dict_train, y_key_list_stacked_tensor_dict_train = create_Xy_key_list_stacked_tensor(tensor_dict, train_key,\n",
    "                                                                                                       var_look_back, \n",
    "                                                                                                       var_look_ahead)\n",
    "\n",
    "#val data\n",
    "X_key_list_stacked_tensor_dict_val, y_key_list_stacked_tensor_dict_val = create_Xy_key_list_stacked_tensor(tensor_dict, val_key,\n",
    "                                                                                                       var_look_back, \n",
    "                                                                                                       var_look_ahead)\n",
    "\n",
    "    \n",
    "    \n",
    "#train dta\n",
    "X_scaled_key_list_stacked_tensor_dict_train, y_scaled_key_list_stacked_tensor_dict_train = create_Xy_key_list_scaled_stacked_tensor(X_key_list_stacked_tensor_dict_train, \n",
    "                                                                                                                            y_key_list_stacked_tensor_dict_train)\n",
    "\n",
    "    \n",
    "X_scaled_key_list_stacked_tensor_dict_val, y_scaled_key_list_stacked_tensor_dict_val = create_Xy_key_list_scaled_stacked_tensor(X_key_list_stacked_tensor_dict_val, \n",
    "                                                                                                                            y_key_list_stacked_tensor_dict_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 5, 32)             6400      \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 1, 1)              0         \n",
      "=================================================================\n",
      "Total params: 9,553\n",
      "Trainable params: 9,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_features=17\n",
    "n_output_features = 2\n",
    "adam= optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "print(n_features)\n",
    "\n",
    "model_vanilla_sigmoid = Sequential()\n",
    "model_vanilla_sigmoid.add(LSTM(32, activation='sigmoid', input_shape=(n_steps, n_features), return_sequences=True))\n",
    "model_vanilla_sigmoid.add(LSTM(16, activation='sigmoid', input_shape=(n_steps, n_features)))\n",
    "#model_vanilla_1.add(LSTM(512, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model_vanilla_sigmoid.add(Dense(1,activation='linear',\n",
    "                    #output_dim=n_output_features,\n",
    "                    kernel_initializer=init_1))\n",
    "model_vanilla_sigmoid.add(RepeatVector(var_look_ahead))\n",
    "### load weights from pretrained model\n",
    "#model_vanilla_sigmoid.load_weights(\"checkpoint_2h512_s5_17to1_sigmoid.keras\")\n",
    "###\n",
    "model_vanilla_sigmoid.compile(optimizer=adam, loss='mse')\n",
    "model_vanilla_sigmoid.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59995 samples, validate on 59995 samples\n",
      "Epoch 1/20\n",
      "59995/59995 [==============================] - 297s 5ms/step - loss: 5.8151e-04 - val_loss: 0.0238\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00007\n",
      "Epoch 2/20\n",
      "59995/59995 [==============================] - 349s 6ms/step - loss: 5.9415e-04 - val_loss: 0.0178\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00007\n",
      "Epoch 3/20\n",
      "59995/59995 [==============================] - 354s 6ms/step - loss: 2.8357e-04 - val_loss: 0.0123\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00007\n",
      "Epoch 4/20\n",
      "59995/59995 [==============================] - 320s 5ms/step - loss: 1.0377e-04 - val_loss: 0.0067\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00007\n",
      "Epoch 5/20\n",
      "59995/59995 [==============================] - 326s 5ms/step - loss: 4.4388e-05 - val_loss: 0.0068\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00007\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 6/20\n",
      "59995/59995 [==============================] - 329s 5ms/step - loss: 8.2434e-05 - val_loss: 2.1338e-04\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00007\n",
      "Epoch 7/20\n",
      "59995/59995 [==============================] - 331s 6ms/step - loss: 3.3466e-05 - val_loss: 2.0639e-04\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00007\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 8/20\n",
      "59995/59995 [==============================] - 329s 5ms/step - loss: 3.0851e-05 - val_loss: 6.1835e-05\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00007 to 0.00006, saving model to checkpoint_2h32_16_s5_17to1_sigmoid2.keras\n",
      "Epoch 9/20\n",
      "59995/59995 [==============================] - 330s 6ms/step - loss: 2.6394e-05 - val_loss: 5.3962e-05\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00006 to 0.00005, saving model to checkpoint_2h32_16_s5_17to1_sigmoid2.keras\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 10/20\n",
      "59995/59995 [==============================] - 331s 6ms/step - loss: 3.0759e-05 - val_loss: 3.5409e-05\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00005 to 0.00004, saving model to checkpoint_2h32_16_s5_17to1_sigmoid2.keras\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 11/20\n",
      "59995/59995 [==============================] - 330s 5ms/step - loss: 2.8314e-05 - val_loss: 3.2908e-05\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00004 to 0.00003, saving model to checkpoint_2h32_16_s5_17to1_sigmoid2.keras\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 12/20\n",
      "59995/59995 [==============================] - 331s 6ms/step - loss: 2.7886e-05 - val_loss: 3.2835e-05\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00003 to 0.00003, saving model to checkpoint_2h32_16_s5_17to1_sigmoid2.keras\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 13/20\n",
      "59995/59995 [==============================] - 332s 6ms/step - loss: 2.7867e-05 - val_loss: 3.2833e-05\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00003 to 0.00003, saving model to checkpoint_2h32_16_s5_17to1_sigmoid2.keras\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1e-09.\n",
      "Epoch 14/20\n",
      "59995/59995 [==============================] - 330s 5ms/step - loss: 2.7866e-05 - val_loss: 3.2832e-05\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00003 to 0.00003, saving model to checkpoint_2h32_16_s5_17to1_sigmoid2.keras\n",
      "Epoch 15/20\n",
      "59995/59995 [==============================] - 332s 6ms/step - loss: 2.7865e-05 - val_loss: 3.2831e-05\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00003 to 0.00003, saving model to checkpoint_2h32_16_s5_17to1_sigmoid2.keras\n",
      "Epoch 16/20\n",
      "59995/59995 [==============================] - 331s 6ms/step - loss: 2.7864e-05 - val_loss: 3.2829e-05\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00003 to 0.00003, saving model to checkpoint_2h32_16_s5_17to1_sigmoid2.keras\n",
      "Epoch 17/20\n",
      "59995/59995 [==============================] - 334s 6ms/step - loss: 2.7864e-05 - val_loss: 3.2828e-05\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00003 to 0.00003, saving model to checkpoint_2h32_16_s5_17to1_sigmoid2.keras\n",
      "Epoch 18/20\n",
      "59995/59995 [==============================] - 332s 6ms/step - loss: 2.7863e-05 - val_loss: 3.2827e-05\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00003 to 0.00003, saving model to checkpoint_2h32_16_s5_17to1_sigmoid2.keras\n",
      "Epoch 19/20\n",
      "59995/59995 [==============================] - 337s 6ms/step - loss: 2.7862e-05 - val_loss: 3.2826e-05\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00003 to 0.00003, saving model to checkpoint_2h32_16_s5_17to1_sigmoid2.keras\n",
      "Epoch 20/20\n",
      "59995/59995 [==============================] - 333s 6ms/step - loss: 2.7861e-05 - val_loss: 3.2824e-05\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00003 to 0.00003, saving model to checkpoint_2h32_16_s5_17to1_sigmoid2.keras\n",
      "Time elapsed (hh:mm:ss.ms) 1:50:23.228612\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now() \n",
    "\n",
    "model_vanilla_sigmoid.fit(X_scaled_key_list_stacked_tensor_dict_train[train_key[0]][:,:,0:n_features], y_scaled_key_list_stacked_tensor_dict_train[train_key[0]][:,:,0:1],\n",
    "                  epochs=20,\n",
    "                  batch_size=1,\n",
    "                  verbose=1,\n",
    "                  validation_data=(X_scaled_key_list_stacked_tensor_dict_val[val_key[0]][:,:,0:n_features], y_scaled_key_list_stacked_tensor_dict_val[val_key[0]][:,:,0:1]),\n",
    "                  shuffle=False,\n",
    "                  callbacks=callbacks_1)\n",
    "    #reset model to learn from beginning\n",
    "    #model_vanilla.reset_states()\n",
    "\n",
    "time_elapsed = datetime.datetime.now() - start_time \n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(model_vanilla_sigmoid.history.history['loss'], label='train')\n",
    "plt.plot(model_vanilla_sigmoid.history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Train and Validation MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59995/59995 [==============================] - 2s 31us/step\n"
     ]
    }
   ],
   "source": [
    "model_vanilla_sigmoid_predictions = model_vanilla_sigmoid.predict(X_scaled_key_list_stacked_tensor_dict_val[val_key[0]][:,:,0:17], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values shape: (59980, 1, 1)\n",
      "actual values shape: (59995, 1, 1)\n",
      "[1.753186e-07]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"predicted values shape: \" + str(model_vanilla_sigmoid_predictions[15:].shape))\n",
    "print(\"actual values shape: \" + str(y_scaled_key_list_stacked_tensor_dict_val[val_key[0]][:,:,0:1].shape))\n",
    "#print(\"residuals shape: \" + str(residuals[:,:,0:1].shape))\n",
    "\n",
    "#best shift=24\n",
    "shift=22\n",
    "start=0\n",
    "end=6000\n",
    "\n",
    "\n",
    "prediction_value_list = model_vanilla_sigmoid_predictions.flatten()\n",
    "actual_value_list = y_scaled_key_list_stacked_tensor_dict[train_key[0]][:,:,0:1].flatten()\n",
    "#residual_value_list = abs(actual_value_list[shift:] - prediction_value_list[0:len(actual_value_list)-shift])\n",
    "\n",
    "s = len(prediction_value_list[shift:])\n",
    "residual_value_list = abs(prediction_value_list[shift:] - actual_value_list[:s])\n",
    "sorted_residuals= np.sort(residual_value_list)\n",
    "print(sorted_residuals[0:1])\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(prediction_value_list[15:6000], color=\"blue\", label='predicted')\n",
    "plt.plot(actual_value_list[:6000], color=\"green\", label='actual')\n",
    "plt.plot(residual_value_list[:6000] , color=\"red\", label='residuals')\n",
    "#plt.axhline(y=sorted_residuals[-4] , linestyle='dashed' ,color=\"black\", label='Threshold')\n",
    "#plt.yticks([theta], label=\"test\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Actual and predicted Values\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#plt.figure(figsize=(20,10))\n",
    "#plt.plot(prediction_value_list[15:], color=\"blue\", label='predicted')\n",
    "#plt.plot(actual_value_list[:], color=\"green\", label='actual')\n",
    "#plt.plot(residual_value_list[:] , color=\"red\", label='residuals')\n",
    "#plt.axhline(y=sorted_residuals[-4] , linestyle='dashed' ,color=\"black\", label='Threshold')\n",
    "#plt.yticks([theta], label=\"test\")\n",
    "#plt.legend()\n",
    "#plt.xlabel(\"Steps\")\n",
    "#plt.ylabel(\"Value\")\n",
    "#plt.title(\"Actual and predicted Values\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#plt.figure(figsize=(20,10))\n",
    "#plt.plot(prediction_value_list[:], color=\"blue\", label='predicted')\n",
    "#plt.plot(actual_value_list[:], color=\"green\", label='actual')\n",
    "#plt.plot(residual_value_list[:] , color=\"red\", label='residuals')\n",
    "#plt.axhline(y=max(residual_value_list) , linestyle='dashed' ,color=\"black\", label='Theta=' + str(round(max(residual_value_list),3)))\n",
    "#plt.yticks([theta], label=\"test\")\n",
    "#plt.legend()\n",
    "#plt.xlabel(\"Steps\")\n",
    "#plt.ylabel(\"Value (scaled)\")\n",
    "#plt.title(\"Actual and predicted Values\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005328646296187177\n",
      "0.00020091213042183954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " c:\\users\\mooc\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\scipy\\stats\\morestats.py:1309: UserWarning:p-value may not be accurate for N > 5000.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2888779640197754, 0.0)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm, normaltest, shapiro\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "residual_value_list=list(residual_value_list)\n",
    "mean = np.mean(residual_value_list)\n",
    "cov = np.cov(residual_value_list)\n",
    "print(mean)\n",
    "print(cov)\n",
    "\n",
    "#k2 , p = normaltest(residual_value_list)\n",
    "\n",
    "#print(p)\n",
    "\n",
    "shapiro(residual_value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJIAAAJPCAYAAAAnnf7UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzs3XmcpVV97/vvM+1519DV1TNTAyKJYoygXgkSjYlBaVGEVzwhUUOuJsYbiS/kpuM1w1WJmIsvjYJoFPVexHCOQkCOkuHoMXgUNRhNQBImhabHmqv2/Iz3j2fvXbt6gKK7eu/VVZ/3P72r9u7aq6pWPc96vuu31mMlSZIIAAAAAAAAeAb2oBsAAAAAAACAEwNBEgAAAAAAAJaFIAkAAAAAAADLQpAEAAAAAACAZSFIAgAAAAAAwLIQJAEAAAAAAGBZCJIAAMCqs3v3br3whS887HN//dd/rTvvvFOSdNZZZ2lmZmbZX/f73/++Lr744mP+Os/GAw88oFe+8pXH5WsDAAA8W+6gGwAAANBPV111lVFfBwAA4ERCRRIAAFhTdu7cqZtvvnnJ5yYnJ3XxxRfr1ltvlSQ9/vjjuvLKK3XppZfqkksu0Ve+8pVn/Dqf+MQndOmll+qVr3xl9+tI0o033qjXvOY12rFjh971rndpcnJSkrR//379/u//vnbs2KGLL75Yn/3sZ7v/50tf+pJe/epX641vfKO+9KUvHfb72L17t17xilfoz/7sz3TJJZfoda97ne6///6j/8EAAAAsA0ESAABY0w4cOKC3vvWtevvb364rrrhCYRjqXe96l66++mrdcccd+uIXv6jPfe5z+vGPf/y0X+ekk07SHXfcoRtuuEHXXXedgiDQ7bffrm9/+9v6yle+orvvvltnnnmmdu7cKUl6z3veo5e85CW6++679bd/+7f66le/qq997Wv6j//4D91www364he/qNtvv12e5x3xPffu3avzzjtPd911l66++mr90R/9kYIgWNGfDwAAQC+CJAAAsKa97W1vUz6f144dOyRJTzzxhHbt2qX3vve9uuSSS/Rbv/Vbajabeuihh57263T2Tjr77LPl+76q1aruvfdeXXrppSoUCpKkN7/5zfre976nSqWif/3Xf9UVV1whSSqXy7r00kt177336r777tP555+v8fFxSdJv/MZvHPE9h4eHu+2+8MIL5TiOHn744WP7gQAAADwN9kgCAABr2vvf/3596lOf0uc//3ldeeWViqJI5XJZd911V/c1U1NTKpfLT1uV5LrpsMqyLElSkiSK47j7sSTFcawwDLvP9zrSc47jHPE9D34ujuOnfT0AAMCxoiIJAACsab/wC7+g6667TjfddJMeeeQRnXbaacrlct0gad++fbr44ov14IMPPuuvfcEFF+j2229XvV6XJN1yyy0677zzVC6X9YIXvKC7l1KlUtGdd96pl73sZTr//PP1ne98R/v375ck/d3f/d0Rv/7MzIzuvfdeSdI3v/lNeZ6n5zznOc+6nQAAAMtFRRIAAFiV6vW6XvjCFy753G233XbY127fvl1/8Ad/oGuuuUZf/vKX9clPflLXXnutPvvZzyoMQ1111VV60YtepO9///vPqg2XXXaZ9u3bp8svv1xxHOuUU07R9ddfL0m6/vrr9f73v1933HGHfN/Xjh07dOmll8qyLF1zzTV6y1veomKxqHPOOeeIXz+bzequu+7S9ddfr1wupxtvvJGKJAAAcFxZycF11QAAADDe7t27tWPHDv3oRz8adFMAAMAawtI2AAAAAAAALAsVSQAAAAAAAFgWKpIAAAAAAACwLARJAAAAAAAAWBaCJAAAAAAAACwLQRIAAAAAAACWxR10A5ZrdramOD7x9wUfGytpero66GYAR0QfhenoozgR0E9hOvooTEcfhelWQx+1bUujo8Vn/f9OmCApjpNVESRJWjXfB1Yv+ihMRx/FiYB+CtPRR2E6+ihMt1b7KEvbAAAAAAAAsCwESQAAAAAAAFgWgiQAAAAAAAAsC0ESAAAAAAAAloUgCQAAAAAAAMtCkAQAAAAAAIBlIUgCAAAAAADAshAkAQAAAAAAYFkIkgAAAAAAALAsBEkAAAAAAABYFoIkAAAAAAAALAtBEgAAAAAAAJaFIAkAAAAAAADLQpAEAAAAAACAZSFIAgAAAAAAwLIQJAEAAAAAAGBZCJIAAAAAAACwLARJAAAAAAAAWBaCJAAAAAAAACwLQRIAAAAAAACWxR10A3BkD/5sWjMLLRWyrsaGc2r6keIkUS7jyLVtZTOOMq6taiNQrRGo3oqUyzqKokRBGMlxbC3UfK0rZ1UuZGRZUtOP5AeRojhRnCRqtiJJ0nApo4zryHEsVeq+ojiRY9vKerYsy1IriOTY6b+ea8tzbAVRLD+IFcWJLEmuYyvj2XJsS00/fQ9JsiRlMo4sSWGUvj4IY+WzrpIkkePYcm0r/dexlCTSQs1XGCVqtEJlPFvFnKdS3lMYx0ri9OvM1VrKuI7yGUdOuz0Z19bEbEPlQkbFnKtKI5BrW4qTREPFjGqNUBtH85qptBREsbKeo7jdniiONVLKqtIINDaUk2Nbmq209NREVVvXFxUniVp+JM+zFUbp92ZbljzXViHrqtYMlHEdxUmiQs5Vq/2zVvv7n6u0VMp7ymddVRuBXMdWIeeq3gzl2Fb6uBUqjtP3cR1bliVlPEeWJUVR+jsr5jy1gki5jKN6M5Qk1VuhkkTyHEulQkZ+GGnXgaouOGezXOfQvDiOE1mWZFmW9kxWdWC2oSRJlCTSBaXcktdGcax903VNzDY0XMooihI5jqWZhZaSJFHGddJfcvt3nUiqNQJ5ri3bslTMe4qTRKWcp1ozUNjuN6W8p2Lekx9EqjUDOU76+vR3H3f/v2Wp/W/7sW3JDyL5Qfs1tiXHtpTxHLWCSEOFjBp+KCVSoxXKsiy5jqWxoZwm5xqK4kRJ+/saH8lrz2RNQ8WMxodzmq/5mq/52jCal2vbCqNYk/MNeY6jph9q47qCLEsKw1gZz5FjW6o1AsWJlPHSvjc2lNPoUFaVWiDbtjRazmpyrqE4TrRQ92VbliqNQMWcK8e2lM+6ymVcWZbkB5FKeU+2nfarejNUMZf2+zhO5Dm2as1Q9VaoreuLagWRZhaasm1LUZSoXMyoWvclSY1WpKxnK06kYt5VIetq92RNhayrdUNZJYk0s9CU49iq1H1lM47K+YxGShlFcaIDM3U5jq1s++eaz7ryg0jlgqdGK9JMpalC1lWpkFEQRqo2AnntvuY66c8ujBPFcaLTNg+plPeW9Kvdk1UlifTTvfMaG8qp3goVhHG3X7qOLduSsp6jKE5UbQTp79iPNL5+QVPTtfTvN0lkW9YhfdyypCCMFUaJSgVPtpX2G9u2tFD3Vc5nZFtSMe9prtpS3O7/SZLIcx0Vcq5ynqNsxlGSSBOzdUmW6q1A5XxGrmMpm3GU9RzNVX1lXFthHKvZ6hyn3e6x9sxtw9o7XZPnpH2k3go1XMyokPNkW1Ih58mxLYVxrOn5Zvf7GS1nVS54qjZCNfxQU3MNjZazCqNEnmsrihO57eNtGMUaLqa/u1YQaaHma7Sck6z237ukcjEjJZLjWJqrthRFiVzX7p4zas30eGtblnIZR7ZtKY7Tn0kp7ymIYrX89JgWxnG3nZZlqdoIND6c0/RCU8WcJ8+1Vcp7qtQDxUn6/p1jbhgliuL0HBBFicIoluNYCqNEWc9WK0j7exwnGiplFIaxgjCW0z5POI6lRvvYNzacUzHvadf+ipL2z6zph6rUg/bXT885I6WsPNdWrRnIUvozy2Uc1ZqBgjDWGduGtXG0sKQP1ZuhZqstTc2l5xTXsbrnvVYQKQzj7vdv21Ixl/6MOsfYdbMNzc835Nhpf5Yl+X6kdUPpscYPIm1ZX+z2/SRJ+60kzSy0ND6SU9Zz5Lm2Fuq+kkSKk0RhGGuomFG1kX4vI+WM4kRqNENZlpTNOKrUArXCSEmSaLiYlSWp0gikJNFCPVDDDzVcyMh1bbmOrSiOVamnf8OJpELW1fhoXrakqfmmLEtaqC8et/ww/V3mMo5KeU8Tsw1NzjfkOrZG2v2w87NoBZFK7T4et88T+Wx6vmu2Qo2Wc/JcWwdm6vJcW5vHCt3zSyuM0/NKIlXqvgo5V5ZlaaiQUTbjqN4MtHW8pLlKS7snqyoXMyq1j5mNVqgwTOSHaZ91bFtJ0j5325bCaPH359iWkiRR1kvHE1GU/o7rzVDZjNPtE7aV/p1uHC2oUg+6P7fNYwXFcaJsxtHuiZryWUd+mP5NDhUyaoVpf0mS9PeQ9RzVW8GSY5djW+l4yrJkt/uUEimK0+9hy/qiTt00dMixrlejFWr/TF1T803ZVvo9u64lx7alJD1HzFVaymddea6tp2YaqiykfbRzrDswU1e5mFEQxhpqjx2l9Hg6V03HMZakXDYdv0zMNfSLzxlXte6rGURpn3VsTS80VW0Ecmxb64dzqtR9WbalYs6VpfQY5IexhgqeHMdWxrWV8dJz7UItUK59fHUdS5V60B0DdsaHSSKV8mk/kiRL6Rh1w2henmNrrtbScDGrIIw1X2vJsS3N13ydsXVYCzVfrmtrdqGl4VJGTT9tdxDGKuRchVGsjOtoar6hQtaVbVvd459tp+0pZF0V854arVCTcw1t21CS70ftcbKjfNaRZVmqNwNFcXrMLhcyqtbT7218JK+p+YYq9aB9bEt/T51zQbURqJjzNFTIaL7Wao9TE9WbocoFT1GcqFzIKI4T2ZZUb4/n03GRVG0Eynnp2DSXcVXIuSrnPe1tnztHSllNzTc6Z0z5YdQdL8ZxelwuZF1lMo58P1LY/h5ynqO5Wks5z1Wlnv4cO8f8XMbRaZvTPtoZP4c9x0THsdqP03F80w+VSNoyVuyO/XJZV2EYd8d8pb0VTU5XpfZxKZNxZFuWZhaa6bVMu11Zz1G53ZcarVBZz1GSJJpZaMlx0uuAzeuKemzPvLZvGVIcp9dTs9WWLKXHgOFSRtPzTbXa5+7TNg9pz1RNSiQ/jDRf9bVuKKehoqes56jRCjU130yPS7La45703Fhuj+WqjUD1ZqjN69Pv0Up/3CrmPCVJov0zdWVcR5atJefUzvczPd9UnCSaq/oaKqRfM33eVqMVpX9PtqWsm34vQRjJsqzueGzfdE1b1pc0Ws5qvj3W8VxHI6WMJmYbyrSPRZ6TjinWlXMKwkixpIxra6EWKEkSrRtKz+9JknT/PnKZdPzjubY81+6O4ZJESpRo/XBec5WWqs1A64dyWj+SV70ZKoxiTcw2lChJ/76s9BqoFUQKo0S1RqByIaMgimVbUpxIw8WMHNvS2HBOs5WWpuebavihXNtWLpuOx4MwUSHrqBlEiqL0Wqwzhmz66Xk2TtLxv221/97afdJzbVmSRkpZ1ZqBKvX0Om3juryiKNHuyao2jOY1PpJXMbd0PLuWWUnnrGq46emq4viEaOrTGh8va3Ky8oyv+9m+BX3g/72/Dy3CarZxXUHvvvwcbei5QNp1oKK/+Py/KJtx9L43n6u/+NwPuqFfx1knjejhp+b63VysUueeNa4/eMPzux9PzTX0f37qvgG2CFhq81hB177tpd2PK3Vf/9dnvq9qIxhgq4ClPvLO8zVazh7y+fmar4/+1x9r10R1AK0CFv35W89TPuto56e/N+imAMdFPuvqxne/vPvxcq/tTWbblsbGSs/6/1GRZKjH9sxLSge3l114umYqrbRqwHOU8RzZluSHaaLaCiLVW2lVy1Aho/UjOU3ONjRSzmrXgaqGCp4ymXSmZXwkr+l2eu60Z+QsWd203lJaITFTaapaTysAMp6tZiudhclnnW4Sm1YOtZTLunKdtEqp0vBVa4QaKacVTp1Ueb7WUiKlM4VRnFYR2Jb8IJ3xCON0ltdpVw9lPUeObasVhJqYbSgIY42Us4qiRMPFdBY1l0mrrxqttEKj0UpnDmvt2b5S3lO9FaqQc7uzYsX2404VUWfGY/9MXbVGqPUjue5MeD7rau9UTbmsm1YPtGelOgm3bVlqBqHCKJ0hTpTOBjq2pcm5dGZ2vuZrvD1j3plFr7fS2fIt7Son17bVCtKZ406g0/Qj5TNpqr6unOtWu7hOmvrP19KZiYyXViXNVJrKuE636mP3ZFXffXC/DszUtfPT39Mf/+YLddbJo5KkHz48KUlq+ZFuuP3fFcWJfvNVZ2r9SF4/eOiAvvfQgUNCpFM3lXX61mH96NFJuY6t119wmtYP5yWls6i9kqRdWVPwFMeJKvVAC3W/+7qx4Zxc29be6ZqafjqbUsylsyz1ZqggSquKhktZJe3Kuc7sRhynM/LzVV9DRU/5jKsoTrRroqJ1Q+ns/WO757VQ83X61iFtGC0ojGLtnaopihNtHS+mFUF1X/f95IBO2lDS5rGCWkGkRivS9HyzW+0zU2nKUlpRtGWsoP/cNadCzlUp76npR2q0Qs1WWt3Z2g0jecWJ9OjuOZ20oaRyIdOdYZ6ca2jr+qLyWVfTC02V2pVY64ZyarSrJ2zLUtMPVch5CoJIuawrz7HV9MP2LG1aFVhrBvKDWBnPViHnqdiePX1s97xO3TzUrSDKuHb7b9NSsxWp4ad/S1vHiwrDdHbmkafmdNqWIa0r55QkiR746Yw2jRVkKa1CKOXTrx+EsWzb0p6pmk7ZWNZwKZ0F9YNIrfbfbSnvyXVt+UGkPVM1nbShpKFCRh/9b/+m2Uqr2z+afrgkRHrjhdsVhLFmFlp6zkkjGhtKqwJzmXR26ol9C9o2XmrP4Lkq5DxtHC9rarqaziBZVrfKQEv/UcuPuseAZivUSCmrhbqfVs+MpDNlB2YbKuZc5bNudybKa1dpdCo4G61Q0wtNbRor6KkD6azUaDmnv//+k9qyvqiNowWtH84piNJKhSCMtXe6ps1jhXRGtl254rm2phda2jxWkOfaiuNEbnv2NIxiDRWz2jiaV5wkenJ/RdMLTY0N5TRUzMiyLO2frklKK06GChl5bnrsGC2nFZR7pqp6dPe8XvSccc1UWhopZdrVFUm3Gmu20lI+62jzWFFJIs3XWirn05m/OEmr3jpVSZ0fa5wk2t2+QC0XM8pnHBXzniyrM7uc6P7/nFAQxjp1c1mb1hXU9CNNLzS1YSTfneX+2f4FjRSzKhU8zSw0tav9szx1U1lxkqjRDLVvpq6TN5QVxbH+1wP7tGG0oDO2DKlcSKtcKg1f9WZagZlWqaUVoraVVhDtmarpjK3DchxL28ZLsi1Ljzw1p+mFpjKeo4War42jecmSTt5Y1kgpq9v+xyP6yROzCqO429b/9j8f6379X3/xydo8VuxWrgZhrNPaf2t+mM6q57Ounpqoat90TadvGdZQMaNSKafZ+bqm55vaM5n+TaTHsbRiLePamlpodivJdk9UtXW8qInZhvwg1uaxgmrNUHa7iqPzPfth1A0TOuce10n7Qud1tWYoz0lnb5/cX9FwKa3M8Ny02mb9SF6FnKtGM9TuyZrGR/KybWmokNFCzU+rKBN1qz7rzUDDxayKOVe1ZtgdJ4RhnFYlFTLyHDudIXbS48D64fR8GkRpdcXYUE5Wu+o0rXZLxwG1RiDPs/XIU/P62b4FveD0MdWaoU7ZWNZIKZNWg4RplVqnOvuBn07LdWytK2dVb5/7903VVci5Gi1nlcukY5VORXMYpRVKnptWeIVR3J2x71RVZD1HfpC2dWq+oU1jBZXynrx2dfdMpSXbSit34naVW6Xu68EnZrRxtNA+HkmubWvz+oL2T9c1OdfoVhyW8l76swnTY/m6ck6FXHoOm6+2NFRM/17VHgt0xmR+GOknP5vRV7/zhJ6aqHZ/90mS6MO3/qse2T3fPZ66jqXnnjyqSy/cLtuy9ODPZrRvqqZMJq1A8YNIlXqgkVJGY8M5lUs5PfLkjEbLWbX8KK08qwXKZx2dumkorRZuV9QdmE37ciKpkHO1Zawox7b048emdNrmIeXbldmd8dHEXEP7p+s6+5RRZTOOxobSyrOZhaYqjUBRlGjjunx75t9SrRHKc20NFzOaa1fsxXEiWVIu46pS87V7qqbnbBvWmdtGtFD3278vX/VWmFZSFTIaG0orMbOeo/3TdZULnmYrLZ19yqge37sgq12lWS5kVKn7qtQDnXXyiGwrPV/PLDS7Y8lS3tOG0YLiJP277VSeTC805YeRWn6kzWNFPbp7TrlMunpg/XBOYZRWcmc9R5vHCto7lR67G35amfbdn+zXLz1/szauy2uokFGSpFWeUZS+T8MPVWtXoqr9d7h9y5Bs29JP9y6omHO1cbSgh56cVb2Zns/WDeVUyLrditS5akvFnKdC+xy+e7KqTLvKpXPueeiJWb3q3G1ybEtPTVS1fjjX/j2GaWWVk1aZTMw1tGEkr0d2z+knP5vRr557kibm6rJk6Yxtw6rWA+2equqe7+3SQ0/O6JFd6RjyV889Sfmso1M2lWXJ0pMHKnrOSSOar7ZULmY0M9/Uv/90WpvWpX9r6bEr0M+fuk4NP9RP9y7opM3D2r1/XpvXFbVhNL+k6rYzpsp6TlqhaaXXGfum68plHG3fOiTHSitZ97XHnPX28atTZZPPOpqYbWiomFamjQ3nVM57evipOVXqgTaM5OWH6c/Cc209tmdeo+WsPNfRk/sXtH3LsCp1Xz9/2jqFYSw/jLV+OKd903X96NEpbdtQVBjG2jxW1PhIvjvOn5ita77mq5BNq8XiRBoqeMplXDmOpcnZhg7MNnTSxpKynqNv/WiPTtqQXuifvnVY+6ZqkiUNF7N6bM+ctqwvdiutw3Zfvfs7T2h8OK/Ttw2p0QyVz7naur6k+VpLeyZr3cqgzWNFPfeUETVbkXZPVjVf81XMeRopp39Puyeq2j1V0wXnbFajGarpRxofyatS9/VP9+/WC84Yk+ukFXdplX76Pd757Z/pwl/Yok3r0rH45FyjO64ZH80rCNLj8lAxo6cmqgqiWGEYa8NoQSdtLKnUrvKdq6QVhY/vXVAp7ymXdVTKeXp415zWDWV16qayoiRRoxVp31RNidLK16FiRrsmqjpz67DWDaXHzf0zdT01UdV5z92QVqM6lpSkKzu+/9ABnbF1WNmM0z1H/Pf7ntRspaVXvWib9k7V9MNHJtVoV0GCiqS+W25qefPXHtIDj0/ro3/4S93BOvBsXHndN7uP33rRc/XyF2yRJN3yDw/rB/9xQLX20pCt40V94Hdf0n3tdD3QNR//tiTpo3/4Syrl3bQsHjgKN935oHZNVPWht6fVHt/60R79f//wsCTpcztfeVRfczXM/sAc//QvT+lvv/GoPn7VBd0w/kNf/KEe3T2vm//4FUd9DqafYqXsnqzqz27+gd7x+ufpvOdukJQuHbr6xu90X3P2KaO65r+88Fl9XfooVtLOT92nbRtK2nWgoo3rCrr6N37hmL8mfRSmeGzPvP7ylh9KWjp+XQ19lIqkVWS+5us7D+xPE3xCJKwA11nsR00/3e/mpA0l/eeuOa0fWron0nNPWadr3/YSbRwtyLbpfzg2Gc9W2N6jRJJmKk1J0p++5dxBNQlYIpdN98BptsJukDRXbenFZ2/gHAwj5Nr7NDX9xZnwT/7dA0te8/oLTutrm4CDbd8ypO89dECS9Mpf3Dbg1gArq/daCinKDAz0P+5/SpK6SwmAYxVFi9V8rSBKy8yH0wDpcPstbB4rEiJhRXSW93U8smtOp28d6m7ICQxa1mtfpLdvjlBvhpqaa2rzWHGQzQK6cpl03rfpL4bynXP0xnXpHojbt3BMxWCNlBbHk+ecPjbAlgAr73A3L1rrqEgyUKY9qN15xS8OuCVYLfz2Hi1SO0hq361OktYdVJEErCTbtpcsS642Q20eKzzN/wD6q7NXTidwf/ipWSWSnnvyyABbBSzq9NHOHbCk9K5P28ZL2nnFC9VoRSxBx8Dlc+ll5XAxoy3rCeKxuhAkHYogyUC19gafp28dHnRTsErsn653H0dRLNe2dPHLTlWSSK/4xa0DbBlWO8daWpFUa9/SGDCF07lIj9OL9P0z6fGys7EpMGid6qPe6uKp+fSmKoWcpwLHVBigmOOyEquX27NSo1L3VS5kBtgaMxCtGWih7muIzokV9I1/3a2fPDEjSYrad8rZPFbU21/381zU47iy23dilNK7DFXbd8IBTNHZ96BzkT4x21Apz8U5zNG562mnujOKY+2erOnkDeVBNgtYorOX13zNH3BLgJXn9FQkzVfp4xJBkpEqNV9DRYIkrKyvffcJSelA1GH/I/RJ7x5JrSBSFCcq5pm1hDncg5YNVeqBhjkHwyC2bcmSFo+l7b2ShgqEnTBH527AwGrUOy6o1AmSJIIkI1WYscdx8J+75iSlA1GHdb7oE9u2urPoD/40rYqjCg4mcdoVSWG7IqneDFRgiQYMY/eE8p1Nt3NZ+inMcf7zNumc08f0kXeeP+imACvOti29783pHYeDnv3q1jKuJg1Ub4ZcaOG4ieNENre0Rp/0Xvx88s4HJREkwSyu3dlsOx0Ycg6GiRxnMZRvdIKk9lIiwASFnKc/uvwFh70bMLAaeG46XghCgiSJIMlItWbA0g8cF3GSpBVJLG1Dnzi2pSRRd58kSSwbglE6eySF7Yv0eitUnkoPGKZ3mXCzlS4hIkgCgP7pjBeoSEr1NUi6++679ZrXvEa/9mu/pltvvbWfb33CiOJYjVbEbCiOi/mq391sG+gH+6BNYiXp5I3cDQvmOHiPpFoz5O5DMI5tWYradxbsLm3L0E8BoF+oSFqqb2egAwcO6KMf/ajuuOMOZTIZvelNb9JLXvISnXHGGf1qwgmh3t6ojv0ZcDxMzNbTzbYdgiT0R6f6rXPSfcMFpynjMYsOcyzukRQrThI1WyHnYBjHcexuIN/0qUgCgH7zOhNPBEmS+liR9N3vflcvfelLNTIyokKhoFe/+tX6+7//+369/QmjcyeOHBdaOA6qjTBd2sYeSeiTTlfz2yddz+XYBrN0KpKiKFGjFSpRutcHYBLncJttEyQBQN90K5Ki5BleuTb0LUiamJjQ+Ph49+MNGzYgwsgTAAAgAElEQVTowIED/Xr7E0ZnzWWnowIrqdYMFMUxS9vQN+lNq6UgSC98XKrhYJjepW3dqmD2SIJh0qVtBwdJ9FMA6JfFpW3RgFtihr6dgeI4ltVTBZEkyZKPn8nY2OrZU2N8vHzE56pBGiStW1d82tcBz8an/+RX9Hsf+oa8rCvJUrGYfdr+Rd/DSim3796SK6b/rl+hYxt9FCul2AwkSbl8Rpl8uhH85o1l+imMksk48jxX4+Nl7ZttSJK2bR1R9hgr2OmjMB19FKboLC/OZL0l/XKt9tG+BUmbNm3S/fff3/14cnJSGzZsWPb/n56uLtms9UQ1Pl7W5GTliM9PTFYlSY1a62lfBzwbcfsOL3NzDQVhpKAVHrF/PVMfBZ6NWs2XJO3aMydJSsLomPsXfRQrqbN/1/xCQ3v3zUuSwlZAP4VZkkS1hq/JyYr2TlRkSZqfrT2rSdmD0UdhOvooTOM6luYXmt1+uRr6qG1bR1W007f1Uy972ct03333aWZmRo1GQ//4j/+ol7/85f16+xNG564xLkvbsIJcd3HD4zjhrm3on841To0bCcBQi5ttJz39lD2SYBbbtroTqjOVls47e8MxhUgAgGfPdWzu2tbWtxH9xo0b9e53v1tvfvObFQSBLrvsMp1zzjn9evsTRqdjdnaFB1aCY9tybEtBFCuKku6dtIDjzbY6IWZnjySObTCLbVlybCvdI6nFHkkwk9MTJPlBdMxL2gAAz57n2t09jde6vo6UduzYoR07dvTzLU84bLaNlXLqprKe2L9Yaum6aYIexVQkof/C9h0uHDbbhoEcx1IUJdp1ID1mUjkH0zi2pTBOx4h+ECtDkAQAfZdWhxIkSX1c2oblCdsVScza41hd/sunL/nYa5dixjEVSeifTlfrLNt1bI5tMI9r2wqjWI88Naf1wznlqUiCYRzbVhwnarRC1Vuhpuebg24SAKw5aXXooFthBkb0hqEiCSvl4KqjjGfLDyIlEkES+qazh0enIolqOJjIdSyFcSI/iHXGtuFBNwc4RGePpE6l8WN75gfcIgBYe2zLUrQKbgC2EkgrDLNYkcTFFo7NwZtweo6tZpDuU8PFPPrFOqgiyaXvwUCOk1YktcJIGSZyYKB0aVvSrfK88jVnD7ZBALAG2ZalJCFIkgiSjNOtSGJpG45RZ5PjYnuvD8+11fTTIImKJPRLtyIp7Cxto+/BPK5jKYridO8Zl71nYB7HTvfxagXpsbRc4M6CANBvlm0pJkiSRJBknM7FFkvbcKysdhfqXMh7rq0WQRL6rNPTOpvEOoTkMJDr2AqiREEYyfPoozCP3b548duVxdy1DQD6r/cOmmsdoyXDdJYecTcOHCv7cEvbfJa2ob/YIwknAs+xFQSRwiihIhhGsi1LSZzIDzvjRPopAPSbbYk9kto4Cxmm6UdyHZu7tuGYHRIkubaafiiJiiT0T3ePJJa2wWCea6vRCruPAdMsViSlx1ImHAGg/9I9kgbdCjMwWjJM04+UyzA4wLHrXMB3/vVch4ok9J3drUgiSIK5PNdWvRMkMZEDA3VmwVudynX28gKAvmOPpEWMlgxTbQQq5tlAEceuExZ1Lttd1+4OQB2bP330SfeubelJlztSwkQ/3bug3ZM1SemxEjBNWpGk7h5JLG0DgP7r7JE0W2nptm88uqaXuXEWMsxCzddwMTPoZmAVsA8qSfIcWwHLi9Bn3aVtnc22CTFhIL99bJSoSIKZbNtSHMdqBpEc22ILBAAYANuyFMWJPn/Pf+gf/+UpPfj41KCbNDCchQzT8iPlWdqGFXDw8rXefT9Y2oZ+6S5ta1+oW3Q9GOhVL9rWfUxFEkxkW5biWKo3QxVz7qCbAwBrkm1JSZIoalfaJ2t4mRujJcOEccwsE1ZE59bAneVsvUESFUnol967tjm21f0YMMnztq/rPqYiCSbqbLZda4ZsgQAAA2K3l7ZBYkrDMGEYMxuKFTFSyuic08f0yy/cKokgCYPR6WlhFMthfyQYqnfjYs7BMFFakZSo1ghUoCIJAAbCtq10cnTQDTEAZyLDhFEsl4t8rADLsvRHl7+g+3GGpW0YgO4eSVHM/kgwltezcbFHkAQDdSqS6s1QwyX20gSAQejskURdKEvbjBNECbOhOC6oSMIgHLy0DTBRb0USS9tgIqdTkdQM2CMJAAakE+qDIMk4UcQeSTg+ei+OqEhCv3QqkoIoJkiCsXorNjkHw0SWre4eSYUcc+EAMAi2ZSlhjyRJLG0zDrP2OF7GhnPdx/Qx9IvV3iUpCGOWDMFYGa+nIol+CgPZVrovRxBGVCQBwIAcXJG0louTOBMZJk4SqkVwXGzfMtx9zF416JdORZIfRFR6wFi94VHWo5/CPI5tKQhjSdLYUO4ZXg0AOB72z9S1d6qmbKYpSYrWcHUSoyXDxHEim9tj4zgo5RdzY8JK9EtnjyQqkmCyzJIgiXuxwDy95+1TNpUH2BIAWLv2TtUkSS0/kpRuS7NWMao3TJIszuADK6m3GoSlbeiXTlfzQyqSYK7ekDNDkAQDWT2Dw3KBu7YBgAnWbj0SQZJx4oSKJBwfvYNQKpLQN92lbVQkwVy9x0cqkmCi3gkgx+EcDgAmWMt7JDGqN0jS7olc5ON4oyIJ/dK5QI/iRB4XPzgBcA6GiXq7pUd1JwBgwDgTGaSzAzwFSTjeCJLQL70nGc+l0gMAjkZvwMkyYQAwxdotSeJMZJC4vVcXS9twvFkESeiT3iVDLhVJAHBUbJa2AYBxWNoGI7C0Df2ylu8wgP7qzcXZIwkAjk7vJCMTjgAwGAcffddwjkSQZBKWtuF4u/Tl2yVJxZw34JZgreitSCJIgsled/6p+i+/cuagmwEcFpOMADB4h1SEruEkyR10A7CIpW043l77v52iX3nRNuWz/OmjP3oPZ+zrAZO9/oLtg24CcESMDQFg8NJj8WJ6lKzhJIlRvUE6HZHBAo4Xy7IIkdBXVCQBwLHrVCQxQgSAwTl4n1n2SIIR4pilbQBWl97DGRVJAHB0OndbZYkbAAwOBR+LGNUbpJ0jMUgAsGpQkQQAx65zKLW4iAGAgXEOvk6nIgkm6N61jUECgFViyV3bqEgCgKPSGRvaHEYBYGAOzZHWbpLE6cggLG0DsNr0BuMuFUkAcFQ6s+AE8gAwOOyRtIizkUFiKpIArGJcAAHA0else1DKewNuCQCsXQdfp6/hHIkgySQJeyQBWGWWLG2jIgkAjkrn4iXHnVcBYGAOuUxfwyVJjOoN0qlIoiAJwGqxZGkbFUkAcFQ6k4yHbPQKAOgbbniwiFG9QTp7JLG0DcBqQUUSABy7ztiQISIADM7Bx+C1W49EkGSUTmUcSSeA1aL3BEtFEgAcnc7d2phsBIDBscRm2x2M6g3S3WybsmUAq0Sn0lKiIgkAjlZnbMhkIwAMzqGH4LWbJDGqN0h3s23GCABWiTghSAKAY9UJkNgjCQAG6OC7tq3dHIkgySSdmXtmmwCsFnG8+DifcQbXEAA4kTHZCAADx03bFhEkGeDK676p2//58cWlbQRJAFaJ3qVthZw3wJYAwInLDyNJksNecwAwMFymL+JsNGBJOzz62n1PLi5t47cCYJWIekqS8lkqkgDgaNSboSSpmHMH3BIAWLsOXTm0dkuSiCwGrLccrlORxNI2AKvFyRvL3cdZjyAJAI5G51h63tkbB9wSAFi7Dr5MX8tL25jWGLDejWg7S0BY2gZgtchnF08zhOQAcHRO2lDS31zzy3JZ2gYAA2MdtEvSGs6RCJIGLekJkpLuHkmDag0ArLzfeOUZeuSpuUE3AwBOaIRIADBYVCQtIkgasN47GnX2pGXWHsBq8uoXn6xXv/jkQTcDAAAAOGqHXqav3SSJqY0+a/qhfvKzme7HS5a2dSqSKEkCAAAAAMAYhyxtW7s5EkFSv9381Z/oI//1x3pqoirpSEvbCJIAAAAAAIB5CJL6bGK2LkmaWWhKWlzOJi0ucyNHAgAAAADAXGu4IIkgqd9KOU+SVG+FkpYubetUJLFHEgAAAAAABjn4Mn0Nr20jSOozz0t/5GGUlh8lPSVJnUfkSAAAAAAAmGvtxkgESX3X2f+oE172Lm1brEjqd6sAAAAAAMCRjJaySz5ewwVJBEn91rkjW9SpSOrtfe2HB+8GDwAAAAAABud3Lz576SfWcJJEkNRnnYqkW/7xEUlL90jqVCdRkQQAAAAAgDmK7f2OQZDUd52KpI7DL20jSQIAAAAAwFRrtx6JIKnveoOkOE6WLm1rI0cCAAAAAMBca3hlG0FSv9k9KdF8zVcc9y5ta1ck9b1VAAAAAABguZI1XJNEkNRnvdVG9WZw2BSTpW0AAAAAABhs7eZIBEmD1PCjgzbb7uyRNKgWAQAAAACAZ7KGcySCpH7rrTZqtsIlFUlJcuhrAAAAAAAATEGQ1GflwuItAxt+tGSPJHWDpD43CgAAAAAALBubbaNvejtboxUu2aCLzbYBAAAAADgRrN0kiSCpz3qDo2YrVBwf+hqWtgEAAAAAYC4qktA3SyqS/EjJ4Tbb7nejAAAAAADAshEkoW86nS3j2mr64ZK7tonNtgEAAAAAOAGs3SSJIKnPOhVI+ayrRivSYXIkNtsGAAAAAMBgVCShb7oVSZ4tP1x617bu0jaSJAAAAAAAYCCCpD5LkkSWJMe2FcfJkj2SFpe2DaRpAAAAAABgGdZwQRJBUr8lkmRJjm0pihPFvTkSm20DAAAAAGC8ZA2vbSNI6rMkSWRblmzbUhQtrUha3COJKAkAAAAAABNlM45KeW/QzRgYd9ANWGs6uZFtW4qTZMld2xKWtgEAAAAAYKxiztWf/855OuPUMc3N1gfdnIEgSOqjf398Wl/55qNyHVvuYZa2sdk2AAAAAABm+tDbX6pCzlW5kJHnOoNuzsAQJPXRgz+dlpRWHKVL22IlMZttAwAAAABguo3rCoNughHYI6mPbDtNiKz2ZttxfPDSNjbbBgAAAAAA5iJI6qNukCSre9e27/3kQPd5NtsGAAAAAAAmY2lbHzk9FUm2bevxvbNLnp+ttAbRLAAAAAAAgGWhIqmPbGvp0raDPfTE7JLXAQAAAAAAmIQgqY864ZFtWYcNkjr5ETkSAAAAAAAwEUFSH9k94dGPHp065HmCJAAAAAAAYDKCpD7qrULqvVtbh6XO0jeSJAAAAAAAYB6CpD6yD7OcrRf5EQAAAAAAMBlBUh/Z9tNXHHU+zWbbAAAAAADARARJfXS4DbaXWryrGwAAAAAAgGlWPEj64Q9/qMsuu0yXXHKJ3vKWt2jPnj2SpIWFBb397W/XRRddpCuuuEKTk5Mr/dbGe6albXZ3s22SJAAAAAAAYJ4VD5KuueYaffCDH9Rdd92lHTt26IMf/KAk6WMf+5jOPfdc3XPPPbr88st17bXXrvRbG895poDI6tQkAQAAAAAAmGdFgyTf93XVVVfpuc99riTprLPO0r59+yRJ3/rWt7Rjxw5J0sUXX6x7771XQRCs5Nsbb3GPpMM/b5EkAQAAAAAAg61okJTJZHTJJZdIkuI41g033KBXvepVkqSJiQmNj49LklzXValU0szMzEq+vfF6l7ad//xNh77AYqNtAAAAAABgLvdo/+M999yjD33oQ0s+t337dn3hC1+Q7/vauXOnwjDU7/3e7x32/ydJIttefo41NlY62qYaY3R4QZLk2LYuf9VZ+s4D+5c8n824sixpfLw8iOYBXfRBmI4+ihMB/RSmo4/CdPRRmG6t9tGjDpIuuugiXXTRRYd8vlar6R3veIdGRkZ00003yfM8SdKGDRs0NTWlTZs2KQxD1Wo1jYyMLPv9pqeriuPkaJtrhGq1KUmKk0QLC41Dng+CSJKlyclKn1sGLBofL9MHYTT6KE4E9FOYjj4K09FHYbrV0Edt2zqqop3jstn2Kaecoo997GPKZDLdz1944YW68847JUlf//rXde6553ZDprXC6eyR1PN4iSTRM9zYDQAAAAAAYGCOuiLpcB566CF94xvf0BlnnKE3vOENktJKpM985jO66qqrtHPnTr32ta9VuVzW9ddfv5JvfULo3SPJcQ7N8BzHZrNtAAAAAABgrBUNkn7u535ODz/88GGfGxkZ0ac+9amVfLsTTrcKyZLcnlDp/7j0+brhjgfSp9hsGwAAAAAAGGrFl7bhyJZUJB1mDVuSJBQkAQAAAAAAYxEk9dGSPZIOs7QtTqhIAgAAAAAA5iJI6qNORVKipRVJnUcJm20DAAAAAACDWUmSJINuxHK88Y2Xa//+/d2PX/e6N+jKK9+mer2u3/zNyw55/ZvedIXe9KYrND09rd/93d8+5Pm3vvV39frXv1F79uzWO9/59kOef8c7/lCvfvVFeuyxR/We91x1yPPvfvc1uvDCV+iBB/5df/qnOw95/r3v/XO9+MUv0Q9+8H395V/+35KkRivUkwcqch1bt/zNjfrEP8xo8sl/U+Pxr2v3VFX5rCs/iHTmthFdf/1f64wzztQ//MM9uummTxzy9W+88W+0des23Xnn7frCF24+5Pmbb75FY2Njuu22W3Xbbbce8vyXvvQVFQoFfe5zn9FXv/p3hzx/551fb7/Px/VP//T3S57L5XK67bY7JEkf+ciH9e1v//OS50dH1+nzn/+iJOmDH/wL3X//D5Y8v3nzFt1002clSe973x/rwQcfWPL86aefoY985OOSpKuvfpcef/yxJc8/73nP1wc/+GFJ0jve8b9r3769S54/99wX633v+wtJ0u/8zm9pdnZmyfMXXHChrr76jyVJb3rTpWo2m0ue/9Vf/XW9853vkiS9/vWv0cFOxL7X6wMfuE7Pf/45+ud//p/66Ef/n0Oe/9znPqt167bQ9+h7hzx/vPveco973/zm1/Xxj99wyPP0vb+QRN87nn3v2Rz3PM9REESS6Hv0vf72vV5P1/c8z9GXv3x3+33oewej7w1+vDc+XtbOne+j7x2EvmfOeO/f/u2H3XO9dGL2vU2bNun227+sZ4uKpD7qXbbmuJZO2zyk17z05INf1d9GAQAAAAAALNMJU5E0PV1VHJ8QTT2i3ZNV/dnNP9BoOauPvPP87ud/9MikPnHHA9q+ZUiTcw399bsuGGArsdaNj5c1OVkZdDOAI6KP4kRAP4Xp6KMwHX0UplsNfdS2LY2NlZ79/zsObcERdDfbPkLRUcJm2wAAAAAAwGAESX3UuVPbkaKiJElY2AYAAAAAAIxFkNRHh96nbam0IqlfrQEAAAAAAHh2CJL6qLMd1RGXtilhaRsAAAAAADAWQVIf5bKuJOnMbSOHfZ6KJAAAAAAAYDJ30A1YS4YKGX3iPa9QRvHSJ9rhEXskAQAAAAAAk1GR1Genbh6S5zqHfY67tgEAAAAAAJMRJBkkThKWtgEAAAAAAGMRJBmGiiQAAAAAAGAqgiSDxInYIwkAAAAAABiLIMkkSUJFEgAAAAAAMBZBkkHSzbYH3QoAAAAAAIDDI0gySExFEgAAAAAAMBhBkkGoSAIAAAAAACYjSDKA1d5iO1HSfQwAAAAAAGAagiSDUJEEAAAAAABMRpBkkCRJCJIAAAAAAICxCJIMklYkkSQBAAAAAAAzESQZJEkSdkgCAAAAAADGIkgySCIqkgAAAAAAgLkIkgySJJJNjgQAAAAAAAxFkGSQJEnE2jYAAAAAAGAqgiSDsNk2AAAAAAAwGUGSQeIk4RcCAAAAAACMRW5hgnYRUpLutj3QpgAAAAAAABwJQZJBEiVstg0AAAAAAIxFkGQQ9kgCAAAAAAAmI0gySJIkg24CAAAAAADAEREkGSRJxNI2AAAAAABgLIIkg7C0DQAAAAAAmIwgySBJknTv4AYAAAAAAGAagiSDJJJsKpIAAAAAAIChCJIMQ4wEAAAAAABMRZBkgN7wiD2SAAAAAACAqQiSDEOOBAAAAAAATEWQBAAAAAAAgGUhSDIMm20DAAAAAABTESQZhhwJAAAAAACYiiDJNCRJAAAAAADAUARJhrHJkQAAAAAAgKEIkgxjUZEEAAAAAAAMRZBkGGIkAAAAAABgKoIkA/QWIVGQBAAAAAAATEWQZBiWtgEAAAAAAFMRJBmGGAkAAAAAAJiKIMkwVCQBAAAAAABTESQZhhwJAAAAAACYiiDJMARJAAAAAADAVARJhmFpGwAAAAAAMBVBkmEIkgAAAAAAgKkIkgxDjAQAAAAAAExFkGSExfiIgiQAAAAAAGAqgiTDsLQNAAAAAACYiiDJMMRIAAAAAADAVARJhqEiCQAAAAAAmIogyTDkSAAAAAAAwFQESYYhSAIAAAAAAKYiSDKMxS5JAAAAAADAUARJhqEiCQAAAAAAmIogyTQESQAAAAAAwFAESQborUKyKUkCAAAAAACGIkgCAAAAAADAshAkGYaCJAAAAAAAYCqCJMNw1zYAAAAAAGAqgiTDUJEEAAAAAABMRZAEAAAAAACAZSFIMoxFSRIAAAAAADAUQZJhyJEAAAAAAICpCJIMQ44EAAAAAABMRZBkgCXhESVJAAAAAADAUARJhrHJkQAAAAAAgKEIkgAAAAAAALAsBEmG4a5tAAAAAADAVARJhiFGAgAAAAAApiJIMgwVSQAAAAAAwFQESQAAAAAAAFgWgiTDUJAEAAAAAABMddyCpIceekjPe97zuh/7vq9rrrlGF110kd7whjfo8ccfP15vfUJjaRsAAAAAADDVcQmSGo2GPvCBDygIgu7nbrnlFuXzed1zzz1673vfqz/5kz85Hm99YrIO+xAAAAAAAMAoxyVIuu666/SWt7xlyee+9a1v6XWve50k6bzzztPMzIz27t17PN7+xEaSBAAAAAAADLXiQdI3vvENNZtN/fqv//qSz09MTGh8fLz78fj4uPbv37/Sb3/CI0cCAAAAAACmco/2P95zzz360Ic+tORz27dvV7Va1Re+8IVDXp8kyZL9f5IkkW0vP8caGysdbVONMz5eXvLx8Eyj+3ionDvkeaDf6IMwHX0UJwL6KUxHH4Xp6KMw3Vrto0cdJF100UW66KKLlnzuy1/+sj796U/riiuu6H7ukksu0a233qqNGzdqYmJCJ598siRpampKGzZsWPb7TU9XFcfJ0TbXGOPjZU1OVpZ8bn6+3n1crbUOeR7op8P1UcAk9FGcCOinMB19FKajj8J0q6GP2rZ1VEU7Rx0kHc7ll1+uyy+/vPvxWWedpbvuukuSdOGFF+quu+7Sueeeq/vvv1/ZbFZbtmxZybdfFVjaBgAAAAAATHVcNts+nN/+7d+W7/t67Wtfq2uvvVZ/9Vd/1a+3PrFYREkAAAAAAMBMK1qRdLCHH364+zibzerDH/7w8Xy7VYEcCQAAAAAAmKpvFUlYHnIkAAAAAABgKoIkA1g98ZFFSRIAAAAAADAUQZJhiJEAAAAAAICpCJJMQ5IEAAAAAAAMRZBkGIskCQAAAAAAGIogyTBskQQAAAAAAExFkGQYgiQAAAAAAGAqgiTDsLQNAAAAAACYiiDJNORIAAAAAADAUARJhmFpGwAAAAAAMBVBkgms3ockSQAAAAAAwEwESYahIgkAAAAAAJiKIAkAAAAAAADLQpBkGJuSJAAAAAAAYCiCJAAAAAAAACwLQZJhKEgCAAAAAACmIkgyjEWSBAAAAAAADEWQZBhiJAAAAAAAYCqCJNOQJAEAAAAAAEMRJBnAWvKYJAkAAAAAAJiJIMkwbJEEAAAAAABMRZBkGIIkAAAAAABgKoIk45AkAQAAAAAAMxEkGcYmRwIAAAAAAIYiSDINQRIAAAAAADAUQZJhuGsbAAAAAAAwFUGSaciRAAAAAACAoQiSDMMvBAAAAAAAmIrcwgBLipAsSpIAAAAAAICZCJIMQ4wEAAAAAABMRZBkGAqSAAAAAACAqQiSDGORJAEAAAAAAEMRJAEAAAAAAGBZCJIMQ0ESAAAAAAAwFUGSYVjaBgAAAAAATEWQZBhiJAAAAAAAYCqCJNOQJAEAAAAAAEMRJJmgZzmbzdI2AAAAAABgKIIkAAAAAAAALAtBkmEoSAIAAAAAAKYiSDKMxSZJAAAAAADAUARJhqEiCQAAAAAAmIogCQAAAAAAAMtCkGQYi5IkAAAAAABgKIIkw5AjAQAAAAAAUxEkGYYcCQAAAAAAmIogyQBLwiNKkgAA/3979xtkZV3/f/x1cBcioQjbdf2H/my60TjTnxmsKEcGxwHXBVHTiWKoyZkYm1LqhgZkOpWGIhMoNqO3dCSbzEpIh8Uai6xkNJzKG+mMY5kgKn9W44/ILnD9blhbKHy5wCP72eXxuCPn2mWvz5l5zzns0891HQAAKJSQVBgZCQAAACiVkFQYG5IAAACAUglJhfGpbQAAAECphKTCyEgAAABAqYSkwtiQBAAAAJRKSCqNkgQAAAAUSkgqjIwEAAAAlEpIKoyQBAAAAJRKSCrA/17N5lPbAAAAgFIJSYXRkQAAAIBSCUkAAAAA1CIkFcalbQAAAECphKTC6EgAAABAqYSkwuhIAAAAQKmEpNLYkgQAAAAUSkgqjI4EAAAAlEpIKoyOBAAAAJRKSCqMT20DAAAASiUkFUZHAgAAAEolJBVGRwIAAABKJSQVR0oCAAAAyiQkFcalbQAAAECphKTCCEkAAABAqYSkwjRc2gYAAAAUSkgqjY4EAAAAFEpIKoyOBAAAAJRKSCpA439ujNRwkyQAAACgUEISAAAAALUISYWxIQkAAAAolZAEAAAAQC1CUmHsSAIAAABKJSQVpuFz2wAAAIBCCUkAAAAA1CIklcaGJAAAAKBQQlJhdCQAAACgVEJSYRrutg0AAAAUSkgCAAAAoJamh6SNGzdm9uzZufDCCzNjxoysX78+SbJ169bMnj07nZ2dmTlzZjZt2tTsUwMAAADwDmp6SLr66qszadKkLF++PNOnT8+iRYuSJEuWLMn48ePT3Rt/cfEAABWtSURBVN2dSy+9NDfccEOzTw0AAADAO6ipIamnpydPP/10ZsyYkST5zGc+k69//etJktWrV2fatGlJkqlTp+aRRx5JX19fM08PAAAAwDuopZk/bN26dTnxxBNz4403Zu3atWlra8u3v/3tJG9c8tbW1vbGSVtaMmrUqPT09OT444+v9bOPO25UM5c6oNraRu/zeOO23gN+DQaCOaR0ZpTBwJxSOjNK6cwopTtaZ/SwQ1J3d3cWLFiwz7FTTz01f/vb33LFFVdk3rx5ue+++zJ37twsW7bsLX+/qqoMG1Z/Q9SWLduzd291uMstRlvb6GzatG2fY6+++lr/n9/8NTjS9jejUBIzymBgTimdGaV0ZpTSDYUZHTascVibdg47JHV2dqazs3OfY88//3wuuuiiTJo0Kckbl7Bdf/31SZL29vZs3rw5HR0d2b17d3bs2JExY8Yc7ukBAAAAOMKaeo+kcePGpaOjI7/73e+SJL/97W9zxhlnJEkmTpyY5cuXJ0lWrlyZ8ePHp7W1tZmnBwAAAOAd1NR7JCXJ0qVLc9111+Xmm2/OqFGjcuONNyZJ5syZk7lz56arqyujR4/u/zQ3kkZjoFcAAAAAcHBND0mnn376fu+JNGbMmNx+++3NPh0AAAAAR0hTL20DAAAAYOgSkgAAAACoRUgCAAAAoBYhCQAAAIBahCQAAAAAahGSAAAAAKhFSAIAAACgFiGpAI00BnoJAAAAAAclJAEAAABQi5AEAAAAQC1CEgAAAAC1CEkAAAAA1CIkAQAAAFCLkAQAAABALUISAAAAALUISQAAAADUIiSVoDHQCwAAAAA4OCEJAAAAgFqEJAAAAABqEZIAAAAAqEVIAgAAAKAWIQkAAACAWoQkAAAAAGoRkgAAAACoRUgCAAAAoBYhqQCNgV4AAAAAQA1CEgAAAAC1CEkAAAAA1CIkAQAAAFCLkAQAAABALUISAAAAALUISQAAAADUIiQBAAAAUIuQBAAAAEAtQlIJGgO9AAAAAICDE5IAAAAAqEVIAgAAAKAWIQkAAACAWoQkAAAAAGoRkgAAAACoRUgCAAAAoBYhCQAAAIBahCQAAAAAahGSCtBIY6CXAAAAAHBQQhIAAAAAtQhJAAAAANQiJAEAAABQi5AEAAAAQC1CEgAAAAC1CEkAAAAA1CIkAQAAAFCLkAQAAABALUJSARqNgV4BAAAAwMEJSQAAAADUIiQBAAAAUIuQBAAAAEAtQhIAAAAAtQhJAAAAANQiJAEAAABQi5AEAAAAQC1CEgAAAAC1CEkAAAAA1CIkAQAAAFCLkAQAAABALUISAAAAALUISQAAAADUIiQBAAAAUIuQBAAAAEAtQhIAAAAAtQhJAAAAANQiJAEAAABQi5BUgEZjoFcAAAAAcHBCEgAAAAC1CEkAAAAA1CIkAQAAAFCLkAQAAABALUISAAAAALUISQAAAADUIiQBAAAAUIuQBAAAAEAtQlIBGmkM9BIAAAAADkpIAgAAAKAWIQkAAACAWpoektavX5+ZM2dm+vTpmTVrVl544YUkSW9vb6666qp0dnbmoosuyrPPPtvsUwMAAADwDmp6SLrlllvS1dWVFStWZPLkyVm8eHGSZNmyZRk5cmS6u7szf/78zJs3r9mnBgAAAOAd1PSQtHfv3mzfvj1JsnPnzrzrXe9KkqxevToXXHBBkuTMM89MT09PNmzY0OzTAwAAAPAOaWn2D5wzZ05mzJiRZcuWpa+vL/fee2+SZOPGjWlra+v/vra2trz00ks58cQTa/3c444b1eylDpi2ttH7PP7Xrj0H/BoMBHNI6cwog4E5pXRmlNKZUUp3tM7oYYek7u7uLFiwYJ9jp59+enbt2pXvfve7Offcc/PQQw/la1/7Wn75y1+mqqo0Gv/9mPuqqjJsWP0NUVu2bM/evdXhLrcYbW2js2nTtn2OvfrKa/1/fvPX4Ejb34xCScwog4E5pXRmlNKZUUo3FGZ02LDGYW3aOeyQ1NnZmc7Ozn2O9fT0pLOzM+eee26SZMqUKbnuuuvyyiuv5Pjjj8/GjRszbty4JMnmzZvT3t5+uKcHAAAA4Ahr6j2S3ve+92XEiBFZu3ZtkuSJJ57Isccem7Fjx2bixIlZsWJFkmTt2rUZMWJE7cvaAAAAABh4Tb1HUqPRyG233Zbvfe97ef3113Psscdm6dKlSZJZs2bl2muvTVdXV4YPH56FCxc289QAAAAAvMOafrPtD3/4w7nvvvvecnzEiBG56aabmn26IaVx8G8BAAAAGDBNvbSNt0lJAgAAAAomJBWkoSQBAAAABROSCtLQkQAAAICCCUkAAAAA1CIkAQAAAFCLkFQQl7YBAAAAJROSCiAgAQAAAIOBkFQURQkAAAAol5AEAAAAQC1CUkFc4gYAAACUTEgqiI4EAAAAlExIKomSBAAAABRMSCpIQ0kCAAAACiYkAQAAAFCLkFQSG5IAAACAgglJAAAAANQiJBXEhiQAAACgZEJSQRpKEgAAAFAwIQkAAACAWoSkotiSBAAAAJRLSCqIjAQAAACUTEgqiHskAQAAACUTkgAAAACoRUgCAAAAoBYhqQAN17QBAAAAg4CQVBBBCQAAACiZkAQAAABALUJSQWxIAgAAAEomJAEAAABQi5BUEBuSAAAAgJIJSSVxbRsAAABQMCGpIDISAAAAUDIhCQAAAIBahKQC2IkEAAAADAZCUkkUJQAAAKBgQlJBdCQAAACgZEJSQRo+tQ0AAAAomJAEAAAAQC1CEgAAAAC1CEkFcWUbAAAAUDIhqSA6EgAAAFAyIakoUhIAAABQLiGpBPoRAAAAMAgISQAAAADUIiQVxM22AQAAgJIJSQAAAADUIiQVxI4kAAAAoGRCUkEa7roNAAAAFExIAgAAAKAWIakkNiQBAAAABROSCqIjAQAAACUTkgogIAEAAACDgZBUEh/bBgAAABRMSAIAAACgFiGpIPYjAQAAACUTkgriyjYAAACgZEISAAAAALUISQAAAADUIiQVoPr3fxuubQMAAAAKJiQVREYCAAAASiYklaA6+LcAAAAADDQhqSS2JAEAAAAFE5IKYEMSAAAAMBgISQWoqjdSkg1JAAAAQMmEpKJISQAAAEC5hKQC/HtDUho6EgAAAFAwIakgOhIAAABQMiEJAAAAgFqEpJLYkgQAAAAUTEgqQJVqoJcAAAAAcFBCUgH6b7ZtSxIAAABQMCGpJDoSAAAAUDAhCQAAAIBahKQC/PfSNgAAAIByCUkF+M/NthtKEgAAAFAwIakE/R/apiQBAAAA5RKSCmJHEgAAAFAyIQkAAACAWoSkAlQH/xYAAACAAScklcCntgEAAACDgJBUgEpJAgAAAAYBIakgDSUJAAAAKJiQVIDKTZIAAACAQeBth6QlS5Zk6dKl/Y+3bt2a2bNnp7OzMzNnzsymTZuSJL29vbnqqqvS2dmZiy66KM8+++zbPfXQY0MSAAAAULDDDknbtm3L/Pnzc+edd+5zfMmSJRk/fny6u7tz6aWX5oYbbkiSLFu2LCNHjkx3d3fmz5+fefPmvb2VDyHVv7ck6UgAAABAyQ47JD388MM57bTT8qUvfWmf46tXr860adOSJFOnTs0jjzySvr6+rF69OhdccEGS5Mwzz0xPT082bNjwNpY+9AhJAAAAQMlaDvcvXnjhhUmyz2VtSbJx48a0tbW98cNbWjJq1Kj09PTsczxJ2tra8tJLL+XEE0+sdb7jjht1uEstTlvb6H0ev7R1V5KkdXjLW74GA8EcUjozymBgTimdGaV0ZpTSHa0zetCQ1N3dnQULFuxz7PTTT89dd91V6wRVVWXYsGGpqiqNRuMtx+vasmV79u4d/HelbmsbnU2btu1z7F+vvpYk6evd/ZavwZG2vxmFkphRBgNzSunMKKUzo5RuKMzosGGNw9q0c9CQ1NnZmc7Ozto/sL29PZs3b05HR0d2796dHTt2ZMyYMTn++OOzcePGjBs3LkmyefPmtLe3H/KCh6L+T21ruLgNAAAAKNfb/tS2N5s4cWKWL1+eJFm5cmXGjx+f1tbWTJw4MStWrEiSrF27NiNGjKh9WdtQ19+RBnQVAAAAAP+3w75H0oHMmTMnc+fOTVdXV0aPHp1FixYlSWbNmpVrr702XV1dGT58eBYuXNjsUw9e//nUNiUJAAAAKNjbDklXXHHFPo/HjBmT22+//S3fN2LEiNx0001v93QAAAAADJCmX9rGoRv8txAHAAAAjgZCUgH+e69t17YBAAAA5RKSSmBLEgAAADAICEkFsSEJAAAAKJmQBAAAAEAtQlIBKte2AQAAAIOAkFSCf3ckV7YBAAAAJROSCtC/H8lNkgAAAICCCUkFqOxIAgAAAAYBIakkShIAAABQMCGpCG62DQAAAJRPSCrAfy9tsyUJAAAAKJeQVIAxo0YkSU7tGDXAKwEAAAA4sJaBXgDJqR2j8+0vjs+px48e6KUAAAAAHJCQVIj/d8J7BnoJAAAAAP8nl7YBAAAAUIuQBAAAAEAtQhIAAAAAtQhJAAAAANQiJAEAAABQi5AEAAAAQC1CEgAAAAC1CEkAAAAA1CIkAQAAAFCLkAQAAABALUISAAAAALUISQAAAADUIiQBAAAAUIuQBAAAAEAtQhIAAAAAtQhJAAAAANQiJAEAAABQi5AEAAAAQC1CEgAAAAC1CEkAAAAA1CIkAQAAAFBLy0AvoK5hwxoDvYSmGUrPhaHJjFI6M8pgYE4pnRmldGaU0g32GT3c9TeqqqqavBYAAAAAhiCXtgEAAABQi5AEAAAAQC1CEgAAAAC1CEkAAAAA1CIkAQAAAFCLkAQAAABALUISAAAAALUISQAAAADUIiQBAAAAUIuQdAQ98MADOf/88zN58uTcc889A70cjjLbt2/P1KlTs379+iTJo48+mmnTpmXy5MlZvHhx//c99dRTufjiizNlypR861vfyu7du5MkGzZsyMyZM3PeeeflK1/5Snbs2DEgz4Oh6bbbbktXV1e6urqycOHCJGaU8txyyy05//zz09XVlTvvvDOJOaVMN910U+bOnZvk0Gdx69atmT17djo7OzNz5sxs2rRpwJ4HQ8+sWbPS1dWV6dOnZ/r06fnrX/96wN+RDvX1FZrhN7/5TS6++OJ0dnbm+uuvT+K9fr8qjoiXXnqpmjRpUvXKK69UO3bsqKZNm1Y988wzA70sjhJ/+ctfqqlTp1ZnnHFGtW7dumrnzp3VxIkTq+eff77q6+urLrvssmr16tVVVVVVV1dX9ec//7mqqqqaN29edc8991RVVVWzZ8+uHnzwwaqqquq2226rFi5cODBPhiHnj3/8Y/XZz3622rVrV9Xb21t94QtfqB544AEzSlEee+yxasaMGVVfX1+1c+fOatKkSdVTTz1lTinOo48+Wn3iE5+ovvnNb1ZVdeiz+J3vfKe64447qqqqqvvvv7+aM2fOkX4KDFF79+6tzjrrrKqvr6//2IF+Rzqcf6vC2/X8889XZ511VvXiiy9Wvb291ec+97lq9erV3uv3w46kI+TRRx/NJz/5yYwZMybvfve7M2XKlKxatWqgl8VR4qc//Wmuu+66tLe3J0mefPLJnHrqqTnllFPS0tKSadOmZdWqVXnhhRfy+uuv56Mf/WiS5OKLL86qVavS19eXP/3pT5kyZco+x6EZ2traMnfu3AwfPjytra35wAc+kOeee86MUpSPf/zjufvuu9PS0pItW7Zkz5492bp1qzmlKK+++moWL16cyy+/PEkOaxZXr16dadOmJUmmTp2aRx55JH19fQPwbBhq/v73vydJLrvsslxwwQX50Y9+dMDfkQ7136rQDL/+9a9z/vnnp6OjI62trVm8eHFGjhzpvX4/hKQjZOPGjWlra+t/3N7enpdffnkAV8TR5IYbbsj48eP7Hx9oHt98vK2tLS+//HJeeeWVjBo1Ki0tLfsch2b44Ac/2P8m/Nxzz6W7uzuNRsOMUpzW1tbceuut6erqyoQJE7yWUpxrr7023/jGN/Ke97wnyVvf7+vM4v/+nZaWlowaNSo9PT1H+JkwFG3dujUTJkzID3/4w9x11135yU9+kg0bNtR6HT3Y6ys0wz//+c/s2bMnl19+eaZPn54f//jH3usPQEg6Qvbu3ZtGo9H/uKqqfR7DkXSgeTzQ8f3Nq/ml2Z555plcdtllufrqq3PKKaeYUYp05ZVXZs2aNXnxxRfz3HPPmVOKcd999+WEE07IhAkT+o81YxarqsqwYX5l4O372Mc+loULF2b06NEZO3ZsLrnkktx6662H9DrqdyreSXv27MmaNWvy/e9/P/fee2+efPLJrFu3znv9frQM9AKOFh0dHVm7dm3/402bNvVfZgRHWkdHxz43z/zPPL75+ObNm9Pe3p6xY8dm27Zt2bNnT4455hjzS9M98cQTufLKKzN//vx0dXXl8ccfN6MU5dlnn01vb28+9KEPZeTIkZk8eXJWrVqVY445pv97zCkDaeXKldm0aVOmT5+ef/3rX3nttdfSaDQOeRbb29uzefPmdHR0ZPfu3dmxY0fGjBkzUE+LIWTt2rXp6+vrj51VVeWkk06q9X5/sNdXaIb3v//9mTBhQsaOHZskOffcc73XH4D/vXCEfOpTn8qaNWvS09OTnTt35le/+lXOPvvsgV4WR6mPfOQj+cc//tG/ffPBBx/M2WefnZNOOikjRozIE088kSRZsWJFzj777LS2tmb8+PFZuXJlkmT58uXml6Z58cUX89WvfjWLFi1KV1dXEjNKedavX59rrrkmvb296e3tzcMPP5wZM2aYU4px55135sEHH8yKFSty5ZVX5pxzzsmCBQsOeRYnTpyY5cuXJ3kjTo0fPz6tra0D86QYUrZt25aFCxdm165d2b59e+6///7cfPPN+/0d6VD/HQDNMGnSpPzhD3/I1q1bs2fPnvz+97/Peeed571+PxpVVVUDvYijxQMPPJA77rgjfX19ueSSS/LlL395oJfEUeacc87J3XffnZNPPjlr1qzJggULsmvXrkycODHz5s1Lo9HI008/nWuuuSbbt2/PGWeckQULFmT48OF54YUXMnfu3GzZsiUnnHBCfvCDH+S9733vQD8lhoDrr78+P//5zzNu3Lj+YzNmzMhpp51mRinK0qVL093dnWOOOSaTJ0/OFVdc4bWUIv3iF7/I448/nhtvvPGQZ/HVV1/N3Llzs27duowePTqLFi3KySefPNBPiSFiyZIleeihh7J37958/vOfzxe/+MUD/o50qK+v0Aw/+9nPctddd6Wvry+f/vSnc8011+Sxxx7zXv8mQhIAAAAAtbi0DQAAAIBahCQAAAAAahGSAAAAAKhFSAIAAACgFiEJAAAAgFqEJAAAAABqEZIAAAAAqEVIAgAAAKCW/w/6w73cQ+RTFgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#residual_value_list.append(0.13)\n",
    "\n",
    "p_values_logpdf= multivariate_normal.logpdf(residual_value_list[:],mean,cov)\n",
    "#p_values_pdf= multivariate_normal.pdf(residual_value_list[:],mean,cov)\n",
    "\n",
    "#print(sorted(p_values_logpdf))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(p_values_logpdf[:6000])\n",
    "plt.axhline(y=-22 , linestyle='dashed' ,color=\"black\", label='Theta=')\n",
    "plt.title(\"Likelihood p\")\n",
    "plt.show()\n",
    "\n",
    "#plt.figure(figsize=(15,5))\n",
    "#plt.plot(p_values_pdf[:])\n",
    "#plt.title(\"pdf\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look_back: 5\n",
      "look_ahead: 1\n",
      "Stacked Sequence Shape: (60000, 17)\n",
      "X look_back: (59995, 5, 17)\n",
      "y look_ahead: (59995, 1, 17)\n"
     ]
    }
   ],
   "source": [
    "n_steps, var_look_back=[5,5]\n",
    "var_look_ahead=1\n",
    "#test_key_c20 =['c20_h130_pl0_v100_s0']\n",
    "#test_key_c20 =['c100_h130_pl1_v100_s0']\n",
    "#test_key_c20 =['c100_h130_pl2_v100_s0']\n",
    "#test_key_c20 =['c100_h130_pl0_v100_s1']\n",
    "#test_key_c20 =['c3_h130_pl0_v100_s0']\n",
    "test_key_c20 =['c100_h130_pl0_v100_s0']\n",
    "\n",
    "\n",
    "X_key_list_stacked_tensor_dict_test_c20, y_key_list_stacked_tensor_dict_test_c20 = create_Xy_key_list_stacked_tensor(tensor_dict, test_key_c20,\n",
    "                                                                                                       var_look_back, \n",
    "                                                                                                       var_look_ahead)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "X_scaled_key_list_stacked_tensor_dict_test_c20, y_scaled_key_list_stacked_tensor_dict_test_c20 = create_Xy_key_list_scaled_stacked_tensor(X_key_list_stacked_tensor_dict_test_c20, \n",
    "                                                                                                                            y_key_list_stacked_tensor_dict_test_c20)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59995/59995 [==============================] - 2s 31us/step\n"
     ]
    }
   ],
   "source": [
    "#model_vanilla_1_predictions_test_c20 = model_vanilla_sigmoid.predict(X_scaled_key_list_stacked_tensor_dict_test_c20[test_key_c20[0]][:,:,0:17], verbose=1)\n",
    "#model_vanilla_1_predictions_test_s1 = model_vanilla_sigmoid.predict(X_scaled_key_list_stacked_tensor_dict_test_c20[test_key_c20[0]][:,:,0:17], verbose=1)\n",
    "#model_vanilla_1_predictions_test_pl2 = model_vanilla_sigmoid.predict(X_scaled_key_list_stacked_tensor_dict_test_c20[test_key_c20[0]][:,:,0:17], verbose=1)\n",
    "#model_vanilla_1_predictions_test_c3 = model_vanilla_sigmoid.predict(X_scaled_key_list_stacked_tensor_dict_test_c20[test_key_c20[0]][:,:,0:17], verbose=1)\n",
    "#model_vanilla_1_predictions_test_pl1 = model_vanilla_sigmoid.predict(X_scaled_key_list_stacked_tensor_dict_test_c20[test_key_c20[0]][:,:,0:17], verbose=1)\n",
    "model_vanilla_1_predictions_test_train = model_vanilla_sigmoid.predict(X_scaled_key_list_stacked_tensor_dict_test_c20[test_key_c20[0]][:,:,0:17], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.62140484e-09]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(\"predicted values shape: \" + str(model_vanilla_1_predictions_test_c20[15:].shape))\n",
    "#print(\"actual values shape: \" + str(y_scaled_key_list_stacked_tensor_dict_test_c20[test_key_c20[0]][:,:,0:1].shape))\n",
    "#print(\"residuals shape: \" + str(residuals[:,:,0:1].shape))\n",
    "\n",
    "#best shift=24\n",
    "shift=0\n",
    "start=0\n",
    "end=6000\n",
    "\n",
    "\n",
    "#prediction_value_list_test_c20 = model_vanilla_1_predictions_test_c20.flatten()\n",
    "#prediction_value_list_test_c20 = model_vanilla_1_predictions_test_s1.flatten()\n",
    "#prediction_value_list_test_c20 = model_vanilla_1_predictions_test_pl2.flatten()\n",
    "#prediction_value_list_test_c20 = model_vanilla_1_predictions_test_c3.flatten()\n",
    "prediction_value_list_test_c20 = model_vanilla_1_predictions_test_train.flatten()\n",
    "\n",
    "actual_value_list_test_c20 = y_scaled_key_list_stacked_tensor_dict_test_c20[test_key_c20[0]][:,:,0:1].flatten()\n",
    "#residual_value_list = abs(actual_value_list[shift:] - prediction_value_list[0:len(actual_value_list)-shift])\n",
    "\n",
    "s = len(prediction_value_list_test_c20[shift:])\n",
    "residual_value_list_test_c20 = abs(prediction_value_list_test_c20[shift:] - actual_value_list_test_c20[:s])\n",
    "sorted_residuals= np.sort(residual_value_list_test_c20)\n",
    "print(sorted_residuals[0:1])\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(prediction_value_list_test_c20[:6000], color=\"blue\", label='predicted')\n",
    "plt.plot(actual_value_list_test_c20[:6000], color=\"green\", label='actual')\n",
    "plt.plot(residual_value_list_test_c20[:6000] , color=\"red\", label='residuals')\n",
    "#plt.axhline(y=sorted_residuals[-4] , linestyle='dashed' ,color=\"black\", label='Threshold')\n",
    "#plt.yticks([theta], label=\"test\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Actual and predicted Values\")\n",
    "plt.show()\n",
    "\n",
    "p_values_logpdf_test_c20= multivariate_normal.logpdf(residual_value_list_test_c20[:],mean,cov)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(p_values_logpdf_test_c20[:6000])\n",
    "plt.axhline(y=-22 , linestyle='dashed' ,color=\"black\", label='Theta=')\n",
    "plt.title(\"Likelihood p \")\n",
    "plt.show()\n",
    "\n",
    "#plt.figure(figsize=(20,10))\n",
    "#plt.plot(prediction_value_list_test_c20[15:], color=\"blue\", label='predicted')\n",
    "#plt.plot(actual_value_list_test_c20[:], color=\"green\", label='actual')\n",
    "#plt.plot(residual_value_list_test_c20[:] , color=\"red\", label='residuals')\n",
    "#plt.axhline(y=sorted_residuals[-4] , linestyle='dashed' ,color=\"black\", label='Threshold')\n",
    "#plt.yticks([theta], label=\"test\")\n",
    "#plt.legend()\n",
    "#plt.xlabel(\"Steps\")\n",
    "#plt.ylabel(\"Value\")\n",
    "#plt.title(\"Actual and predicted Values\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#plt.figure(figsize=(20,10))\n",
    "#plt.plot(prediction_value_list_test_c20[:], color=\"blue\", label='predicted')\n",
    "#plt.plot(actual_value_list_test_c20[:], color=\"green\", label='actual')\n",
    "#plt.plot(residual_value_list_test_c20[:] , color=\"red\", label='residuals')\n",
    "#plt.axhline(y=max(residual_value_list_test_c20) , linestyle='dashed' ,color=\"black\", label='Theta=' + str(round(max(residual_value_list),3)))\n",
    "#plt.yticks([theta], label=\"test\")\n",
    "#plt.legend()\n",
    "#plt.xlabel(\"Steps\")\n",
    "#plt.ylabel(\"Value (scaled)\")\n",
    "#plt.title(\"Actual and predicted Values\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_values_logpdf_test_c20= multivariate_normal.logpdf(residual_value_list_test_c20[:],mean,cov)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(p_values_logpdf_test_c20[:6000])\n",
    "plt.axhline(y=-21 , linestyle='dashed' ,color=\"black\", label='Theta=')\n",
    "plt.title(\"Likelihood p \")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
